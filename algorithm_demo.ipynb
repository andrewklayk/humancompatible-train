{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efc20cc",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use the **constrained training algorithms** implemented in this toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ccbc7",
   "metadata": {},
   "source": [
    "To train a network, instantiate an algorithm, passing to it the model, the dataset, a list of `FairnessConstraint`s and the algorithm's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe05ba",
   "metadata": {},
   "source": [
    "Load and prepare data from `folktables`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58feeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from folktables import ACSDataSource, ACSIncome\n",
    "\n",
    "# load folktables data\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"OK\"], download=True)\n",
    "features, labels, groups = ACSIncome.df_to_numpy(acs_data)\n",
    "# split\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    features, labels, groups, test_size=0.2, random_state=42)\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute (RAC1P)\n",
    "features_train = torch.tensor(X_train, dtype=torch.float32)[:,:-1]\n",
    "labels_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, labels_train)\n",
    "\n",
    "c_batch_size = 128\n",
    "min_subgroup_size = c_batch_size\n",
    "# For each subgroup, FairnessConstraint needs a list of indices of samples belonging to that subgroup\n",
    "group_indices_train = [\n",
    "    np.nonzero(groups_train == group_id)[0] for group_id in np.unique(groups_train)\n",
    "    if np.count_nonzero(groups == group_id) > min_subgroup_size\n",
    "]\n",
    "\n",
    "# repeat for test set\n",
    "features_test = torch.tensor(X_test, dtype=torch.float32)[:,:-1]\n",
    "labels_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "dataset_test = torch.utils.data.TensorDataset(features_test, labels_test)\n",
    "group_indices_test = [\n",
    "    np.nonzero(groups_test == group_id)[0] for group_id in np.unique(groups_test)\n",
    "    if np.count_nonzero(groups == group_id) > min_subgroup_size\n",
    "]\n",
    "\n",
    "print(f'Protected attribute: {ACSIncome.group}')\n",
    "print(f'Number of subgroups considered: {len(group_indices_train)}')\n",
    "print(f'Size of subgroups: {[len(g) for g in group_indices_train]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feed165",
   "metadata": {},
   "source": [
    "Let's say we want to add an equal loss constraint on the model.\n",
    "\n",
    "We can do that by using the `FairnessConstraint` class, which will handle sampling (if possible, sampling an equal number of samples from each relevant subgroup in each minibatch), and passing it the function that will calculate the value of the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from src.constraints.constraint import FairnessConstraint\n",
    "from src.constraints.constraint_fns import abs_loss_equality, tpr_equality\n",
    "\n",
    "# the protected attribute is \"Race\" (RAC1P)\n",
    "# we put a pairwise constraint on each combination of subgroups\n",
    "constraint_bound = 0.05\n",
    "constraints = []\n",
    "for gr1, gr2 in combinations(group_indices_train, 2):\n",
    "    c = FairnessConstraint(\n",
    "        dataset=dataset_train,\n",
    "        group_indices=[gr1, gr2],\n",
    "        # subtract bound from absolute loss difference to bring constraint to form $c \\leq 0$\n",
    "        fn=lambda model, samples: tpr_equality(torch.nn.BCEWithLogitsLoss(), model, samples) - constraint_bound,\n",
    "        batch_size=c_batch_size\n",
    "    )\n",
    "\n",
    "    constraints.append(c)\n",
    "\n",
    "print(f'Number of constraints: {len(constraints)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to analyze model performance\n",
    "\n",
    "def model_stats(model, features, labels, groups, constraints, constraint_bound):\n",
    "    with torch.inference_mode():\n",
    "        gr_ind = list(combinations(groups, 2))\n",
    "        vals = []\n",
    "        acc_dif = []\n",
    "        for i, c in enumerate(constraints):\n",
    "            idx1, idx2 = gr_ind[i]\n",
    "            val = c.eval(model, [(features[idx1], labels[idx1]), (features[idx2], labels[idx2])]) + constraint_bound\n",
    "            vals.append(val.numpy().item())\n",
    "\n",
    "            logits1 = model(features[idx1])\n",
    "            logits2 = model(features[idx2])\n",
    "            outs1 = torch.nn.functional.sigmoid(logits1).numpy()\n",
    "            outs2 = torch.nn.functional.sigmoid(logits2).numpy()\n",
    "            preds1 = (outs1.T > 0.5).astype(float)\n",
    "            preds2 = (outs2.T > 0.5).astype(float) \n",
    "            acc1 = np.mean(preds1 == labels[idx1].numpy())\n",
    "            acc2 = np.mean(preds2 == labels[idx2].numpy())\n",
    "            acc_dif.append(abs(acc1-acc2))\n",
    "\n",
    "        print(f'constraints (should be <= {constraint_bound}):')\n",
    "        print(np.round(vals, decimals=3))\n",
    "        print(f'c mean: {np.mean(vals)}')\n",
    "        print(f'c min: {np.min(vals)}')\n",
    "        print(f'c max: {np.max(vals)}')\n",
    "        print('---')\n",
    "\n",
    "        logits = model(features)\n",
    "        outs = torch.nn.functional.sigmoid(logits).numpy()\n",
    "        preds = (outs.T > 0.5).astype(float)\n",
    "        acc = np.sum(preds == labels.numpy())/len(labels)\n",
    "        print(f'accuracy: {acc}')\n",
    "        print('accuracy abs. difference:')\n",
    "        print(np.round(acc_dif, decimals=3))\n",
    "        print(f'acc abs dif mean: {np.mean(acc_dif)}')\n",
    "        print(f'acc abs dif min: {np.min(acc_dif)}')\n",
    "        print(f'acc abs dif max: {np.max(acc_dif)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06f697",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26213fe",
   "metadata": {},
   "source": [
    "For comparison, let us first train a model **without constraints**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec89642",
   "metadata": {},
   "source": [
    "Define a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c957562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_uncon = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409a977",
   "metadata": {},
   "source": [
    "And start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model_uncon.parameters())\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_feat, batch_label in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model_uncon(batch_feat)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit.squeeze(), batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch: {epoch}, loss: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ebc1d",
   "metadata": {},
   "source": [
    "Let's now analyze how well the **unconstrained** model does in terms of constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN')\n",
    "model_stats(model_uncon, features_train, labels_train, group_indices_train, constraints, constraint_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST')\n",
    "model_stats(model_uncon, features_test, labels_test, group_indices_test, constraints, constraint_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c4fa0",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbc50a",
   "metadata": {},
   "source": [
    "Now let us train the same model with one of the **constrained** training algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.algorithms import SSLALM, SSG\n",
    "from torch.nn import Sequential\n",
    "\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")\n",
    "\n",
    "optimizer = SSG(\n",
    "    net=model,\n",
    "    data=dataset_train,\n",
    "    loss=torch.nn.BCEWithLogitsLoss(),\n",
    "    constraints=constraints,\n",
    ")\n",
    "\n",
    "history = optimizer.optimize(\n",
    "    max_runtime=180,\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    ctol=0.1,\n",
    "    f_stepsize_rule='dimin',\n",
    "    f_stepsize=0.1,\n",
    "    c_stepsize_rule='dimin',\n",
    "    c_stepsize=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN')\n",
    "model_stats(model, features_train, labels_train, group_indices_train, constraints, constraint_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST')\n",
    "model_stats(model, features_test, labels_test, group_indices_test, constraints, constraint_bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
