{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efc20cc",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use the **constrained training algorithms** implemented in this toolkit with **PyTorch**-like API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ccbc7",
   "metadata": {},
   "source": [
    "The algorithms implemented in the **humancompatible.train.torch** subpackage share a similar idea. Before the training, you initialize an algorithm like you would a PyTorch one. Then, during the training process, you:\n",
    "\n",
    "1. Evaluate a constraint and compute its gradient\n",
    "2. Call the `dual_step` function to update dual parameters and save the constraint gradient for the primal update\n",
    "3. Call the `step` function to update the primal parameters (generally, model weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3224eb",
   "metadata": {},
   "source": [
    "Let's try the Stochastic Smooth Linearized Augmented Lagrangian (SSLALM) algorithm on a constrained learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f86ab",
   "metadata": {},
   "source": [
    "Let's train a simple classification model, putting a constraint on the norm of each layer's parameters.\n",
    "\n",
    "In the canonical form, the algorithm expects equality constraints that are equal to 0; however, we can easily transform arbitrary inequality constraints to that form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome, generate_categories\n",
    "\n",
    "# load folktables data\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"VA\"], download=True)\n",
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(features=ACSIncome.features, definition_df=definition_df)\n",
    "df_feat, df_labels, _ = ACSIncome.df_to_pandas(acs_data,categories=categories, dummies=True)\n",
    "\n",
    "sens_cols = ['SEX_Female', 'SEX_Male']\n",
    "features = df_feat.drop(columns=sens_cols).to_numpy(dtype=\"float\")\n",
    "groups = df_feat[sens_cols].to_numpy(dtype=\"float\")\n",
    "labels = df_labels.to_numpy(dtype=\"float\")\n",
    "# split\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    features, labels, groups, test_size=0.2, random_state=42)\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "sens_train = torch.tensor(groups_train)\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.algorithms.torch import SSLALM\n",
    "import torch\n",
    "from torch.nn import Sequential\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")\n",
    "\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "optimizer = SSLALM(\n",
    "    params=model.parameters(),\n",
    "    m=m,\n",
    "    lr=0.01,\n",
    "    dual_lr=0.1\n",
    ")\n",
    "# bounds for the constraints: norm of each param group should be <= 1\n",
    "constraint_bounds = [1.]*m\n",
    "\n",
    "epochs = 10\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8758f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    slack_log = []\n",
    "    duals_log = []\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        c_log.append([])\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "            # convert constraint to equality\n",
    "            norm_viol = torch.max(\n",
    "                norm \n",
    "                - constraint_bounds[i],\n",
    "                torch.zeros(1)\n",
    "            )\n",
    "            norm_viol.backward()\n",
    "            # for the Lagrangian family of algorithms, dual_step requires the index of constraint and the value as arguments\n",
    "            # to update the corresponding dual multiplier\n",
    "            # in a stochastic-constrained setting, this estimate needs (in theory) to be independent from the one used to update dual parameters\n",
    "            # in practice, it makes little difference  \n",
    "            optimizer.dual_step(i, c_val=norm_viol)\n",
    "            optimizer.zero_grad()\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "        \n",
    "        # calculate loss and grad\n",
    "        batch_output = model(batch_input)\n",
    "        loss = criterion(batch_output, batch_label)\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4e546",
   "metadata": {},
   "source": [
    "The model is now trained subject to the constraints we set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06f697",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f190b",
   "metadata": {},
   "source": [
    "It is also possible to train a network subject to **stochastic constraints**. One of the main use-cases for that is **fairness**. Let's train a network on the `folktables` dataset without constraints first, so we can identify some biases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec89642",
   "metadata": {},
   "source": [
    "Define a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c957562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_uncon = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409a977",
   "metadata": {},
   "source": [
    "And start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d457fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model_uncon.parameters())\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_feat, batch_sens, batch_label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model_uncon(batch_feat)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch: {epoch}, loss: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd521be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "preds = torch.nn.functional.sigmoid(model_uncon(features_train))\n",
    "pr = PositiveRate()\n",
    "pr(preds, sens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c4fa0",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbc50a",
   "metadata": {},
   "source": [
    "Now let us train the same model with one of the **constrained** training algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef60dce",
   "metadata": {},
   "source": [
    "Here, to make sure each batch contains representatives of each protected group, we can use the BalancedBatchSampler from the `utils` subpackage - a custom PyTorch `Sampler` which yields an equal number of samples from each subgroup in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "sampler = BalancedBatchSampler(\n",
    "    subgroup_onehot=sens_train,\n",
    "    batch_size=256,\n",
    "    drop_last=True\n",
    "    )\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")\n",
    "\n",
    "optimizer = SSLALM(\n",
    "    params=model_con.parameters(),\n",
    "    m=1,\n",
    "    lr=0.05,\n",
    "    dual_lr=0.05,\n",
    "    dual_bound=5,\n",
    "    rho=1.\n",
    ")\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "        fair_constraint = torch.max(fair_loss -  fair_crit_bound, torch.zeros(1))\n",
    "        fair_constraint.backward()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # fair_loss = fair_criterion(out[128:], batch_sens[128:])\n",
    "        # fair_constraint = torch.max(fair_loss -  fair_crit_bound, torch.zeros(1))\n",
    "        optimizer.dual_step(0, c_val=fair_constraint)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        c_log.append([fair_loss.detach().item()])\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "        # calculate loss and grad\n",
    "        batch_output = model_con(batch_input)\n",
    "        loss = criterion(batch_output, batch_label)\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993899e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "\n",
    "preds = torch.nn.functional.sigmoid(model_con(features_train))\n",
    "pr = PositiveRate()\n",
    "pr(preds, sens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_criterion(model_con(features_train), sens_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-mkl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
