{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efc20cc",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use the **constrained training algorithms** implemented in this toolkit with **PyTorch**-like API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ccbc7",
   "metadata": {},
   "source": [
    "The algorithms implemented in the **humancompatible.train.torch** subpackage share a similar idea. Before the training, you initialize an algorithm like you would a PyTorch one. Then, during the training process, you:\n",
    "\n",
    "1. Evaluate a constraint and compute its gradient\n",
    "2. Call the `dual_step` function to update dual parameters and save the constraint gradient for the primal update\n",
    "3. Call the `step` function to update the primal parameters (generally, model weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3224eb",
   "metadata": {},
   "source": [
    "Let's try the Stochastic Smooth Linearized Augmented Lagrangian (SSLALM) algorithm on a constrained learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f86ab",
   "metadata": {},
   "source": [
    "Let's train a simple classification model, putting a constraint on the norm of each layer's parameters.\n",
    "\n",
    "In the canonical form, the algorithm expects equality constraints that are equal to 0; however, we can easily transform arbitrary inequality constraints to that form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome, generate_categories\n",
    "\n",
    "# load folktables data\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"VA\"], download=True)\n",
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(features=ACSIncome.features, definition_df=definition_df)\n",
    "df_feat, df_labels, _ = ACSIncome.df_to_pandas(acs_data,categories=categories, dummies=True)\n",
    "\n",
    "sens_cols = ['SEX_Female', 'SEX_Male']\n",
    "features = df_feat.drop(columns=sens_cols).to_numpy(dtype=\"float\")\n",
    "groups = df_feat[sens_cols].to_numpy(dtype=\"float\")\n",
    "labels = df_labels.to_numpy(dtype=\"float\")\n",
    "# split\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    features, labels, groups, test_size=0.2, random_state=42)\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "sens_train = torch.tensor(groups_train)\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.algorithms.torch import SSLALM\n",
    "import torch\n",
    "from torch.nn import Sequential\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")\n",
    "\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "optimizer = SSLALM(\n",
    "    params=model.parameters(),\n",
    "    m=m,\n",
    "    lr=0.01,\n",
    "    dual_lr=0.1\n",
    ")\n",
    "# bounds for the constraints: norm of each param group should be <= 1\n",
    "constraint_bounds = [1.]*m\n",
    "\n",
    "epochs = 10\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8758f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.547227680683136, constraints: [0.91162425 0.2931389  0.9883125  0.4690594  0.89254874 0.04176571], dual: [0.1717298  0.         0.16387744 0.         0.20794065 0.        ]\n",
      "Epoch: 1, loss: 0.44799599051475525, constraints: [0.9997784  0.7573364  0.999926   0.60372984 0.9997402  0.01519962], dual: [0.22261804 0.         0.24267721 0.         0.27600586 0.        ]\n",
      "Epoch: 2, loss: 0.4300524890422821, constraints: [0.99943334 0.9939212  0.9994772  0.649871   0.9993597  0.02684791], dual: [0.25665787 0.02926148 0.29758924 0.         0.32820562 0.        ]\n",
      "Epoch: 3, loss: 0.4226565361022949, constraints: [0.99923307 0.9998924  0.99918497 0.6757832  0.9990267  0.02891587], dual: [0.2852176  0.03667939 0.34263647 0.         0.37016106 0.        ]\n",
      "Epoch: 4, loss: 0.4160747528076172, constraints: [0.9991638  0.9998352  0.99890137 0.6951654  0.9987838  0.03798234], dual: [0.30832767 0.04193355 0.37953085 0.         0.4063269  0.        ]\n",
      "Epoch: 5, loss: 0.4118559658527374, constraints: [0.99910057 0.99980515 0.9986878  0.70961475 0.9985786  0.03676871], dual: [0.32893416 0.04544878 0.4128549  0.         0.43792698 0.        ]\n",
      "Epoch: 6, loss: 0.40776124596595764, constraints: [0.9990157  0.9997706  0.99854416 0.7220395  0.99835426 0.03650802], dual: [0.34680298 0.04827128 0.44270232 0.         0.46793914 0.        ]\n",
      "Epoch: 7, loss: 0.40476709604263306, constraints: [0.99901295 0.99975055 0.998372   0.73138773 0.9982105  0.03420755], dual: [0.36359015 0.05065143 0.47156116 0.         0.49334034 0.        ]\n",
      "Epoch: 8, loss: 0.4015985429286957, constraints: [0.99916    0.9997194  0.998161   0.73860496 0.9980458  0.03075136], dual: [0.3800049  0.05248834 0.49792564 0.         0.5177125  0.        ]\n",
      "Epoch: 9, loss: 0.39895424246788025, constraints: [0.9992326  0.99971706 0.9981078  0.745724   0.9979355  0.03013416], dual: [0.39538252 0.05432736 0.5228174  0.         0.54028624 0.        ]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    slack_log = []\n",
    "    duals_log = []\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        c_log.append([])\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "            # convert constraint to equality\n",
    "            norm_viol = torch.max(\n",
    "                norm \n",
    "                - constraint_bounds[i],\n",
    "                torch.zeros(1)\n",
    "            )\n",
    "            norm_viol.backward()\n",
    "            # for the Lagrangian family of algorithms, dual_step requires the index of constraint and the value as arguments\n",
    "            # to update the corresponding dual multiplier\n",
    "            optimizer.dual_step(i, c_val=norm_viol)\n",
    "            optimizer.zero_grad()\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "        \n",
    "        # calculate loss and grad\n",
    "        batch_output = model(batch_input)\n",
    "        loss = criterion(batch_output, batch_label)\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4e546",
   "metadata": {},
   "source": [
    "The model is now trained subject to the constraints we set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06f697",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f190b",
   "metadata": {},
   "source": [
    "It is also possible to train a network subject to **stochastic constraints**. One of the main use-cases for that is **fairness**. Let's train a network on the `folktables` dataset without constraints first, so we can identify some biases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec89642",
   "metadata": {},
   "source": [
    "Define a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c957562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_uncon = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409a977",
   "metadata": {},
   "source": [
    "And start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d457fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.47336341450954306, fair: 0.3440811772483088\n",
      "Epoch: 1, loss: 0.3969301203201557, fair: 0.3440811772483088\n",
      "Epoch: 2, loss: 0.3833263358165478, fair: 0.3440811772483088\n",
      "Epoch: 3, loss: 0.3736273459319411, fair: 0.3440811772483088\n",
      "Epoch: 4, loss: 0.3659545639465595, fair: 0.3440811772483088\n",
      "Epoch: 5, loss: 0.3573377646248916, fair: 0.3440811772483088\n",
      "Epoch: 6, loss: 0.3477083052026814, fair: 0.3440811772483088\n",
      "Epoch: 7, loss: 0.3387393544460165, fair: 0.3440811772483088\n",
      "Epoch: 8, loss: 0.32858473226941864, fair: 0.3440811772483088\n",
      "Epoch: 9, loss: 0.3172479239003412, fair: 0.3440811772483088\n",
      "Epoch: 10, loss: 0.30479535460472107, fair: 0.3440811772483088\n",
      "Epoch: 11, loss: 0.2953071137954449, fair: 0.3440811772483088\n",
      "Epoch: 12, loss: 0.2825759767458357, fair: 0.3440811772483088\n",
      "Epoch: 13, loss: 0.27013415735343405, fair: 0.3440811772483088\n",
      "Epoch: 14, loss: 0.26126931391913316, fair: 0.3440811772483088\n",
      "Epoch: 15, loss: 0.2519400581203658, fair: 0.3440811772483088\n",
      "Epoch: 16, loss: 0.24074923005597346, fair: 0.3440811772483088\n",
      "Epoch: 17, loss: 0.23306182797612815, fair: 0.3440811772483088\n",
      "Epoch: 18, loss: 0.22353637896735093, fair: 0.3440811772483088\n",
      "Epoch: 19, loss: 0.2173248700026808, fair: 0.3440811772483088\n",
      "Epoch: 20, loss: 0.21001399354688052, fair: 0.3440811772483088\n",
      "Epoch: 21, loss: 0.20224601616119517, fair: 0.3440811772483088\n",
      "Epoch: 22, loss: 0.19591633574715975, fair: 0.3440811772483088\n",
      "Epoch: 23, loss: 0.18763602858987347, fair: 0.3440811772483088\n",
      "Epoch: 24, loss: 0.18068946589683665, fair: 0.3440811772483088\n",
      "Epoch: 25, loss: 0.17734110817827028, fair: 0.3440811772483088\n",
      "Epoch: 26, loss: 0.1724342206942624, fair: 0.3440811772483088\n",
      "Epoch: 27, loss: 0.1696911079102549, fair: 0.3440811772483088\n",
      "Epoch: 28, loss: 0.16269697310595677, fair: 0.3440811772483088\n",
      "Epoch: 29, loss: 0.16033414941409538, fair: 0.3440811772483088\n",
      "Epoch: 30, loss: 0.15510844634524706, fair: 0.3440811772483088\n",
      "Epoch: 31, loss: 0.15363781030835777, fair: 0.3440811772483088\n",
      "Epoch: 32, loss: 0.15155355991988345, fair: 0.3440811772483088\n",
      "Epoch: 33, loss: 0.14391816633528676, fair: 0.3440811772483088\n",
      "Epoch: 34, loss: 0.14082763837329274, fair: 0.3440811772483088\n",
      "Epoch: 35, loss: 0.13778507817408134, fair: 0.3440811772483088\n",
      "Epoch: 36, loss: 0.13394721587156427, fair: 0.3440811772483088\n",
      "Epoch: 37, loss: 0.13104917900315646, fair: 0.3440811772483088\n",
      "Epoch: 38, loss: 0.12840454280376434, fair: 0.3440811772483088\n",
      "Epoch: 39, loss: 0.12542162028879955, fair: 0.3440811772483088\n",
      "Epoch: 40, loss: 0.12317344551456386, fair: 0.3440811772483088\n",
      "Epoch: 41, loss: 0.12349426209412773, fair: 0.3440811772483088\n",
      "Epoch: 42, loss: 0.12211511520476177, fair: 0.3440811772483088\n",
      "Epoch: 43, loss: 0.12079247806606622, fair: 0.3440811772483088\n",
      "Epoch: 44, loss: 0.11831798712755072, fair: 0.3440811772483088\n",
      "Epoch: 45, loss: 0.11529308087353049, fair: 0.3440811772483088\n",
      "Epoch: 46, loss: 0.11282219748044836, fair: 0.3440811772483088\n",
      "Epoch: 47, loss: 0.11013243743057909, fair: 0.3440811772483088\n",
      "Epoch: 48, loss: 0.10811105059652493, fair: 0.3440811772483088\n",
      "Epoch: 49, loss: 0.10545272829717603, fair: 0.3440811772483088\n",
      "Epoch: 50, loss: 0.10614671732844977, fair: 0.3440811772483088\n",
      "Epoch: 51, loss: 0.10963406614188491, fair: 0.3440811772483088\n",
      "Epoch: 52, loss: 0.10455393128353974, fair: 0.3440811772483088\n",
      "Epoch: 53, loss: 0.11338809381785064, fair: 0.3440811772483088\n",
      "Epoch: 54, loss: 0.11376273262089696, fair: 0.3440811772483088\n",
      "Epoch: 55, loss: 0.10984768924014322, fair: 0.3440811772483088\n",
      "Epoch: 56, loss: 0.1044268976768543, fair: 0.3440811772483088\n",
      "Epoch: 57, loss: 0.10345342716780202, fair: 0.3440811772483088\n",
      "Epoch: 58, loss: 0.09417557844827915, fair: 0.3440811772483088\n",
      "Epoch: 59, loss: 0.09350219137709716, fair: 0.3440811772483088\n",
      "Epoch: 60, loss: 0.08979327909905335, fair: 0.3440811772483088\n",
      "Epoch: 61, loss: 0.09110615844356602, fair: 0.3440811772483088\n",
      "Epoch: 62, loss: 0.08699682873898539, fair: 0.3440811772483088\n",
      "Epoch: 63, loss: 0.08450555215621816, fair: 0.3440811772483088\n",
      "Epoch: 64, loss: 0.08587500774141016, fair: 0.3440811772483088\n",
      "Epoch: 65, loss: 0.0919346843043278, fair: 0.3440811772483088\n",
      "Epoch: 66, loss: 0.0872608380970256, fair: 0.3440811772483088\n",
      "Epoch: 67, loss: 0.08739064389775539, fair: 0.3440811772483088\n",
      "Epoch: 68, loss: 0.09043391490804738, fair: 0.3440811772483088\n",
      "Epoch: 69, loss: 0.08295345522206406, fair: 0.3440811772483088\n",
      "Epoch: 70, loss: 0.0877191886305809, fair: 0.3440811772483088\n",
      "Epoch: 71, loss: 0.08858945462210426, fair: 0.3440811772483088\n",
      "Epoch: 72, loss: 0.08977059400801, fair: 0.3440811772483088\n",
      "Epoch: 73, loss: 0.08987456082270064, fair: 0.3440811772483088\n",
      "Epoch: 74, loss: 0.08685900520147949, fair: 0.3440811772483088\n",
      "Epoch: 75, loss: 0.08343055011897252, fair: 0.3440811772483088\n",
      "Epoch: 76, loss: 0.08031825985887955, fair: 0.3440811772483088\n",
      "Epoch: 77, loss: 0.07631444319568831, fair: 0.3440811772483088\n",
      "Epoch: 78, loss: 0.07925770589760665, fair: 0.3440811772483088\n",
      "Epoch: 79, loss: 0.07540494992301382, fair: 0.3440811772483088\n",
      "Epoch: 80, loss: 0.08714523831831998, fair: 0.3440811772483088\n",
      "Epoch: 81, loss: 0.07824495956301689, fair: 0.3440811772483088\n",
      "Epoch: 82, loss: 0.07608943342648704, fair: 0.3440811772483088\n",
      "Epoch: 83, loss: 0.07513779060080134, fair: 0.3440811772483088\n",
      "Epoch: 84, loss: 0.07316375161553251, fair: 0.3440811772483088\n",
      "Epoch: 85, loss: 0.07000740131941335, fair: 0.3440811772483088\n",
      "Epoch: 86, loss: 0.06897154214053318, fair: 0.3440811772483088\n",
      "Epoch: 87, loss: 0.07162133218913243, fair: 0.3440811772483088\n",
      "Epoch: 88, loss: 0.06672834818219317, fair: 0.3440811772483088\n",
      "Epoch: 89, loss: 0.0662862968085141, fair: 0.3440811772483088\n",
      "Epoch: 90, loss: 0.06630420048945937, fair: 0.3440811772483088\n",
      "Epoch: 91, loss: 0.0675410704366092, fair: 0.3440811772483088\n",
      "Epoch: 92, loss: 0.0654628990924564, fair: 0.3440811772483088\n",
      "Epoch: 93, loss: 0.0637110064127322, fair: 0.3440811772483088\n",
      "Epoch: 94, loss: 0.06291937289823746, fair: 0.3440811772483088\n",
      "Epoch: 95, loss: 0.06270722451395001, fair: 0.3440811772483088\n",
      "Epoch: 96, loss: 0.06589257939365403, fair: 0.3440811772483088\n",
      "Epoch: 97, loss: 0.06886284711032079, fair: 0.3440811772483088\n",
      "Epoch: 98, loss: 0.07163212498457268, fair: 0.3440811772483088\n",
      "Epoch: 99, loss: 0.07097387360087756, fair: 0.3440811772483088\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model_uncon.parameters())\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_feat, batch_sens, batch_label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model_uncon(batch_feat)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logit, batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch: {epoch}, loss: {np.mean(losses)}, fair: {np.mean(fair)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd521be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3554, 0.5108], dtype=torch.float64, grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "preds = torch.nn.functional.sigmoid(model_uncon(features_train))\n",
    "pr = PositiveRate()\n",
    "pr(preds, sens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c4fa0",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbc50a",
   "metadata": {},
   "source": [
    "Now let us train the same model with one of the **constrained** training algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1)\n",
    ")\n",
    "\n",
    "optimizer = SSLALM(\n",
    "    params=model_con.parameters(),\n",
    "    m=1,\n",
    "    lr=0.05,\n",
    "    dual_lr=0.05\n",
    ")\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.6634813547134399, constraints: [0.00981526], dual: [0.]\n",
      "Epoch: 1, loss: 0.5301157832145691, constraints: [0.07230375], dual: [0.00784478]\n",
      "Epoch: 2, loss: 0.42276254296302795, constraints: [0.16573656], dual: [0.13601923]\n",
      "Epoch: 3, loss: 0.3991071283817291, constraints: [0.1685044], dual: [0.2906894]\n",
      "Epoch: 4, loss: 0.3933959901332855, constraints: [0.14941347], dual: [0.4277189]\n",
      "Epoch: 5, loss: 0.39016368985176086, constraints: [0.13931526], dual: [0.51401454]\n",
      "Epoch: 6, loss: 0.3857656419277191, constraints: [0.13841809], dual: [0.59241724]\n",
      "Epoch: 7, loss: 0.3842432200908661, constraints: [0.12440836], dual: [0.6724649]\n",
      "Epoch: 8, loss: 0.38192689418792725, constraints: [0.12657558], dual: [0.740716]\n",
      "Epoch: 9, loss: 0.37957674264907837, constraints: [0.1206379], dual: [0.81986153]\n",
      "Epoch: 10, loss: 0.3767065703868866, constraints: [0.11726112], dual: [0.8704299]\n",
      "Epoch: 11, loss: 0.37544986605644226, constraints: [0.11596639], dual: [0.9260662]\n",
      "Epoch: 12, loss: 0.3748477101325989, constraints: [0.12131293], dual: [0.9856643]\n",
      "Epoch: 13, loss: 0.3750757575035095, constraints: [0.11961197], dual: [1.0589486]\n",
      "Epoch: 14, loss: 0.3733205795288086, constraints: [0.10713772], dual: [1.1107854]\n",
      "Epoch: 15, loss: 0.372745543718338, constraints: [0.11136617], dual: [1.1735306]\n",
      "Epoch: 16, loss: 0.3686768114566803, constraints: [0.11345174], dual: [1.2466012]\n",
      "Epoch: 17, loss: 0.3669039011001587, constraints: [0.10496456], dual: [1.3006356]\n",
      "Epoch: 18, loss: 0.36821532249450684, constraints: [0.10386657], dual: [1.3511075]\n",
      "Epoch: 19, loss: 0.3679036498069763, constraints: [0.09872209], dual: [1.4035592]\n",
      "Epoch: 20, loss: 0.36670053005218506, constraints: [0.10176969], dual: [1.4401655]\n",
      "Epoch: 21, loss: 0.3635801076889038, constraints: [0.09899759], dual: [1.4880503]\n",
      "Epoch: 22, loss: 0.36080652475357056, constraints: [0.09958655], dual: [1.5224601]\n",
      "Epoch: 23, loss: 0.3619651794433594, constraints: [0.10362873], dual: [1.5798225]\n",
      "Epoch: 24, loss: 0.35832127928733826, constraints: [0.10206651], dual: [1.6336251]\n",
      "Epoch: 25, loss: 0.3583712875843048, constraints: [0.10634474], dual: [1.6708288]\n",
      "Epoch: 26, loss: 0.35545405745506287, constraints: [0.10311586], dual: [1.6993207]\n",
      "Epoch: 27, loss: 0.3524969220161438, constraints: [0.11083485], dual: [1.7332113]\n",
      "Epoch: 28, loss: 0.3567010164260864, constraints: [0.09684263], dual: [1.7772151]\n",
      "Epoch: 29, loss: 0.3511161506175995, constraints: [0.09534855], dual: [1.8086169]\n",
      "Epoch: 30, loss: 0.34660497307777405, constraints: [0.10470875], dual: [1.8403552]\n",
      "Epoch: 31, loss: 0.34388434886932373, constraints: [0.09955056], dual: [1.8742104]\n",
      "Epoch: 32, loss: 0.3483128547668457, constraints: [0.10261702], dual: [1.9221027]\n",
      "Epoch: 33, loss: 0.34841060638427734, constraints: [0.09548213], dual: [1.959423]\n",
      "Epoch: 34, loss: 0.3498260974884033, constraints: [0.10179964], dual: [1.9928424]\n",
      "Epoch: 35, loss: 0.337911993265152, constraints: [0.09579523], dual: [2.0300496]\n",
      "Epoch: 36, loss: 0.3413286805152893, constraints: [0.09448567], dual: [2.0564258]\n",
      "Epoch: 37, loss: 0.33702611923217773, constraints: [0.10406455], dual: [2.0750768]\n",
      "Epoch: 38, loss: 0.3396722078323364, constraints: [0.10011654], dual: [2.1245744]\n",
      "Epoch: 39, loss: 0.33604666590690613, constraints: [0.09529416], dual: [2.1556573]\n",
      "Epoch: 40, loss: 0.33464646339416504, constraints: [0.09592812], dual: [2.198374]\n",
      "Epoch: 41, loss: 0.3410811722278595, constraints: [0.10021026], dual: [2.2326095]\n",
      "Epoch: 42, loss: 0.33117082715034485, constraints: [0.09660894], dual: [2.2578764]\n",
      "Epoch: 43, loss: 0.3329881727695465, constraints: [0.09311062], dual: [2.2914667]\n",
      "Epoch: 44, loss: 0.3251921534538269, constraints: [0.10534754], dual: [2.32508]\n",
      "Epoch: 45, loss: 0.3255162537097931, constraints: [0.0952514], dual: [2.3517275]\n",
      "Epoch: 46, loss: 0.3273082673549652, constraints: [0.10133083], dual: [2.3989048]\n",
      "Epoch: 47, loss: 0.3183152675628662, constraints: [0.10026248], dual: [2.4249368]\n",
      "Epoch: 48, loss: 0.32804635167121887, constraints: [0.09744305], dual: [2.4557638]\n",
      "Epoch: 49, loss: 0.320365846157074, constraints: [0.08977536], dual: [2.489659]\n",
      "Epoch: 50, loss: 0.326649934053421, constraints: [0.10195145], dual: [2.5155277]\n",
      "Epoch: 51, loss: 0.31737080216407776, constraints: [0.10596629], dual: [2.576275]\n",
      "Epoch: 52, loss: 0.32927146553993225, constraints: [0.09780074], dual: [2.6209052]\n",
      "Epoch: 53, loss: 0.3214603662490845, constraints: [0.10327071], dual: [2.6480982]\n",
      "Epoch: 54, loss: 0.32331421971321106, constraints: [0.09602478], dual: [2.6962652]\n",
      "Epoch: 55, loss: 0.32492005825042725, constraints: [0.0902909], dual: [2.7339685]\n",
      "Epoch: 56, loss: 0.3203880786895752, constraints: [0.09413392], dual: [2.7648046]\n",
      "Epoch: 57, loss: 0.32611390948295593, constraints: [0.10255422], dual: [2.8072052]\n",
      "Epoch: 58, loss: 0.32883957028388977, constraints: [0.09523521], dual: [2.8325162]\n",
      "Epoch: 59, loss: 0.3156088590621948, constraints: [0.09571523], dual: [2.862566]\n",
      "Epoch: 60, loss: 0.31795138120651245, constraints: [0.09786171], dual: [2.8942323]\n",
      "Epoch: 61, loss: 0.31953200697898865, constraints: [0.09694803], dual: [2.9184098]\n",
      "Epoch: 62, loss: 0.31506985425949097, constraints: [0.08874872], dual: [2.959698]\n",
      "Epoch: 63, loss: 0.3133557140827179, constraints: [0.09037021], dual: [2.9816096]\n",
      "Epoch: 64, loss: 0.3061583936214447, constraints: [0.09489188], dual: [3.0248244]\n",
      "Epoch: 65, loss: 0.3133995831012726, constraints: [0.09887273], dual: [3.0533798]\n",
      "Epoch: 66, loss: 0.30961355566978455, constraints: [0.09390114], dual: [3.0843012]\n",
      "Epoch: 67, loss: 0.31241828203201294, constraints: [0.09081028], dual: [3.1139123]\n",
      "Epoch: 68, loss: 0.2921217083930969, constraints: [0.0934338], dual: [3.1327822]\n",
      "Epoch: 69, loss: 0.2998720705509186, constraints: [0.09079777], dual: [3.1524498]\n",
      "Epoch: 70, loss: 0.29348570108413696, constraints: [0.09638616], dual: [3.181954]\n",
      "Epoch: 71, loss: 0.30123037099838257, constraints: [0.09357067], dual: [3.2126002]\n",
      "Epoch: 72, loss: 0.3102804124355316, constraints: [0.10114506], dual: [3.2360058]\n",
      "Epoch: 73, loss: 0.29564520716667175, constraints: [0.10099829], dual: [3.2703574]\n",
      "Epoch: 74, loss: 0.3096795380115509, constraints: [0.09413254], dual: [3.3082557]\n",
      "Epoch: 75, loss: 0.3053491413593292, constraints: [0.09390692], dual: [3.3608687]\n",
      "Epoch: 76, loss: 0.29851698875427246, constraints: [0.09662108], dual: [3.388315]\n",
      "Epoch: 77, loss: 0.3045424818992615, constraints: [0.08941362], dual: [3.4214158]\n",
      "Epoch: 78, loss: 0.2917429506778717, constraints: [0.08331759], dual: [3.433659]\n",
      "Epoch: 79, loss: 0.3026694059371948, constraints: [0.09574226], dual: [3.4634416]\n",
      "Epoch: 80, loss: 0.28725239634513855, constraints: [0.08716873], dual: [3.4975734]\n",
      "Epoch: 81, loss: 0.2944769561290741, constraints: [0.09142475], dual: [3.515856]\n",
      "Epoch: 82, loss: 0.2846989929676056, constraints: [0.09801792], dual: [3.5310917]\n",
      "Epoch: 83, loss: 0.30974075198173523, constraints: [0.09353821], dual: [3.5649111]\n",
      "Epoch: 84, loss: 0.2943311929702759, constraints: [0.09702248], dual: [3.6034205]\n",
      "Epoch: 85, loss: 0.2969624400138855, constraints: [0.09057375], dual: [3.6424174]\n",
      "Epoch: 86, loss: 0.28384044766426086, constraints: [0.09706003], dual: [3.6738133]\n",
      "Epoch: 87, loss: 0.2957431375980377, constraints: [0.0994367], dual: [3.7088056]\n",
      "Epoch: 88, loss: 0.30304306745529175, constraints: [0.08635722], dual: [3.739778]\n",
      "Epoch: 89, loss: 0.28921520709991455, constraints: [0.08896884], dual: [3.7812693]\n",
      "Epoch: 90, loss: 0.3063935935497284, constraints: [0.08899227], dual: [3.8074126]\n",
      "Epoch: 91, loss: 0.28874436020851135, constraints: [0.09355739], dual: [3.8310218]\n",
      "Epoch: 92, loss: 0.30015596747398376, constraints: [0.09688847], dual: [3.8676379]\n",
      "Epoch: 93, loss: 0.2935751676559448, constraints: [0.09102925], dual: [3.8887537]\n",
      "Epoch: 94, loss: 0.2760356366634369, constraints: [0.0932366], dual: [3.900644]\n",
      "Epoch: 95, loss: 0.2840070128440857, constraints: [0.09167038], dual: [3.9189157]\n",
      "Epoch: 96, loss: 0.2940511405467987, constraints: [0.08969043], dual: [3.943329]\n",
      "Epoch: 97, loss: 0.2895054221153259, constraints: [0.09602472], dual: [3.9642406]\n",
      "Epoch: 98, loss: 0.28316062688827515, constraints: [0.08582042], dual: [3.9874895]\n",
      "Epoch: 99, loss: 0.2735463082790375, constraints: [0.09454797], dual: [4.0088825]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "        fair_constraint = torch.max(fair_loss -  fair_crit_bound, torch.zeros(1))\n",
    "        fair_constraint.backward()\n",
    "        optimizer.dual_step(0, c_val=fair_constraint)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        c_log.append([fair_loss.detach().numpy()])\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "        # calculate loss and grad\n",
    "        batch_output = model_con(batch_input)\n",
    "        loss = criterion(batch_output, batch_label)\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993899e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4479, 0.4525], dtype=torch.float64, grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "preds = torch.nn.functional.sigmoid(model_con(features_train))\n",
    "pr = PositiveRate()\n",
    "pr(preds, sens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5a57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0101, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair_criterion(model_con(features_train), sens_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
