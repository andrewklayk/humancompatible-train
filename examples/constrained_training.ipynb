{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efc20cc",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use the **constrained training algorithms** implemented in this toolkit, designed to augment your normal **PyTorch** training routine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ccbc7",
   "metadata": {},
   "source": [
    "The algorithms implemented in the **humancompatible.train.optim** subpackage share a similar idea.\n",
    "\n",
    "1. Define your PyTorch optimizer like you would normally. Let's call it`opt`.\n",
    "2. Define a dual \"optimizer\" from`humancompatible.train.optim`; it will keep track of and update the dual variables. Let's call it`dual`.\n",
    "3. In the training loop:\n",
    "    - Compute the constraints and the objective function (loss);\n",
    "    - Calculate the Lagrangian and update the dual variables using `dual.forward_update` method;\n",
    "    - Run a `backward` pass through the Lagrangian (instead of the loss);\n",
    "    - Run `opt.step`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f86ab",
   "metadata": {},
   "source": [
    "Let's train a simple classification model, putting a constraint on the norm of each layer's parameters.\n",
    "\n",
    "In the canonical form, the algorithm expects equality constraints that are equal to 0; however, we can easily transform arbitrary inequality constraints to that form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome, generate_categories\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# load folktables data\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(\n",
    "    features=ACSIncome.features, definition_df=definition_df\n",
    ")\n",
    "df_feat, df_labels, _ = ACSIncome.df_to_pandas(\n",
    "    acs_data, categories=categories, dummies=True\n",
    ")\n",
    "\n",
    "sens_cols = [\"SEX_Female\", \"SEX_Male\"]\n",
    "features = df_feat.drop(columns=sens_cols).to_numpy(dtype=float)\n",
    "groups = df_feat[sens_cols].to_numpy(dtype=float)\n",
    "labels = df_labels.to_numpy(dtype=float)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    features, labels, groups, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# further split test into val and test\n",
    "X_val, X_test, y_val, y_test, groups_val, groups_test = train_test_split(\n",
    "    X_test, y_test, groups_test, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_train = torch.tensor(X_train)\n",
    "labels_train = torch.tensor(y_train)\n",
    "sens_train = torch.tensor(groups_train)\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, labels_train)\n",
    "dataset_train_s = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# make val and test into pytorch datasets\n",
    "features_val = torch.tensor(X_val)\n",
    "labels_val = torch.tensor(y_val)\n",
    "sens_val = torch.tensor(groups_val)\n",
    "dataset_val = torch.utils.data.TensorDataset(features_val, labels_val)\n",
    "dataset_val_s = torch.utils.data.TensorDataset(features_val, sens_val, labels_val)\n",
    "\n",
    "features_test = torch.tensor(X_test)\n",
    "labels_test = torch.tensor(y_test)\n",
    "sens_test = torch.tensor(groups_test)\n",
    "dataset_test = torch.utils.data.TensorDataset(features_test, labels_test)\n",
    "dataset_test_s = torch.utils.data.TensorDataset(features_test, sens_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418398d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from humancompatible.train.dual_optim import ALM, PBM\n",
    "from humancompatible.train.dual_optim import MoreauEnvelope\n",
    "from torch.nn import Sequential\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=False)\n",
    "\n",
    "hsize1 = 12\n",
    "hsize2 = 12\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "optimizer = MoreauEnvelope(\n",
    "    torch.optim.Adam(model.parameters(), lr=0.001),\n",
    ")\n",
    "\n",
    "m = len(list(model.parameters()))\n",
    "dual = ALM(m=6, lr=0.1, momentum=0.5, dampening=0.5)\n",
    "\n",
    "constraint_bounds = [1.0] * m\n",
    "epochs = 10\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# do a dummy backward pass\n",
    "model(dataset_train[0][0]).backward()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a5f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.4742393715781231, constraints: [array(0.99822405), array(0.99977582), array(0.98370329), array(0.99975662), array(1.00032864), array(0.27740726)], dual: tensor([0.8365, 0.1397, 4.4786, 0.0683, 0.3129, 0.0000])\n",
      "Epoch: 1, loss: 0.46727603619805774, constraints: [array(0.99974732), array(0.99991013), array(0.97601688), array(0.99970829), array(0.99954425), array(0.4327701)], dual: tensor([0.9207, 0.1785, 4.4865, 0.0948, 0.3925, 0.0000])\n",
      "Epoch: 2, loss: 0.4497933717844898, constraints: [array(1.0006221), array(0.99918517), array(0.98203093), array(0.99990305), array(0.99923101), array(0.61347269)], dual: tensor([0.9860, 0.2028, 4.4930, 0.1103, 0.4512, 0.0000])\n",
      "Epoch: 3, loss: 0.4470780470827978, constraints: [array(0.99765493), array(0.99988715), array(0.98583056), array(0.99938058), array(0.99993007), array(0.78371915)], dual: tensor([1.0353, 0.2229, 4.4991, 0.1206, 0.5010, 0.0000])\n",
      "Epoch: 4, loss: 0.43683666477725475, constraints: [array(0.99835512), array(0.99917183), array(0.99356798), array(0.99949289), array(0.99794654), array(0.91807297)], dual: tensor([1.0776, 0.2385, 4.5045, 0.1295, 0.5472, 0.0000])\n",
      "Epoch: 5, loss: 0.43186674605858555, constraints: [array(0.99761295), array(0.99949934), array(0.98916314), array(0.99764843), array(0.99957463), array(0.99501021)], dual: tensor([1.1095, 0.2512, 4.5088, 0.1427, 0.5906, 0.0336])\n",
      "Epoch: 6, loss: 0.43059447787323946, constraints: [array(0.99933513), array(0.99953488), array(0.98886307), array(0.99932875), array(1.00015475), array(0.99463431)], dual: tensor([1.1385, 0.2630, 4.5130, 0.1563, 0.6301, 0.0683])\n",
      "Epoch: 7, loss: 0.4308959203404695, constraints: [array(0.99926659), array(0.99955522), array(0.99725459), array(0.99995628), array(1.00038809), array(0.99466266)], dual: tensor([1.1670, 0.2732, 4.5172, 0.1678, 0.6636, 0.0835])\n",
      "Epoch: 8, loss: 0.4317110832904212, constraints: [array(0.99781005), array(0.99995287), array(0.98937758), array(0.99999825), array(0.99894296), array(0.99482352)], dual: tensor([1.1946, 0.2833, 4.5215, 0.1783, 0.6979, 0.0945])\n",
      "Epoch: 9, loss: 0.4341012799124808, constraints: [array(0.99314479), array(0.99979048), array(0.98955534), array(0.99868508), array(0.99971304), array(0.9940136)], dual: tensor([1.2235, 0.2932, 4.5258, 0.1886, 0.7281, 0.1040])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    slack_log = []\n",
    "    duals_log = []\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        c_log.append([])\n",
    "        constraints = []\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            norm = torch.linalg.norm(param)\n",
    "            # convert constraint to equality\n",
    "            norm_viol = torch.max(norm - constraint_bounds[i], torch.zeros(1, requires_grad=True))\n",
    "            constraints.append(norm_viol)\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "\n",
    "        constraints = torch.cat(constraints)\n",
    "        batch_output = model(batch_input)\n",
    "        bce_loss = criterion(batch_output, batch_label)\n",
    "        \n",
    "        lag_loss = dual.forward_update(bce_loss, constraints)\n",
    "        lag_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_log.append(bce_loss.detach().numpy())\n",
    "        duals_log.append(dual.duals)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {loss_log[-1]}, \"\n",
    "        f\"constraints: {c_log[-1]}, \"\n",
    "        f\"dual: {duals_log[-1]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4e546",
   "metadata": {},
   "source": [
    "The model is now trained subject to the constraints we set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06f697",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f190b",
   "metadata": {},
   "source": [
    "It is also possible to train subject to **stochastic constraints**. One of the main use-cases for that is **fairness**. Let's now train a fairness-constrained model on the `folktables` dataset.\n",
    "\n",
    "To track the \"fairness\" of our model, we will track **Positive Rate** across groups. To calculate it, we will use the `fairret` package and its `NormLoss`.\n",
    "The latter calculates the ratio between the value of a statistic for each group and the overall value: $\\sum_{s\\in S}{|1-\\frac{f(\\theta, X_s, y_s)}{f(\\theta, X, y)}|}$, where $S$ is the set of groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95cf50",
   "metadata": {},
   "source": [
    "To make sure each batch contains representatives of each protected group, we can use the BalancedBatchSampler from the `fairness.utils` subpackage - a custom PyTorch `Sampler` which yields an equal number of samples from each subgroup in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "fair_sampler = BalancedBatchSampler(group_onehot=sens_train, batch_size=128)\n",
    "loader = torch.utils.data.DataLoader(dataset_train_s, sampler=fair_sampler)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "\n",
    "hsize1 = 32\n",
    "hsize2 = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dae742",
   "metadata": {},
   "source": [
    "First, let us train an unconstrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.3968636369462622 / 0.40470909243747405, constraint: 0.18165649822563323 / 0.18781794489482784\n",
      "Epoch: 1, loss: 0.3906023357402648 / 0.40295611185247887, constraint: 0.16304437408194883 / 0.16775616049726738\n",
      "Epoch: 2, loss: 0.3827542259947103 / 0.3991090992544509, constraint: 0.1830651758212012 / 0.18514395786567173\n",
      "Epoch: 3, loss: 0.3790003210896969 / 0.3978286528551326, constraint: 0.17819499988186338 / 0.179512346701729\n",
      "Epoch: 4, loss: 0.379721713846987 / 0.40115765726758496, constraint: 0.17223594705362688 / 0.17338911341025487\n",
      "Epoch: 5, loss: 0.37080572509470167 / 0.39682120662030707, constraint: 0.18351866706274733 / 0.18596974220973295\n",
      "Epoch: 6, loss: 0.3671832809863401 / 0.40050300410026235, constraint: 0.1963664795819673 / 0.19770557394897526\n",
      "Epoch: 7, loss: 0.36277761952639254 / 0.40109646608534527, constraint: 0.19989953869687838 / 0.19996672677926564\n",
      "Epoch: 8, loss: 0.36184892644534755 / 0.4030079680737153, constraint: 0.1893552547808227 / 0.192681738889383\n",
      "Epoch: 9, loss: 0.3585031108671064 / 0.3989385345132682, constraint: 0.19154889526015584 / 0.1900389467862611\n",
      "Epoch: 10, loss: 0.3559381718837889 / 0.4016768979926585, constraint: 0.18881007631268887 / 0.18745438891117172\n",
      "Epoch: 11, loss: 0.35192446610917255 / 0.40359689153160455, constraint: 0.19468359732950846 / 0.19808325768122226\n",
      "Epoch: 12, loss: 0.35632652535123543 / 0.40691562329231035, constraint: 0.18593917239521762 / 0.18661521172309659\n",
      "Epoch: 13, loss: 0.34591016159221794 / 0.408002102362776, constraint: 0.19355409736717277 / 0.19226542167436855\n",
      "Epoch: 14, loss: 0.352230566824132 / 0.4287683479503198, constraint: 0.1884807801102547 / 0.18611800530949163\n",
      "Epoch: 15, loss: 0.34034264841872475 / 0.4139345306904318, constraint: 0.19505705520969863 / 0.1935254350387311\n",
      "Epoch: 16, loss: 0.3431245394342468 / 0.40640181597283337, constraint: 0.19522613973617675 / 0.19378093966791543\n",
      "Epoch: 17, loss: 0.3389019073074307 / 0.4117369790509885, constraint: 0.1878866717320109 / 0.18339622559534097\n",
      "Epoch: 18, loss: 0.33613360016292926 / 0.42847728779546296, constraint: 0.2088573343596991 / 0.20414923803767349\n",
      "Epoch: 19, loss: 0.335393091872077 / 0.4276684386745826, constraint: 0.19956345828596556 / 0.19296548138274838\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import Sequential\n",
    "\n",
    "model_uncon = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "optimizer = Adam(model_uncon.parameters())\n",
    "\n",
    "train_losses = []\n",
    "train_f = []\n",
    "val_losses =[]\n",
    "val_f = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_feat, _, batch_label in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model_uncon(batch_feat)\n",
    "        loss = criterion(logit, batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    # logging\n",
    "    with torch.no_grad():\n",
    "        logits_train = model_uncon(features_train)\n",
    "        loss_train = criterion(logits_train, labels_train)\n",
    "        fair_train = fair_criterion(model_uncon(features_train), sens_train)\n",
    "        logits_val = model_uncon(features_val)\n",
    "        loss_val = criterion(logits_val, labels_val)\n",
    "        fair_val = fair_criterion(model_uncon(features_val), sens_val)\n",
    "        train_losses.append(loss_train)\n",
    "        train_f.append(fair_train)\n",
    "        val_losses.append(loss_val)\n",
    "        val_f.append(fair_val)\n",
    "    print(f\"Epoch: {epoch}, loss: {loss_train} / {loss_val}, constraint: {fair_train} / {fair_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd521be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate by group:\n",
      "Train: [0.38232849 0.46760742]\n",
      "Test: [0.38632841 0.46746498]\n"
     ]
    }
   ],
   "source": [
    "pr = PositiveRate()\n",
    "print('Positive rate by group:')\n",
    "preds = torch.nn.functional.sigmoid(model_uncon(features_train))\n",
    "print(f'Train: {pr(preds, sens_train).detach().numpy()}')\n",
    "preds_test = torch.nn.functional.sigmoid(model_uncon(features_test))\n",
    "print(f'Test: {pr(preds_test, sens_test).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c7803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "Train: 0.335393091872077\n",
      "Test: 0.4385469714010663\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\")\n",
    "print(f'Train: {criterion(model_uncon(features_train), labels_train).detach().numpy()}')\n",
    "print(f'Test: {criterion(model_uncon(features_test), labels_test).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c4fa0",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dbc50a",
   "metadata": {},
   "source": [
    "We see that unconstrained optimization leads to a large **discrepancy in the positive rate**. Say we want the fairret term to be no greater than 0.1. We can enforce this constraint with the **constrained** training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e8b33",
   "metadata": {},
   "source": [
    "First, we will try the **Augmented Lagrangian**. Natively, it only works with equality constraints, so we will introduce **slack variables**.\n",
    "\n",
    "A note: stochastic constrained optimization algorithm benefit greatly from smoothing. We provide the `MoreauEnvelope` class that adds an L2 smoothing term to the model's loss function gradient (without spending any resources during the backward call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.optim import Adam\n",
    "from humancompatible.train.dual_optim import ALM, MoreauEnvelope, PBM\n",
    "\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "# Define data and optimizers\n",
    "optimizer = MoreauEnvelope(Adam(model_con.parameters(), lr=0.001))\n",
    "dual = ALM(m=1, lr=0.01)\n",
    "\n",
    "slack_vars = torch.zeros(1, requires_grad=True)\n",
    "optimizer.add_param_group({\"params\":slack_vars})\n",
    "\n",
    "# fairness constraint bound\n",
    "fair_crit_bound = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2af16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.40394780530396024 / 0.410732146277, fairret loss: 0.05203240917074947 / 0.061091899135758077, L: 0.41707331541453646\n",
      "Epoch: 1, loss: 0.4040077916559369 / 0.41456315954936923, fairret loss: 0.044065317615095134 / 0.050434035002550726, L: 0.4239857269973955\n",
      "Epoch: 2, loss: 0.4042374101783031 / 0.41510828319779564, fairret loss: 0.03651663806609562 / 0.043627017051205086, L: 0.42555957964784896\n",
      "Epoch: 3, loss: 0.4017089130338191 / 0.41375565931964703, fairret loss: 0.0392035864604946 / 0.04589470945646934, L: 0.4288281278418492\n",
      "Epoch: 4, loss: 0.40321648229099527 / 0.41545547866675675, fairret loss: 0.02132574583257052 / 0.030285629561043748, L: 0.4189467696237422\n",
      "Epoch: 5, loss: 0.4018237916265805 / 0.4151791621663035, fairret loss: 0.03185733342310659 / 0.040023449363411956, L: 0.4268584228473389\n",
      "Epoch: 6, loss: 0.4062603489590199 / 0.4195608239498349, fairret loss: 0.01627218377530759 / 0.024136046147144485, L: 0.4201776068471261\n",
      "Epoch: 7, loss: 0.3984736191478028 / 0.4152037955750281, fairret loss: 0.016683202727259205 / 0.023548749450414963, L: 0.41350769974973234\n",
      "Epoch: 8, loss: 0.39906041255488683 / 0.41611712996819283, fairret loss: 0.025784149291487535 / 0.03637654785990829, L: 0.4230737656502572\n",
      "Epoch: 9, loss: 0.40302156329719824 / 0.42036047345697386, fairret loss: 0.00633548022608843 / 0.01498414813491078, L: 0.4090917843519207\n",
      "Epoch: 10, loss: 0.3981878677233399 / 0.41573904562710057, fairret loss: 0.02632781674911333 / 0.0341674086731274, L: 0.4244732733462705\n",
      "Epoch: 11, loss: 0.4031913028066434 / 0.42151046271495174, fairret loss: 0.012315675910210322 / 0.022720048967797335, L: 0.41578637477369407\n",
      "Epoch: 12, loss: 0.40019025617365356 / 0.41720849672460925, fairret loss: 0.01952376607057793 / 0.030119013481632706, L: 0.4203932365855678\n",
      "Epoch: 13, loss: 0.396078232836599 / 0.4148838602873273, fairret loss: 0.03290339855979896 / 0.04285659419584431, L: 0.42950088497510036\n",
      "Epoch: 14, loss: 0.3982000695158776 / 0.4169535490388576, fairret loss: 0.028858249421016335 / 0.04112541874384734, L: 0.42856168228389757\n",
      "Epoch: 15, loss: 0.4004613299404268 / 0.4196002493851793, fairret loss: 0.006036901192016608 / 0.018698247573703086, L: 0.4068601731553953\n",
      "Epoch: 16, loss: 0.3963496688148315 / 0.4164223225469526, fairret loss: 0.008420514419971159 / 0.02087923200927, L: 0.4053515653363285\n",
      "Epoch: 17, loss: 0.39814953340405024 / 0.4188050679509244, fairret loss: 0.013932265107501784 / 0.023335683129890095, L: 0.4133781952952947\n",
      "Epoch: 18, loss: 0.3940945737631954 / 0.4162085280273669, fairret loss: 0.01927578554419984 / 0.029586328688681718, L: 0.4150910499643744\n",
      "Epoch: 19, loss: 0.3926763912970974 / 0.41462712587813566, fairret loss: 0.023312999588212868 / 0.03274645251012909, L: 0.4184615183265685\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_f = []\n",
    "val_losses =[]\n",
    "val_f = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_input, batch_sens, batch_label in loader:\n",
    "        # do forward pass\n",
    "        batch_out = model_con(batch_input)\n",
    "        loss = criterion(batch_out, batch_label)\n",
    "\n",
    "        # evaluate fairness criterion (constraint)\n",
    "        fair_loss = fair_criterion(batch_out.squeeze(0), batch_sens.squeeze(0))\n",
    "        fair_constraint = fair_loss - fair_crit_bound + slack_vars[0]\n",
    "        \n",
    "        # calculate lagrangian, update dual variables\n",
    "        lagrangian = dual.forward_update(loss, fair_constraint.unsqueeze(0))\n",
    "\n",
    "        # gradient, optimizer step\n",
    "        lagrangian.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # set slacks to be non-negative\n",
    "        with torch.no_grad():\n",
    "            for s in slack_vars:\n",
    "                if s < 0:\n",
    "                    s.zero_()\n",
    "    \n",
    "    # logging\n",
    "    with torch.no_grad():\n",
    "        logits_train = model_con(features_train)\n",
    "        loss_train = torch.nn.functional.binary_cross_entropy_with_logits(logits_train, labels_train)\n",
    "        fair_train = fair_criterion(logits_train, sens_train)\n",
    "        lagr_train = dual.forward(loss_train, fair_train.unsqueeze(0))\n",
    "        logits_val = model_con(features_val)\n",
    "        loss_val = torch.nn.functional.binary_cross_entropy_with_logits(logits_val, labels_val)\n",
    "        fair_val = fair_criterion(logits_val, sens_val)\n",
    "        train_losses.append(loss_train)\n",
    "        train_f.append(fair_train)\n",
    "        val_losses.append(loss_val)\n",
    "        val_f.append(fair_val)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, loss: {loss_train} / {loss_val}, fairret loss: {fair_train} / {fair_val}, L: {lagr_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate by group:\n",
      "Train: [0.43184613 0.44203915]\n",
      "Test: [0.43123685 0.44325591]\n"
     ]
    }
   ],
   "source": [
    "pr = PositiveRate()\n",
    "print('Positive rate by group:')\n",
    "preds = torch.nn.functional.sigmoid(model_con(features_train))\n",
    "print(f'Train: {pr(preds, sens_train).detach().numpy()}')\n",
    "preds = torch.nn.functional.sigmoid(model_con(features_test))\n",
    "print(f'Test: {pr(preds, sens_test).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925911bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness criterion value:\n",
      "Train: 0.023312999588212868\n",
      "Test: 0.027467776537341893\n"
     ]
    }
   ],
   "source": [
    "print('Fairness criterion value:')\n",
    "print(f'Train: {fair_criterion(model_con(features_train), sens_train).detach().numpy()}')\n",
    "print(f'Test: {fair_criterion(model_con(features_test), sens_test).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "Train: 0.3926763912970974\n",
      "Test: 0.4190584715397698\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\")\n",
    "print(f'Train: {criterion(model_con(features_train), labels_train).detach().numpy()}')\n",
    "print(f'Test: {criterion(model_con(features_test), labels_test).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb87626",
   "metadata": {},
   "source": [
    "Curoiusly, in this case, the Augmented Lagrangian converges to a feasible local minimum not on the constraint boundary (our constraint is strictly less than the bound we set for it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b491ee",
   "metadata": {},
   "source": [
    "***\n",
    "Now, let us try the **Penalty-Barrier** method. In contrast to the ALM, it can handle inequality constraints natively, so we have no need for slacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.optim import Adam\n",
    "from humancompatible.train.dual_optim import ALM, MoreauEnvelope, PBM\n",
    "\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "# Define data and optimizers\n",
    "optimizer = MoreauEnvelope(Adam(model_con.parameters(), lr=0.001))\n",
    "\n",
    "dual = PBM(m=1, mu=0.1,\n",
    "    penalty_update='dimin',\n",
    "    lr=0.95,\n",
    "    penalty_range=(0.001, 100),\n",
    "    init_penalties=100.,\n",
    "    dual_range=(0.01, 100),\n",
    "    init_duals=0.01\n",
    ")\n",
    "\n",
    "# fairness constraint bound\n",
    "fair_crit_bound = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.3921223925491205 / 0.4014447260266624, fairret loss: 0.1637256617563726 / 0.1680401242116627, L: 0.3953740697073134\n",
      "Epoch: 1, loss: 0.3845667147927025 / 0.39732545991580837, fairret loss: 0.1558060491783967 / 0.1584644607588589, L: 0.39214009649027004\n",
      "Epoch: 2, loss: 0.38050414751603734 / 0.396364260518223, fairret loss: 0.1278090867802667 / 0.13490724066253623, L: 0.39389339022260517\n",
      "Epoch: 3, loss: 0.3792019705589975 / 0.3988856914466905, fairret loss: 0.10417460506117415 / 0.10891968051621037, L: 0.40010469344958915\n",
      "Epoch: 4, loss: 0.38003440376419956 / 0.4015328275397177, fairret loss: 0.06694725931199819 / 0.07669726760012474, L: 0.40106554710017955\n",
      "Epoch: 5, loss: 0.3805996948469289 / 0.4034603573921707, fairret loss: 0.06844513545425046 / 0.0776009863696131, L: 0.40969238675949393\n",
      "Epoch: 6, loss: 0.38326046394857244 / 0.4065245130493526, fairret loss: 0.041615551456070765 / 0.050314030491271367, L: 0.40638157283123216\n",
      "Epoch: 7, loss: 0.38385194130402844 / 0.4069235441150286, fairret loss: 0.04238863460305098 / 0.053434935020309826, L: 0.41122297788455286\n",
      "Epoch: 8, loss: 0.3838131801868628 / 0.4085745623895958, fairret loss: 0.059254266964475 / 0.07061186564948185, L: 0.4287528848307649\n",
      "Epoch: 9, loss: 0.3914419197339405 / 0.4155856655025739, fairret loss: 0.026555447542948474 / 0.036130614733314914, L: 0.4159175623451484\n",
      "Epoch: 10, loss: 0.3942388461164465 / 0.41828431450333675, fairret loss: 0.03360245833008502 / 0.043269458218500145, L: 0.4272754481891096\n",
      "Epoch: 11, loss: 0.38925518548083743 / 0.4136586551266766, fairret loss: 0.03845546413182277 / 0.051355253038453896, L: 0.42786998942285853\n",
      "Epoch: 12, loss: 0.3947970420795067 / 0.41992464509164984, fairret loss: 0.0179849653939137 / 0.029159844248293543, L: 0.41363906882726303\n",
      "Epoch: 13, loss: 0.395748762286912 / 0.4199066118565616, fairret loss: 0.017923948754580254 / 0.028475541440345142, L: 0.4164361121326346\n",
      "Epoch: 14, loss: 0.39530299670766084 / 0.4181163420541998, fairret loss: 0.029939814987222424 / 0.042949819143745294, L: 0.431251590076396\n",
      "Epoch: 15, loss: 0.39798390937088707 / 0.42016591169704093, fairret loss: 0.026506799047924967 / 0.038030087926957834, L: 0.4300571231834812\n",
      "Epoch: 16, loss: 0.3995456254122149 / 0.4231291365083533, fairret loss: 0.013130023304092964 / 0.02359813785057041, L: 0.4157479449074636\n",
      "Epoch: 17, loss: 0.40445863726104864 / 0.42827456829761934, fairret loss: 0.003480864511005932 / 0.015366521376424025, L: 0.4086906842331501\n",
      "Epoch: 18, loss: 0.39390730503858085 / 0.42013145759906356, fairret loss: 0.0026186503475691403 / 0.014981203549697897, L: 0.3970396144916277\n",
      "Epoch: 19, loss: 0.39351061534436116 / 0.4188187045893804, fairret loss: 0.0351606154662224 / 0.04590353358066479, L: 0.4364067014116283\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_f = []\n",
    "val_losses =[]\n",
    "val_f = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_input, batch_sens, batch_label in loader:\n",
    " \n",
    "        batch_out = model_con(batch_input)\n",
    "        loss = criterion(batch_out, batch_label)\n",
    "\n",
    "        fair_loss = fair_criterion(batch_out.squeeze(0), batch_sens.squeeze(0))\n",
    "        fair_constraint = fair_loss - fair_crit_bound\n",
    "\n",
    "        lagrangian = dual.forward_update(loss, fair_constraint.unsqueeze(0))\n",
    "        \n",
    "        lagrangian.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    dual.update_penalties()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_train = model_con(features_train)\n",
    "        loss_train = torch.nn.functional.binary_cross_entropy_with_logits(logits_train, labels_train)\n",
    "        fair_train = fair_criterion(logits_train, sens_train)\n",
    "        lagr_train = dual.forward(loss_train, fair_train.unsqueeze(0))\n",
    "        logits_val = model_con(features_val)\n",
    "        loss_val = torch.nn.functional.binary_cross_entropy_with_logits(logits_val, labels_val)\n",
    "        fair_val = fair_criterion(logits_val, sens_val)\n",
    "        train_losses.append(loss_train)\n",
    "        train_f.append(fair_train)\n",
    "        val_losses.append(loss_val)\n",
    "        val_f.append(fair_val)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, loss: {loss_train} / {loss_val}, fairret loss: {fair_train} / {fair_val}, L: {lagr_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11237a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate by group:\n",
      "Train: [0.43673083 0.45237684]\n",
      "Test: [0.43601392 0.45366248]\n"
     ]
    }
   ],
   "source": [
    "pr = PositiveRate()\n",
    "print('Positive rate by group:')\n",
    "preds = torch.nn.functional.sigmoid(model_con(features_train))\n",
    "print(f'Train: {pr(preds, sens_train).detach().numpy()}')\n",
    "preds = torch.nn.functional.sigmoid(model_con(features_test))\n",
    "print(f'Test: {pr(preds, sens_test).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a62ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness criterion value:\n",
      "Train: 0.0351606154662224\n",
      "Test: 0.03963182594842718\n"
     ]
    }
   ],
   "source": [
    "print('Fairness criterion value:')\n",
    "print(f'Train: {fair_criterion(model_con(features_train), sens_train).detach().numpy()}')\n",
    "print(f'Test: {fair_criterion(model_con(features_test), sens_test).detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40065d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "Train: 0.39351061534436116\n",
      "Test: 0.422021726193791\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\")\n",
    "print(f'Train: {criterion(model_con(features_train), labels_train).detach().numpy()}')\n",
    "print(f'Test: {criterion(model_con(features_test), labels_test).detach().numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
