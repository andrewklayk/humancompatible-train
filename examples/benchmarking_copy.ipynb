{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699c492c",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "This notebook benchmarks 5 algorithms on a ASC dataset for constrained neural networks training and is divided as follows: \n",
    "\n",
    "##### Deterministic Constraints:\n",
    "1. Unconstrained Optimization Adam\n",
    "2. [SSL-ALM](https://arxiv.org/abs/2504.07607) (Adam + SGD)\n",
    "3. [Switching Subgradient](https://arxiv.org/abs/2301.13314) \n",
    "4. [Cooper](https://github.com/cooper-org/cooper?tab=readme-ov-file) (ALM version)\n",
    "5. [CHOP](https://github.com/openopt/chop)\n",
    "\n",
    "##### Stochastic Constraints:\n",
    "1. Unconstrained Optimization Adam\n",
    "2. [SSL-ALM](https://arxiv.org/abs/2504.07607) (Adam + SGD)\n",
    "3. [Switching Subgradient](https://arxiv.org/abs/2301.13314) \n",
    "4. [Cooper](https://github.com/cooper-org/cooper?tab=readme-ov-file) (ALM version)\n",
    "\n",
    "##### Importance Sampling:\n",
    "1. Train a model using the SSL-ALM with balanced sampling over groups (each subgroups has equal number of samples in each batch),\n",
    "2. Train a model using the SSL-ALM without a balanced sampling over groups.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c4a64",
   "metadata": {},
   "source": [
    "# Deterministic constraints: Weight Regularization\n",
    " \n",
    "We aim to minimize the expected loss of $f$, where the implicit regression model is a neural network with its weights in a unit sphere. We problem can be defined as\n",
    "\n",
    "\\begin{aligned}\n",
    "\\min_{x \\in \\mathbb{R}^n} \\quad & \\mathbb{E}[f(x,\\xi)] \\\\\n",
    "\\text{s.t.} \\quad & \\|W_i\\|_F \\le 1, \\quad \\forall i \\in \\{1,\\ldots,L\\},\n",
    "\\end{aligned}\n",
    "\n",
    "where the $W_i$ is a weight matrix of the fully connected neural network, $||\\cdot||_F$ is a Frobenius norm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644329e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the torch seed here\n",
    "seed_n = 1\n",
    "n_epochs = 20\n",
    "\n",
    "# log path file\n",
    "log_path = \"./data/logs/log_benchmark.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e39b8",
   "metadata": {},
   "source": [
    "#### 1. Unconstrained Optimization\n",
    "\n",
    "For setting a reference bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41351743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome, generate_categories\n",
    "\n",
    "# load folktables data\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "acs_data = data_source.get_data(states=[\"VA\"], download=True)\n",
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(\n",
    "    features=ACSIncome.features, definition_df=definition_df\n",
    ")\n",
    "df_feat, df_labels, _ = ACSIncome.df_to_pandas(\n",
    "    acs_data, categories=categories, dummies=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252518b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SEX_Female', 'SEX_Male']\n",
      "(46144, 800)\n",
      "(46144, 2)\n",
      "(46144, 1)\n"
     ]
    }
   ],
   "source": [
    "sens_cols = [\"SEX_Female\", \"SEX_Male\"]\n",
    "features = df_feat.drop(columns=sens_cols).to_numpy(dtype=\"float\")\n",
    "groups = df_feat[sens_cols].to_numpy(dtype=\"float\")\n",
    "labels = df_labels.to_numpy(dtype=\"float\")\n",
    "\n",
    "print(sens_cols)\n",
    "print(features.shape)\n",
    "print(groups.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    features, labels, groups, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "sens_train = torch.tensor(groups_train)\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, labels_train)\n",
    "\n",
    "features_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "labels_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "sens_test = torch.tensor(groups_test)\n",
    "dataset_test = torch.utils.data.TensorDataset(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ff0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP FOR BENCHMARK TASK - common elements for all optimizers\n",
    "from humancompatible.train.benchmark.algorithms.optim_wrapper import OptimLoopWrapper\n",
    "from torch.nn import Sequential\n",
    "import torch\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "def create_model():\n",
    "    model = Sequential(\n",
    "        torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(latent_size1, latent_size2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(latent_size2, 1),\n",
    "    )    \n",
    "    return model\n",
    "\n",
    "m = 6 # number of params in model\n",
    "torch.manual_seed(seed_n)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "bounds = [1.0]*m\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# function to calculate the constraints\n",
    "def param_norms(model):\n",
    "    norms = []\n",
    "    for param in model.parameters():\n",
    "        norm = torch.linalg.norm(param, ord=2)\n",
    "        norms.append(norm)\n",
    "    return norms\n",
    "\n",
    "# forward function: expects to have (model, batch) as arguments\n",
    "def fwd_unconstrained(model, batch):\n",
    "    batch_inputs, batch_labels = batch\n",
    "    out = model(batch_inputs)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    return loss\n",
    "\n",
    "def fwd_constrained_max(model, batch):\n",
    "    batch_inputs, batch_labels = batch\n",
    "    out = model(batch_inputs)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    norms = param_norms(model)\n",
    "    norms_bounded_eq = [torch.max(norm - bounds[i], torch.zeros_like(norm)) for i, norm in enumerate(norms)]\n",
    "    return loss, norms_bounded_eq\n",
    "\n",
    "def fwd_constrained_slack(model, batch, slack_vars):\n",
    "    batch_inputs, batch_labels = batch\n",
    "    out = model(batch_inputs)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    norms = param_norms(model)\n",
    "    norms_bounded_eq = [norm + slack_vars[i] - bounds[i] for i, norm in enumerate(norms)]\n",
    "    return loss, norms_bounded_eq\n",
    "\n",
    "def fwd_constrained(model, batch, slack_vars = None):\n",
    "    if slack_vars is not None:\n",
    "        with torch.no_grad():\n",
    "            for s in slack_vars:\n",
    "                if s < 0:\n",
    "                    s.zero_()\n",
    "    batch_inputs, batch_labels = batch\n",
    "    out = model(batch_inputs)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    norms = param_norms(model)\n",
    "    if slack_vars is not None:\n",
    "        norms_bounded_eq = [norm + slack_vars[i] - bounds[i] for i, norm in enumerate(norms)]\n",
    "    else:\n",
    "        norms_bounded_eq = [torch.max(norm - bounds[i], torch.zeros_like(norm)) for i, norm in enumerate(norms)]\n",
    "    return loss, norms_bounded_eq\n",
    "\n",
    "# \n",
    "# def train_iter_cooper(model, batch):    \n",
    "\n",
    "@torch.inference_mode\n",
    "def eval(model, batch):\n",
    "    inputs, labels = batch\n",
    "    output = model(inputs)\n",
    "    constraints = param_norms(model)\n",
    "    constraints = [c.numpy().item() for c in constraints]\n",
    "    loss = criterion(output, labels)\n",
    "    return loss.detach().numpy().item(), constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58612f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     0 iteration     0 total_iters     0 time     0 train_loss 0.6882 c_0_train 0.7378008365631104 c_1_train 0.1681336760520935 c_2_train 0.9287227988243103 c_3_train 0.39018142223358154 c_4_train 0.5339652895927429 c_5_train 0.11916934698820114\n",
      "epoch     1 iteration     0 total_iters  2308 time 1.4245 train_loss 0.3852 c_0_train 7.194159507751465 c_1_train 0.4124360978603363 c_2_train 1.5141581296920776 c_3_train 0.4677649140357971 c_4_train 0.5245175957679749 c_5_train 0.15342265367507935\n",
      "epoch     2 iteration     0 total_iters  4616 time 2.7759 train_loss 0.3690 c_0_train 9.270196914672852 c_1_train 0.5593320727348328 c_2_train 1.8806036710739136 c_3_train 0.5051527619361877 c_4_train 0.4639599025249481 c_5_train 0.1524360030889511\n",
      "epoch     3 iteration     0 total_iters  6924 time 4.1223 train_loss 0.3570 c_0_train 10.61091423034668 c_1_train 0.7029558420181274 c_2_train 2.1593101024627686 c_3_train 0.5515404343605042 c_4_train 0.43976500630378723 c_5_train 0.1437537521123886\n",
      "epoch     4 iteration     0 total_iters  9232 time 5.5130 train_loss 0.3459 c_0_train 11.758267402648926 c_1_train 1.0064617395401 c_2_train 2.46819806098938 c_3_train 0.723560631275177 c_4_train 0.4665699601173401 c_5_train 0.13364724814891815\n",
      "epoch     5 iteration     0 total_iters 11540 time 6.8312 train_loss 0.3270 c_0_train 12.982372283935547 c_1_train 1.389795184135437 c_2_train 2.954558849334717 c_3_train 0.9096386432647705 c_4_train 0.5831418037414551 c_5_train 0.1224178820848465\n",
      "epoch     6 iteration     0 total_iters 13848 time 8.1595 train_loss 0.3162 c_0_train 14.26968765258789 c_1_train 1.8027517795562744 c_2_train 3.296165704727173 c_3_train 1.089735746383667 c_4_train 0.6665899157524109 c_5_train 0.1234809011220932\n",
      "epoch     7 iteration     0 total_iters 16156 time 9.4972 train_loss 0.2961 c_0_train 15.594507217407227 c_1_train 2.212723731994629 c_2_train 3.7918779850006104 c_3_train 1.251241683959961 c_4_train 0.8102781176567078 c_5_train 0.125243678689003\n",
      "epoch     8 iteration     0 total_iters 18464 time 10.9298 train_loss 0.2751 c_0_train 17.76483726501465 c_1_train 2.6740055084228516 c_2_train 4.150144100189209 c_3_train 1.3774757385253906 c_4_train 0.9657105207443237 c_5_train 0.12713418900966644\n",
      "epoch     9 iteration     0 total_iters 20772 time 12.3041 train_loss 0.2600 c_0_train 19.547046661376953 c_1_train 3.0270869731903076 c_2_train 4.521046161651611 c_3_train 1.4852569103240967 c_4_train 1.1318382024765015 c_5_train 0.11229681223630905\n",
      "epoch    10 iteration     0 total_iters 23080 time 13.6812 train_loss 0.2470 c_0_train 21.761497497558594 c_1_train 3.3836188316345215 c_2_train 4.906130313873291 c_3_train 1.5752007961273193 c_4_train 1.306636095046997 c_5_train 0.11123666912317276\n",
      "epoch    11 iteration     0 total_iters 25388 time 15.0670 train_loss 0.2328 c_0_train 23.249120712280273 c_1_train 3.650702714920044 c_2_train 5.2033371925354 c_3_train 1.6627850532531738 c_4_train 1.4564300775527954 c_5_train 0.09251214563846588\n",
      "epoch    12 iteration     0 total_iters 27696 time 16.4779 train_loss 0.2232 c_0_train 25.13959312438965 c_1_train 3.9356372356414795 c_2_train 5.463476657867432 c_3_train 1.7169933319091797 c_4_train 1.6822052001953125 c_5_train 0.07108423113822937\n",
      "epoch    13 iteration     0 total_iters 30004 time 17.8250 train_loss 0.2061 c_0_train 27.05849266052246 c_1_train 4.185943603515625 c_2_train 5.760786056518555 c_3_train 1.7762757539749146 c_4_train 1.8643200397491455 c_5_train 0.025772809982299805\n",
      "epoch    14 iteration     0 total_iters 32312 time 19.1229 train_loss 0.1933 c_0_train 28.864669799804688 c_1_train 4.390016555786133 c_2_train 5.988391876220703 c_3_train 1.8209983110427856 c_4_train 2.105112075805664 c_5_train 0.007975197397172451\n",
      "epoch    15 iteration     0 total_iters 34620 time 20.4044 train_loss 0.1843 c_0_train 30.544092178344727 c_1_train 4.599330425262451 c_2_train 6.239526271820068 c_3_train 1.8684685230255127 c_4_train 2.311025619506836 c_5_train 0.03780011087656021\n",
      "epoch    16 iteration     0 total_iters 36928 time 21.7930 train_loss 0.1776 c_0_train 31.89621925354004 c_1_train 4.746724605560303 c_2_train 6.484369277954102 c_3_train 1.8927525281906128 c_4_train 2.4368677139282227 c_5_train 0.07092942297458649\n",
      "epoch    17 iteration     0 total_iters 39236 time 23.1368 train_loss 0.1706 c_0_train 33.782470703125 c_1_train 4.9585137367248535 c_2_train 6.653316020965576 c_3_train 1.934718370437622 c_4_train 2.6072261333465576 c_5_train 0.10621307790279388\n",
      "epoch    18 iteration     0 total_iters 41544 time 24.5338 train_loss 0.1610 c_0_train 35.017093658447266 c_1_train 5.085838317871094 c_2_train 6.837343215942383 c_3_train 1.9825481176376343 c_4_train 2.752983570098877 c_5_train 0.13435901701450348\n",
      "epoch    19 iteration     0 total_iters 43852 time 25.8317 train_loss 0.1542 c_0_train 36.15077590942383 c_1_train 5.2091474533081055 c_2_train 7.029716491699219 c_3_train 1.987430214881897 c_4_train 2.8902029991149902 c_5_train 0.17939822375774384\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "model = create_model()\n",
    "# first backward pass sometimes takes ages\n",
    "model(features_train[0]).backward()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=0.001,\n",
    ")\n",
    "\n",
    "adam = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_unconstrained,\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, labels_train)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "adam.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9995aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adam_histories = []\n",
    "\n",
    "# 4 runs of Adam:\n",
    "for run in range(4):\n",
    "    model = create_model()\n",
    "    # first backward pass sometimes takes ages\n",
    "    model(features_train[0]).backward()\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),lr=0.001)\n",
    "    adam = OptimLoopWrapper(\n",
    "        model = model,\n",
    "        fwd = fwd_unconstrained,\n",
    "        eval = eval,\n",
    "        train_data = dataloader,\n",
    "        eval_data = {'train': (features_train, labels_train)},\n",
    "        optimizer = optimizer,\n",
    "        use_vanilla_torch=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    adam.training_loop(epochs=5, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)\n",
    "    adam_histories.append(\n",
    "        pd.DataFrame(adam.history)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52767d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSLALM with Max\n",
    "from humancompatible.train.optim import SSLALM\n",
    "model = create_model()\n",
    "# first backward pass sometimes takes ages\n",
    "model(features_train[0]).backward()\n",
    "\n",
    "optimizer = SSLALM(params=model.parameters(), m=m, lr=0.01, dual_lr=0.1)\n",
    "slack_vars = torch.zeros(m, requires_grad=True)\n",
    "optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})\n",
    "\n",
    "alm = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = lambda model, batch: fwd_constrained(model, batch, slack_vars),\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, labels_train)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch = False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "alm.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a425a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSLALM with Max\n",
    "from humancompatible.train.optim import SSLALM_Adam\n",
    "model = create_model()\n",
    "# first backward pass sometimes takes agesplot_losses_and_constraints_multi\n",
    "model(features_train[0]).backward()\n",
    "\n",
    "optimizer = SSLALM_Adam(params=model.parameters(), m=m, lr=0.01, dual_lr=0.1)\n",
    "\n",
    "alm_adam = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_constrained_max,\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, labels_train)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch = False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "alm_adam.training_loop(epochs=n_epochs, max_iter=np.inf, max_runplot_losses_and_constraints_multitime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSLALM with Max\n",
    "from humancompatible.train.optim.PBM import PBM\n",
    "model = create_model()\n",
    "# first backward pass sometimes takes ages\n",
    "model(features_train[0]).backward()\n",
    "\n",
    "optimizer = PBM(params=model.parameters(), m=m, lr=0.001, dual_beta=0.95, penalty_update_m='CONST', barrier=\"quadratic_logarithmic\")\n",
    "\n",
    "pbm = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_constrained_max,\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, labels_train)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch = False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "pbm.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e145db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x480 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# def plot_one_trajectory(trajectory, ax_obj, ax_con, kwargs = {}):\n",
    "#     ax_obj.plot(trajectory['train_loss'], **kwargs)\n",
    "#     for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_train')]:\n",
    "#         ax_con.plot(df[c], **kwargs)\n",
    "#     for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_train')]:\n",
    "#         ax_con.plot(df[c], **kwargs)\n",
    "\n",
    "# def fill_between_quantiles(q1, q2, ax_obj, ax_con, kwargs = {}):\n",
    "#     q\n",
    "\n",
    "\n",
    "# def plot_trajectories(histories, quantiles=(0.25, 0.75)):\n",
    "#     \"\"\"\n",
    "#     histories: a dict of lists of DataFrames (multiple algorithms, multiple runs)\n",
    "#     \"\"\"\n",
    "#     # multiple algorithms, multiple runs\n",
    "#     #  {'adam': [adam, adam, adam], 'alm': [alm, alm, alm], ...}\n",
    "#     n_algs = len(histories)\n",
    "#     f, axs = plt.subplots(nrows=2, ncols=n_algs, sharex=True, sharey='row')\n",
    "#     axs[0][0].set_label('Loss')\n",
    "#     axs[0][0].set_label('Constraints')\n",
    "#     for alg_idx, (alg_name, alg_trajectories) in enumerate(histories.items()): # iterate over algorithms\n",
    "#         alg_trajectories_grouped = pd.concat(alg_trajectories).groupby('epoch')\n",
    "#         mean = alg_trajectories_grouped.mean()\n",
    "#         q0 = alg_trajectories_grouped.quantile(quantiles[0])\n",
    "#         q1 = alg_trajectories_grouped.quantile(quantiles[1])\n",
    "#         axs[0][alg_idx].set_title(alg_name)\n",
    "#         plot_one_trajectory(mean, axs[0][alg_idx], axs[1][alg_idx])\n",
    "#         plot_one_trajectory(q0, axs[0][alg_idx], axs[1][alg_idx], {'ls': '-'})\n",
    "#         plot_one_trajectory(q1, axs[0][alg_idx], axs[1][alg_idx], {'ls': '-'})\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# trajectories = [adam.history, alm.history, alm_adam.history, pbm.history]\n",
    "# f, axs = plt.subplots(nrows=2, ncols=len(trajectories), sharex=True, sharey='row')\n",
    "# f.set_figwidth(len(trajectories)*4)\n",
    "# f.tight_layout()\n",
    "# for plt_idx, df in enumerate([pd.DataFrame(alg) for alg in trajectories]):\n",
    "#     ax1 = axs[0][plt_idx]\n",
    "#     ax2 = axs[1][plt_idx]\n",
    "#     ax1.plot(df['train_loss'], color='red', ls='-')\n",
    "#     ax1.grid()\n",
    "#     ax2.grid()\n",
    "#     for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_train')]:\n",
    "#         ax2.plot(df[c], color='red', ls='-', alpha=0.1)\n",
    "#     for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_train')]:\n",
    "#         ax2.plot(df[c], color='red', ls='-', alpha=0.1)\n",
    "\n",
    "#     ax2.hlines(bounds[0], 0, 20,ls='--', color='black', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses_and_constraints_multi(\n",
    "    losses_list,\n",
    "    constraints_list,\n",
    "    constraint_thresholds,\n",
    "    losses_std_list=None,\n",
    "    titles=None,\n",
    "    plot_individual_constraints=True,\n",
    "    eval_points=2,\n",
    "    std_multiplier=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot loss and constraint values for multiple ML algorithms with min-max, std, and eval points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    losses_list : list of np.ndarray or list of 2D np.ndarray\n",
    "        Each element is either:\n",
    "        - 1D array of losses for one algorithm, or\n",
    "        - 2D array (num_runs x timesteps) for multiple runs.\n",
    "    constraints_list : list of list of np.ndarray\n",
    "        Each element is a list of 1D arrays (constraint values for that algorithm).\n",
    "    constraint_thresholds : np.ndarray or list\n",
    "        Threshold lines for constraints.\n",
    "    losses_std_list : list of np.ndarray, optional\n",
    "        1D array of std per algorithm.\n",
    "    titles : list of str, optional\n",
    "        Algorithm names.\n",
    "    plot_individual_constraints : bool, default True\n",
    "        Plot individual constraint curves.\n",
    "    eval_points : int or array-like, default 2\n",
    "        Epoch indices to mark with symbols.\n",
    "    std_multiplier : float, default 2\n",
    "        Multiplier for shading std bands.\n",
    "    \"\"\"\n",
    "    # --- Color palette: Tableau 10 ---\n",
    "    colors = [\n",
    "        \"#4E79A7\",\n",
    "        \"#F28E2B\",\n",
    "        \"#E15759\",\n",
    "        \"#76B7B2\",\n",
    "        \"#59A14F\",\n",
    "        \"#EDC948\",\n",
    "        \"#B07AA1\",\n",
    "        \"#FF9DA7\",\n",
    "        \"#9C755F\",\n",
    "        \"#BAB0AB\",\n",
    "    ]\n",
    "    # --- Marker styles ---\n",
    "    marker_styles = [\"o\", \"s\", \"D\", \"^\", \"v\", \"<\", \">\", \"P\", \"X\", \"*\"]\n",
    "\n",
    "    num_algos = len(losses_list)\n",
    "    if titles is None:\n",
    "        titles = [f\"Algorithm {i + 1}\" for i in range(num_algos)]\n",
    "    constraint_thresholds = np.atleast_1d(constraint_thresholds)\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_algos, figsize=(5 * num_algos, 8), sharex=\"col\")\n",
    "    if num_algos == 1:\n",
    "        axes = np.array([[axes[0]], [axes[1]]])  # consistent 2D indexing\n",
    "\n",
    "    for j in range(num_algos):\n",
    "        loss = np.asarray(losses_list[j])\n",
    "        x = np.arange(loss.shape[-1])\n",
    "        color = colors[j % len(colors)]\n",
    "\n",
    "        # --- LOSS PLOT ---\n",
    "        ax_loss = axes[0, j]\n",
    "        # Multiple runs: min-max shading\n",
    "        if loss.ndim == 2:\n",
    "            loss_mean = np.mean(loss, axis=0)\n",
    "            loss_min = np.min(loss, axis=0)\n",
    "            loss_max = np.max(loss, axis=0)\n",
    "            ax_loss.fill_between(\n",
    "                x, loss_min, loss_max, color=color, alpha=0.15, label=\"Mean Loss +- Std\"\n",
    "            )\n",
    "        else:\n",
    "            loss_mean = loss\n",
    "\n",
    "        # Std shading\n",
    "        if losses_std_list is not None and losses_std_list[j] is not None:\n",
    "            loss_std = np.asarray(losses_std_list[j])\n",
    "            ax_loss.fill_between(\n",
    "                x,\n",
    "                loss_mean - std_multiplier * loss_std,\n",
    "                loss_mean + std_multiplier * loss_std,\n",
    "                color=color,\n",
    "                alpha=0.25,\n",
    "                label=f\"Loss  Std\",\n",
    "            )\n",
    "\n",
    "        # Mean curve\n",
    "        ax_loss.plot(x, loss_mean, color=color, lw=2, label=titles[j])\n",
    "\n",
    "        # Eval points\n",
    "        if eval_points is not None:\n",
    "            if isinstance(eval_points, int):\n",
    "                idx = np.arange(0, len(loss_mean), eval_points)\n",
    "            else:\n",
    "                idx = np.array(eval_points)\n",
    "                idx = idx[idx < len(loss_mean)]\n",
    "            ax_loss.plot(\n",
    "                x[idx], loss_mean[idx], marker_styles[j], color=color, markersize=6\n",
    "            )\n",
    "\n",
    "        ax_loss.set_title(titles[j])\n",
    "        ax_loss.set_ylabel(\"Mean Loss\")\n",
    "        ax_loss.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        ax_loss.legend(fontsize=9)\n",
    "\n",
    "        # --- CONSTRAINT PLOT ---\n",
    "        ax_constr = axes[1, j]\n",
    "        constraints = np.array(constraints_list[j])\n",
    "        x = np.arange(constraints.shape[1])\n",
    "        color = colors[j % len(colors)]\n",
    "\n",
    "        # Optional: individual constraint curves\n",
    "        if plot_individual_constraints:\n",
    "            for c in constraints:\n",
    "                ax_constr.plot(x, c, color=color, alpha=0.6, lw=1.5)\n",
    "\n",
    "        # Min-Max band for constraints\n",
    "        c_min = np.min(constraints, axis=0)\n",
    "        c_max = np.max(constraints, axis=0)\n",
    "        ax_constr.fill_between(\n",
    "            x, c_min, c_max, color=color, alpha=0.15, label=\"Constraint Range\"\n",
    "        )\n",
    "\n",
    "        # Threshold lines\n",
    "        for th in constraint_thresholds:\n",
    "            ax_constr.axhline(\n",
    "                th, color=\"red\", linestyle=\"--\", lw=1.4, label=\"Threshold\"\n",
    "            )\n",
    "\n",
    "        ax_constr.set_ylabel(\"Constraint\")\n",
    "        ax_constr.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        ax_constr.legend(fontsize=9)\n",
    "\n",
    "    axes[-1, 0].set_xlabel(\"Epoch\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5106ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses_and_constraints_multi(\n",
    "    losses_list = [[hist['train_loss']for hist in adam_histories]],\n",
    "    constraints_list = [np.array(hist[[c for c in hist.columns if c.startswith('c_')]]).T for hist in adam_histories],\n",
    "    constraint_thresholds=bounds[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=[np.array(Adam_loss_log_plotting)],\n",
    "    constraints=[np.array(Adam_c_log_plotting).T],\n",
    "    losses_std=[np.array(Adam_std_loss_log_plotting)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current results\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "thresholds = [constraint_bounds[0]]\n",
    "\n",
    "plot_losses_and_constraints_multi(\n",
    "    losses,\n",
    "    constraints,\n",
    "    thresholds,\n",
    "    losses_std_list=losses_std,\n",
    "    titles=[\"Unconstrained Adam\"],\n",
    "    plot_individual_constraints=True,\n",
    "    std_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.optim import SSLALM\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# create small FC network\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# get number of layers + number of biases\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "# create the SSLALM optimizer\n",
    "optimizer = SSLALM(params=model.parameters(), m=m, lr=0.01, dual_lr=0.1)\n",
    "\n",
    "# bounds for the constraints: norm of each weight matrix should be <= 1\n",
    "constraint_bounds = [1.0] * m\n",
    "\n",
    "# define epochs + loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb699bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSL_ALM_loss_log_plotting = []\n",
    "SSL_ALM_c_log_plotting = []\n",
    "SSL_ALM_std_loss_log_plotting = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go through all data\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads - constraint per each weight matrix\n",
    "        c_log.append([])\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            # norm of the w. matrix\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "\n",
    "            # convert constraint to equality11.559413\n",
    "            norm_viol = torch.max(norm - constraint_bounds[i], torch.zeros(1))\n",
    "\n",
    "            # calculate the Jacobian of the constraint\n",
    "            norm_viol.backward()\n",
    "\n",
    "            # update the dual variable + save the Jacobian for later - its needed in the primal variable update\n",
    "            optimizer.dual_step(i, c_val=norm_viol)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the value of the constraint\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "\n",
    "        # calculate loss and grad\n",
    "        batch_output = model(batch_input)\n",
    "        loss = criterion(batch_output, batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        # save the loss and the dual variables\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "        # update the primal variables together with smoothing dual variable\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # save the epoch values for plotting\n",
    "    SSL_ALM_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSL_ALM_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSL_ALM_std_loss_log_plotting.append(np.std(loss_log))\n",
    "\n",
    "    # print out the epoch values\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSL_ALM_loss_log_plotting)]\n",
    "constraints += [np.array(SSL_ALM_c_log_plotting).T]\n",
    "losses_std += [np.array(SSL_ALM_std_loss_log_plotting)]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(log_path, losses=losses, constraints=constraints, losses_std=losses_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48866e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current results\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "thresholds = [constraint_bounds[0]]\n",
    "\n",
    "plot_losses_and_constraints_multi(\n",
    "    losses,\n",
    "    constraints,\n",
    "    thresholds,\n",
    "    losses_std_list=losses_std,\n",
    "    std_multiplier=1,\n",
    "    titles=[\"Unconstrained Adam\", \"SSL-ALM-SGD\"],\n",
    "    plot_individual_constraints=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3a5ef",
   "metadata": {},
   "source": [
    "#### 2.2 SSL-ALM Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbf556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Add the ../src folder to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# create small FC network\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# get number of layers + number of biases\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "# create the SSLALM optimizer\n",
    "optimizer = SSLALM_Adam(params=model.parameters(), m=m, lr=0.001, dual_lr=0.1)\n",
    "\n",
    "# bounds for the constraints: norm of each weight matrix should be <= 1\n",
    "constraint_bounds = [1.0] * m\n",
    "\n",
    "# define epochs + loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf46ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSL_ALM_Adam_loss_log_plotting = []\n",
    "SSL_ALM_Adam_c_log_plotting = []\n",
    "SSL_ALM_Adam_std_c_log_plotting = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go through all data\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads - constraint per each weight matrix\n",
    "        c_log.append([])\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            # norm of the w. matrix\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "\n",
    "            # convert constraint to equality\n",
    "            norm_viol = torch.max(norm - constraint_bounds[i], torch.zeros(1))\n",
    "\n",
    "            # calculate the Jacobian of the constraint\n",
    "            norm_viol.backward()\n",
    "\n",
    "            # update the dual variable + save the Jacobian for later - its needed in the primal variable update\n",
    "            optimizer.dual_step(i, c_val=norm_viol[0])\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the value of the constraint\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "\n",
    "        # calculate loss and grad\n",
    "        batch_output = model(batch_input)\n",
    "        loss = criterion(batch_output, batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        # save the loss and the dual variables\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "        # update the primal variables together with smoothing dual variable\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # save the epoch values for plotting\n",
    "    SSL_ALM_Adam_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSL_ALM_Adam_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSL_ALM_Adam_std_c_log_plotting.append(np.std(loss_log))\n",
    "\n",
    "    # print out the epoch values\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSL_ALM_Adam_loss_log_plotting)]\n",
    "constraints += [np.array(SSL_ALM_Adam_c_log_plotting).T]\n",
    "losses_std += [np.array(SSL_ALM_Adam_std_c_log_plotting)]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(log_path, losses=losses, constraints=constraints, losses_std=losses_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d10334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current results\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "thresholds = [constraint_bounds[0]]\n",
    "\n",
    "plot_losses_and_constraints_multi(\n",
    "    losses,\n",
    "    constraints,\n",
    "    thresholds,\n",
    "    losses_std_list=losses_std,\n",
    "    std_multiplier=1,\n",
    "    titles=[\"Unconstrained Adam\", \"SSL-ALM-SGD\", \"SSL-ALM-Adam\"],\n",
    "    plot_individual_constraints=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e956e47",
   "metadata": {},
   "source": [
    "#### 3. Switching-Subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.algorithms import SSG\n",
    "\n",
    "# set the seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# create small FC network - same as the other algorithms\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# get number of layers + number of biases\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "# create the SSLALM optimizer\n",
    "optimizer = SSG(params=model.parameters(), m=1, lr=0.01, dual_lr=0.1)\n",
    "\n",
    "# bounds for the constraints: norm of max each weight matrix should be <= 1\n",
    "constraint_bounds = [1.0]\n",
    "\n",
    "# define epochs + loss function - same loss should be defined for all algorithms\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc the plotting array\n",
    "SSG_c_log_plotting = []\n",
    "SSG_loss_log_plotting = []\n",
    "SSG_std_loss_log_plotting = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc logging array\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # train for all data\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # prepare the max of the violation\n",
    "        max_norm_viol = torch.zeros(1)\n",
    "        c_log.append([])\n",
    "\n",
    "        # calculate constraints and constraint grads - max of constraint per each weight matrix\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            # norm of the w. matrix\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "\n",
    "            # convert constraint to equality\n",
    "            norm_viol = torch.max(norm - constraint_bounds[0], torch.zeros(1))\n",
    "\n",
    "            # save the max\n",
    "            max_norm_viol = torch.max(max_norm_viol, norm_viol)\n",
    "\n",
    "            # save the value of the constraint\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "\n",
    "        # calculate the Jacobian of the max-violating norm constraint\n",
    "        max_norm_viol.backward()\n",
    "\n",
    "        # save the gradient of the constraint\n",
    "        optimizer.dual_step(0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate loss and grad\n",
    "        out = model(batch_input)\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        # save the loss value\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "\n",
    "        # perform a step - either update based on the loss grad or constraint grad\n",
    "        optimizer.step(max_norm_viol)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # save the epoch values for plotting\n",
    "    SSG_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSG_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSG_std_loss_log_plotting.append(np.std(loss_log))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSG_loss_log_plotting)]\n",
    "constraints += [np.array(SSG_c_log_plotting).T]\n",
    "losses_std += [np.array(SSG_std_loss_log_plotting)]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(log_path, losses=losses, constraints=constraints, losses_std=losses_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5757dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current results\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "thresholds = [constraint_bounds[0]]\n",
    "\n",
    "plot_losses_and_constraints_multi(\n",
    "    losses,\n",
    "    constraints,\n",
    "    thresholds,\n",
    "    losses_std_list=losses_std,\n",
    "    std_multiplier=1,\n",
    "    titles=[\"Unconstrained Adam\", \"SSL-ALM-SGD\", \"SSL-ALM-Adam\", \"SSG\"],\n",
    "    plot_individual_constraints=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33924c2b",
   "metadata": {},
   "source": [
    "#### 4. Cooper - Augmented Lagrangian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b60c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# create small FC network - same as the other algorithms\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# get number of layers + number of biases\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "# bounds for the constraints: norm of max each weight matrix should be <= 1\n",
    "constraint_bounds = [1.0] * m\n",
    "\n",
    "# define epochs + loss function - same loss should be defined for all algorithms\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cooper\n",
    "\n",
    "# select device\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "# define the problem\n",
    "class ACS_Deterministic_Constr_Fair_ALM(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, formulation_type):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the number of multipliers - same as the number of constraints\n",
    "        if not formulation_type.expects_multiplier:\n",
    "            self.multiplier = None\n",
    "        else:\n",
    "            self.multiplier = cooper.multipliers.DenseMultiplier(\n",
    "                num_constraints=len(list(model.parameters())), device=DEVICE\n",
    "            )\n",
    "\n",
    "        # add the penalty coefficient - augmented lagrangian\n",
    "        if not formulation_type.expects_penalty_coefficient:\n",
    "            self.penalty = None\n",
    "        else:\n",
    "            self.penalty = cooper.penalty_coefficients.DensePenaltyCoefficient(\n",
    "                init=torch.tensor(1.0, device=DEVICE),\n",
    "            )\n",
    "\n",
    "        # inequality constraints on the norm of the matrix\n",
    "        self.constraint = cooper.Constraint(\n",
    "            constraint_type=cooper.ConstraintType.INEQUALITY,\n",
    "            formulation_type=formulation_type,\n",
    "            multiplier=self.multiplier,\n",
    "            penalty_coefficient=self.penalty,\n",
    "        )\n",
    "\n",
    "    # function that computes the constraints and the loss function\n",
    "    def compute_cmp_state(self, model, inputs, targets):\n",
    "        # compute the yhat loss\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        loss = criterion(model(inputs), targets)\n",
    "\n",
    "        # calculate constraints\n",
    "        norm_values_c = []\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            # norm of the w. matrix\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "\n",
    "            # compute the violation of the constraint\n",
    "            norm_viol = norm - constraint_bounds[i]\n",
    "            norm_values_c.append(norm_viol)\n",
    "\n",
    "        # compute the constraint violation\n",
    "        constraint_state = cooper.ConstraintState(violation=torch.stack(norm_values_c))\n",
    "        observed_constraints = {self.constraint: constraint_state}\n",
    "\n",
    "        return cooper.CMPState(loss=loss, observed_constraints=observed_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the problem\n",
    "problem = ACS_Deterministic_Constr_Fair_ALM(cooper.formulations.AugmentedLagrangian)\n",
    "\n",
    "# flag of dual variables\n",
    "has_dual_variables = problem.multiplier is not None\n",
    "\n",
    "# create the primal optimizer\n",
    "primal_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# create the dual optimizer\n",
    "if has_dual_variables:\n",
    "    dual_optimizer = torch.optim.SGD(problem.dual_parameters(), lr=0.1, maximize=True)\n",
    "\n",
    "    # combine the optimizers\n",
    "    constrained_optimizer = cooper.optim.SimultaneousOptimizer(\n",
    "        cmp=problem,\n",
    "        primal_optimizers=primal_optimizer,\n",
    "        dual_optimizers=dual_optimizer,\n",
    "    )\n",
    "else:\n",
    "    # Formulations without dual variables, such as the Quadratic Penalty\n",
    "    # formulation, do not require a dual optimizer\n",
    "    constrained_optimizer = cooper.optim.UnconstrainedOptimizer(\n",
    "        cmp=problem,\n",
    "        primal_optimizers=primal_optimizer,\n",
    "    )\n",
    "\n",
    "# Increase the penalty coefficient by `increment` if the constraint is violate by more\n",
    "# than `violation_tolerance`- keep this the same - since its not included in SSL-ALM implementation\n",
    "penalty_scheduler = cooper.penalty_coefficients.AdditivePenaltyCoefficientUpdater(\n",
    "    increment=0.0,\n",
    "    violation_tolerance=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f23d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "Cooper_c_log_plotting = []\n",
    "Cooper_loss_log_plotting = []\n",
    "Cooper_std_loss_log_plotting = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "    penalty_coefficient_values = []\n",
    "\n",
    "    # alloc norm values array\n",
    "    norm_values_c = []\n",
    "\n",
    "    # go through all data\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints - just for logging\n",
    "        c_log.append([])\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            # norm of the w. matrix\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "\n",
    "            # save the value of the constraint\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "\n",
    "        # compute the loss - just for logging purposes\n",
    "        loss = criterion(model(batch_input), batch_label)\n",
    "        loss_log.append(loss.item())\n",
    "\n",
    "        # compute the states\n",
    "        compute_cmp_state_kwargs = {\n",
    "            \"model\": model,\n",
    "            \"inputs\": batch_input,\n",
    "            \"targets\": batch_label,\n",
    "        }\n",
    "        roll_out = constrained_optimizer.roll(\n",
    "            compute_cmp_state_kwargs=compute_cmp_state_kwargs\n",
    "        )\n",
    "\n",
    "        # Update the penalty coefficient\n",
    "        constraint_state = roll_out.cmp_state.observed_constraints[problem.constraint]\n",
    "        penalty_scheduler.update_penalty_coefficient_(\n",
    "            problem.constraint, constraint_state\n",
    "        )\n",
    "\n",
    "        # get the dual variables and the coefficients\n",
    "        multiplier_value = (\n",
    "            problem.multiplier.weight.detach().numpy() if has_dual_variables else None\n",
    "        )\n",
    "        penalty_coefficient_value = problem.constraint.penalty_coefficient().item()\n",
    "\n",
    "        # save the duals and penalty coefficients\n",
    "        duals_log.append(multiplier_value)\n",
    "        penalty_coefficient_values.append(penalty_coefficient_value)\n",
    "\n",
    "    # save the epoch values for plotting\n",
    "    Cooper_loss_log_plotting.append(np.mean(loss_log))\n",
    "    Cooper_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    Cooper_std_loss_log_plotting.append(np.std(loss_log))\n",
    "\n",
    "    # print out the epoch values\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}, \"\n",
    "        f\"penalty coefficients: {np.mean(penalty_coefficient_values, axis=0)}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef81709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(Cooper_loss_log_plotting)]\n",
    "constraints += [np.array(Cooper_c_log_plotting).T]\n",
    "losses_std += [np.array(Cooper_std_loss_log_plotting)]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(log_path, losses=losses, constraints=constraints, losses_std=losses_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current results\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "thresholds = [constraint_bounds[0]]\n",
    "\n",
    "plot_losses_and_constraints_multi(\n",
    "    losses,\n",
    "    constraints,\n",
    "    thresholds,\n",
    "    losses_std_list=losses_std,\n",
    "    std_multiplier=1,\n",
    "    titles=[\"Unconstrained Adam\", \"SSL-ALM-SGD\", \"SSL-ALM-Adam\", \"SSG\", \"Cooper-ALM\"],\n",
    "    plot_individual_constraints=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba8f34",
   "metadata": {},
   "source": [
    "#### 5. CHOP\n",
    "\n",
    "The library is based on the [Stochastic Franke-Wolfe (SFW)](https://arxiv.org/pdf/2010.07243), which constraints the parameters on a convex set. Thus, the model weights are to be in a closed ball, simplex and such. This can only be used for the determinitic case of benchmarking but cannot be extended to the stochastic constrained case. \n",
    "\n",
    "NOTE: The repo has an error in it. <code>chop/constraints.py</code>, line 292: \n",
    "\n",
    "<code>update_direction = iterate.clone().detach()</code> change to \n",
    "\n",
    "<code>update_direction = -iterate.clone().detach()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chop\n",
    "\n",
    "# set the seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# create small FC network - same as the other algorithms\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# get number of layers + number of biases\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "# bounds for the constraints: norm of max each weight matrix should be <= 1\n",
    "constraint_bounds = [1.0]\n",
    "\n",
    "# define epochs + loss function - same loss should be defined for all algorithms\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an optimizer with a ball constraints\n",
    "optimizer = chop.stochastic.FrankWolfe(\n",
    "    model.parameters(),\n",
    "    chop.constraints.L2Ball(alpha=constraint_bounds[0]),\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    normalization=\"gradient\",\n",
    ")\n",
    "\n",
    "bias_params = [param for name, param in model.named_parameters() if \"bias\" in name]\n",
    "bias_opt = chop.stochastic.PGD(\n",
    "    bias_params, chop.constraints.L2Ball(alpha=constraint_bounds[0]), lr=0.001\n",
    ")\n",
    "\n",
    "# project the model weights into the feasible region (an assumption of the franke-wolfe)\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    param_proj = chop.constraints.L2Ball(alpha=constraint_bounds[0]).prox(param)\n",
    "\n",
    "    # Update the parameter in-place\n",
    "    with torch.no_grad():\n",
    "        param.copy_(param_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1307648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "CHOP_c_log_plotting = []\n",
    "CHOP_loss_log_plotting = []\n",
    "CHOP_std_loss_log_plotting = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # alloc norm values array\n",
    "    norm_values_c = []\n",
    "\n",
    "    # go through all data\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints - just for logging\n",
    "        c_log.append([])\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            # norm of the w. matrix\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "\n",
    "            # save the value of the constraint\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "\n",
    "        # compute the loss - just for logging purposes\n",
    "        optimizer.zero_grad()\n",
    "        bias_opt.zero_grad()\n",
    "        loss = criterion(model(batch_input), batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bias_opt.step()\n",
    "\n",
    "        # save the loss for later\n",
    "        loss_log.append(loss.item())\n",
    "\n",
    "    # save the epoch values for plotting\n",
    "    CHOP_loss_log_plotting.append(np.mean(loss_log))\n",
    "    CHOP_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    CHOP_std_loss_log_plotting.append(np.std(loss_log))\n",
    "\n",
    "    # print out the epoch values\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(CHOP_loss_log_plotting)]\n",
    "constraints += [np.array(CHOP_c_log_plotting).T]\n",
    "losses_std += [np.array(CHOP_std_loss_log_plotting)]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(log_path, losses=losses, constraints=constraints, losses_std=losses_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2770be5",
   "metadata": {},
   "source": [
    "#### 6. Penalty/Barrier Method (PBM)\n",
    "\n",
    "PBM can only handle inequality constraints. That is, the problem of type: \n",
    "\n",
    "\\begin{aligned}\n",
    "& \\min_{x \\in \\mathbb{R}^n} \\; \\mathbb{E}[f(x,\\xi)] \\\\\n",
    "& \\text{s.t.} \\quad \\mathbb{E}[c(x,\\zeta)] \\le 0. \\quad\n",
    "\\end{aligned}\n",
    "\n",
    "Which means, there is no need to introduce the slack variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba849ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import subprocess\n",
    "# Add the ../src folder to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.PBM import PBM\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# create small FC network\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# get number of layers + number of biases\n",
    "m = len(list(model.parameters()))\n",
    "\n",
    "# create the SSLALM optimizer\n",
    "optimizer = PBM(params=model.parameters(), m=m, lr=0.001, dual_beta=0.95, penalty_update_m='CONST', barrier=\"quadratic_logarithmic\")\n",
    "\n",
    "# bounds for the constraints: norm of each weight matrix should be <= 1\n",
    "constraint_bounds = [1.0] * m\n",
    "\n",
    "# define epochs + loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35276f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSL_ALM_Adam_loss_log_plotting = []\n",
    "SSL_ALM_Adam_c_log_plotting = []\n",
    "SSL_ALM_Adam_std_c_log_plotting = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go through all data\n",
    "    for batch_input, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads - constraint per each weight matrix\n",
    "        c_log.append([])\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            # norm of the w. matrix\n",
    "            norm = torch.linalg.norm(param, ord=2)\n",
    "\n",
    "            # copmute the constraint\n",
    "            norm_viol = norm - constraint_bounds[i]\n",
    "            \n",
    "            # update the dual variable + save the Jacobian for later - its needed in the primal variable update\n",
    "            optimizer.dual_step(i, c_val=norm_viol)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the value of the constraint\n",
    "            c_log[-1].append(norm.detach().numpy())\n",
    "\n",
    "        # calculate loss and grad\n",
    "        batch_output = model(batch_input)\n",
    "        loss = criterion(batch_output, batch_label)\n",
    "        loss.backward()\n",
    "\n",
    "        # save the loss and the dual variables\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "        # update the primal variables together with smoothing dual variable\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # save the epoch values for plotting\n",
    "    SSL_ALM_Adam_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSL_ALM_Adam_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSL_ALM_Adam_std_c_log_plotting.append(np.std(loss_log))\n",
    "\n",
    "    # print out the epoch values\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSL_ALM_Adam_loss_log_plotting)]\n",
    "constraints += [np.array(SSL_ALM_Adam_c_log_plotting).T]\n",
    "losses_std += [np.array(SSL_ALM_Adam_std_c_log_plotting)]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(log_path, losses=losses, constraints=constraints, losses_std=losses_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43c738",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We first plot all algorithms separately. This makes it easy to see the violation of each constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be02c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the current results\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "thresholds = [constraint_bounds[0]]\n",
    "\n",
    "plot_losses_and_constraints_multi(\n",
    "    losses,\n",
    "    constraints,\n",
    "    thresholds,\n",
    "    losses_std_list=losses_std,\n",
    "    std_multiplier=1,\n",
    "    titles=[\n",
    "        \"Unconstrained Adam\",\n",
    "        \"SSL-ALM-SGD\",\n",
    "        \"SSL-ALM-Adam\",\n",
    "        \"SSG\",\n",
    "        \"Cooper-ALM\",\n",
    "        \"SFW\",\n",
    "        \"PBM\"\n",
    "    ],\n",
    "    plot_individual_constraints=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdedcf8",
   "metadata": {},
   "source": [
    "To better compare the results, we plot all losses in a single Figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3580251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses_and_constraints_single(\n",
    "    losses_list,\n",
    "    losses_std_list,\n",
    "    constraints_list,\n",
    "    constraint_thresholds,\n",
    "    titles=None,\n",
    "    eval_points=2,\n",
    "    std_multiplier=2,\n",
    "    log_constraints=False,\n",
    "    log_loss=False,\n",
    "):\n",
    "    # --- Color palette: Tableau 10 ---\n",
    "    colors = [\n",
    "        \"#4E79A7\",\n",
    "        \"#F28E2B\",\n",
    "        \"#E15759\",\n",
    "        \"#76B7B2\",\n",
    "        \"#59A14F\",\n",
    "        \"#EDC948\",\n",
    "        \"#B07AA1\",\n",
    "        \"#FF9DA7\",\n",
    "        \"#9C755F\",\n",
    "        \"#BAB0AB\",\n",
    "    ]\n",
    "\n",
    "    # --- Marker styles (reused from inspired function) ---\n",
    "    marker_styles = [\"o\", \"s\", \"D\", \"^\", \"v\", \"<\", \">\", \"P\", \"X\", \"*\"]\n",
    "    marker_styles = (marker_styles * ((len(losses_list) // len(marker_styles)) + 1))[\n",
    "        : len(losses_list)\n",
    "    ]\n",
    "\n",
    "    num_algos = len(losses_list)\n",
    "    if titles is None:\n",
    "        titles = [f\"Algorithm {i + 1}\" for i in range(num_algos)]\n",
    "    constraint_thresholds = np.atleast_1d(constraint_thresholds)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(9, 11))\n",
    "    ax_loss, ax_constr = axes\n",
    "\n",
    "    # --- LOSS PLOT ---\n",
    "    for j, (loss, loss_std) in enumerate(zip(losses_list, losses_std_list)):\n",
    "        x = np.arange(len(loss))\n",
    "        color = colors[j % len(colors)]\n",
    "\n",
    "        if log_loss:\n",
    "            loss = np.log(loss)\n",
    "\n",
    "        upper = loss + std_multiplier * loss_std\n",
    "        lower = loss - std_multiplier * loss_std\n",
    "\n",
    "        # Mean curve\n",
    "        ax_loss.plot(x, loss, lw=2.8, color=color, label=titles[j])\n",
    "        # Std shading\n",
    "        ax_loss.fill_between(x, lower, upper, color=color, alpha=0.09)\n",
    "\n",
    "        # Eval points\n",
    "        if eval_points is not None:\n",
    "            if isinstance(eval_points, int):\n",
    "                idx = np.arange(0, len(loss), eval_points)\n",
    "            else:\n",
    "                idx = np.array(eval_points)\n",
    "                idx = idx[idx < len(loss)]\n",
    "            ax_loss.plot(\n",
    "                x[idx],\n",
    "                loss[idx],\n",
    "                marker_styles[j],\n",
    "                color=color,\n",
    "                markersize=7,\n",
    "                alpha=0.9,\n",
    "            )\n",
    "\n",
    "    ax_loss.set_ylabel(\"Mean Loss\")\n",
    "    ax_loss.set_title(\"Loss Comparison\")\n",
    "    ax_loss.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "    ax_loss.legend(fontsize=9)\n",
    "\n",
    "    # --- CONSTRAINT PLOT ---\n",
    "    for j, constraints in enumerate(constraints_list):\n",
    "        color = colors[j % len(colors)]\n",
    "\n",
    "        # to make smooth constraints\n",
    "        eval_points = 1\n",
    "\n",
    "        if log_constraints:\n",
    "            constraints = np.log(np.array(constraints))\n",
    "        x = np.arange(constraints.shape[1])\n",
    "\n",
    "        c_min = np.min(constraints, axis=0)\n",
    "        c_max = np.max(constraints, axis=0)\n",
    "\n",
    "        # Fill min-max range\n",
    "        ax_constr.fill_between(x, c_min, c_max, color=color, alpha=0.1, label=titles[j])\n",
    "\n",
    "        # Plot mean curves with markers\n",
    "        for c_mean in constraints:\n",
    "            # ax_constr.plot(x, c_mean, lw=0.3, color=color, alpha=0.01)\n",
    "            if eval_points is not None:\n",
    "                if isinstance(eval_points, int):\n",
    "                    idx = np.arange(0, len(c_mean), eval_points)\n",
    "                else:\n",
    "                    idx = np.array(eval_points)\n",
    "                    idx = idx[idx < len(c_mean)]\n",
    "                ax_constr.plot(\n",
    "                    x[idx],\n",
    "                    c_mean[idx],\n",
    "                    # marker_styles[j],\n",
    "                    color=color,\n",
    "                    # markersize=5,\n",
    "                    lw=0.7,\n",
    "                    alpha=0.6,\n",
    "                )\n",
    "\n",
    "    # Threshold lines\n",
    "    for th in constraint_thresholds:\n",
    "        y = np.log(th) if log_constraints else th\n",
    "        ax_constr.axhline(y, color=\"red\", linestyle=\"--\", lw=1.4, label=\"Threshold\")\n",
    "\n",
    "    ax_constr.set_ylabel(\"Log Constraint\" if log_constraints else \"Constraint\")\n",
    "    ax_constr.set_xlabel(\"Epoch\")\n",
    "    ax_constr.set_title(\"Constraint Comparison\")\n",
    "    ax_constr.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "    ax_constr.legend(fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "thresholds = [constraint_bounds[0]]\n",
    "\n",
    "plot_losses_and_constraints_single(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    thresholds,\n",
    "    log_constraints=True,\n",
    "    log_loss=False,\n",
    "    titles=[\n",
    "        \"Unconstrained Adam\",\n",
    "        \"SSL-ALM-SGD\",\n",
    "        \"SSL-ALM-Adam\",\n",
    "        \"SSG\",\n",
    "        \"Cooper-ALM\",\n",
    "        \"SFW\",\n",
    "        \"PBM\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec5000",
   "metadata": {},
   "source": [
    "Note the unconstrained Adam is in this case superior to the constrained optimization in terms of the regression loss. This is due to the high restriction of the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3d877",
   "metadata": {},
   "source": [
    "# Stochastic constraints: ACSIncome - Uniform Positive Rate \n",
    "\n",
    "To benchmark the stochastic constraints, we define the regression problem for binary classification of income above a threshold as\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\min_{x \\in \\mathbb{R}^n} \\; \\mathbb{E}[f(x,\\xi)] \\\\\n",
    "& \\text{s.t.} \\quad \\mathbb{E}[c(x,\\zeta)] \\le 0, \\quad\n",
    "\\end{aligned}\n",
    "\n",
    "where the $f: \\mathbb{R}^n \\times  \\rightarrow \\mathbb{R}$ is a loss, the constraint $c : \\mathbb{R}^n \\times Z \\rightarrow \\mathbb{R}$ is a norm of positive rate fair metric among the sensitive groups of $S = \\{$ man, woman $\\}$, of size $k=2$. That is, we aim to have an unbiased regression model $g$ that has the property of\n",
    "\n",
    "\\begin{aligned}\n",
    "\\forall S_k \\in S :\\; P\\big(g(x, \\zeta_i) = 1 \\mid S_k = 1\\big)\n",
    "= P\\big(g(x, \\zeta_i) = 1\\big).\n",
    "\\end{aligned}\n",
    "\n",
    "Such statistics gives us a violation vector $\\bold{v} \\in \\mathbb{R}^k$, we define the constraint as $c(x, \\zeta) = || \\bold{v}(x, \\zeta) ||_p$. We choose a Manhatton $p=1$. For more details about the fairness metrics see [Fairret](https://openreview.net/pdf?id=NnyD0Rjx2B). Such model should be good at classification while maintaining the fairness among sensitive demographics groups.\n",
    "\n",
    "#### 1. Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43150d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the torch seed here\n",
    "seed_n = 1\n",
    "n_epochs = 60\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# log path file\n",
    "log_path = \"./data/logs/log_benchmark_stochastic.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb809404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "# get the dataset\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=128, drop_last=True\n",
    ")\n",
    "# create a dataloader from the sampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "# define the criterion\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.2  # define the bound on the criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create small FC network\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# create the SSLALM optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=0.001,\n",
    ")\n",
    "\n",
    "# define epochs + loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "Adam_S_loss_log_plotting = []  # mean\n",
    "Adam_S_c_log_plotting = []  # mean\n",
    "Adam_S_loss_std_log_plotting = []  # std\n",
    "Adam_S_c_std_log_plotting = []  # std\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "\n",
    "        # calculate the fair constraint violation\n",
    "        fair_constraint = fair_loss\n",
    "\n",
    "        # save the fair loss violation for logging\n",
    "        c_log.append([fair_loss.detach().item()])\n",
    "\n",
    "        # calculate primal loss and grad\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    Adam_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    Adam_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    Adam_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    Adam_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses_and_constraints_single_stochastic(\n",
    "    losses_list,\n",
    "    losses_std_list,\n",
    "    constraints_list,\n",
    "    constraints_std_list,\n",
    "    constraint_thresholds,\n",
    "    titles=None,\n",
    "    eval_points=2,\n",
    "    std_multiplier=2,\n",
    "    log_constraints=False,\n",
    "):\n",
    "    # --- Color palette: Tableau 10 ---\n",
    "    colors = [\n",
    "        \"#4E79A7\",\n",
    "        \"#F28E2B\",\n",
    "        \"#E15759\",\n",
    "        \"#76B7B2\",\n",
    "        \"#59A14F\",\n",
    "        \"#EDC948\",\n",
    "        \"#B07AA1\",\n",
    "        \"#FF9DA7\",\n",
    "        \"#9C755F\",\n",
    "        \"#BAB0AB\",\n",
    "    ]\n",
    "\n",
    "    # --- Marker styles (reused from inspired function) ---\n",
    "    marker_styles = [\"o\", \"s\", \"D\", \"^\", \"v\", \"<\", \">\", \"P\", \"X\", \"*\"]\n",
    "    marker_styles = (marker_styles * ((len(losses_list) // len(marker_styles)) + 1))[\n",
    "        : len(losses_list)\n",
    "    ]\n",
    "\n",
    "    num_algos = len(losses_list)\n",
    "    if titles is None:\n",
    "        titles = [f\"Algorithm {i + 1}\" for i in range(num_algos)]\n",
    "    constraint_thresholds = np.atleast_1d(constraint_thresholds)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(9, 11))\n",
    "    ax_loss, ax_constr = axes\n",
    "\n",
    "    # --- LOSS PLOT ---\n",
    "    for j, (loss, loss_std) in enumerate(zip(losses_list, losses_std_list)):\n",
    "        x = np.arange(len(loss))\n",
    "        color = colors[j % len(colors)]\n",
    "        upper = loss + std_multiplier * loss_std\n",
    "        lower = loss - std_multiplier * loss_std\n",
    "\n",
    "        # Mean curve\n",
    "        ax_loss.plot(x, loss, lw=2.2, color=color, label=titles[j])\n",
    "        # Std shading\n",
    "        ax_loss.fill_between(x, lower, upper, color=color, alpha=0.15)\n",
    "\n",
    "        # Eval points\n",
    "        if eval_points is not None:\n",
    "            if isinstance(eval_points, int):\n",
    "                idx = np.arange(0, len(loss), eval_points)\n",
    "            else:\n",
    "                idx = np.array(eval_points)\n",
    "                idx = idx[idx < len(loss)]\n",
    "            ax_loss.plot(\n",
    "                x[idx],\n",
    "                loss[idx],\n",
    "                marker_styles[j],\n",
    "                color=color,\n",
    "                markersize=6,\n",
    "                alpha=0.8,\n",
    "            )\n",
    "\n",
    "    ax_loss.set_ylabel(\"Mean Loss\")\n",
    "    ax_loss.set_title(\"Loss Comparison\")\n",
    "    ax_loss.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "    ax_loss.legend(fontsize=9)\n",
    "\n",
    "    # --- CONSTRAINT PLOT ---\n",
    "    for j, (constraints, constraints_std) in enumerate(\n",
    "        zip(constraints_list, constraints_std_list)\n",
    "    ):\n",
    "        color = colors[j % len(colors)]\n",
    "        constraints = np.array(constraints)\n",
    "        constraints_std = np.array(constraints_std)\n",
    "        x = np.arange(constraints.shape[1])\n",
    "\n",
    "        c_min = np.min(constraints - std_multiplier * constraints_std, axis=0)\n",
    "        c_max = np.max(constraints + std_multiplier * constraints_std, axis=0)\n",
    "\n",
    "        # Fill min-max range\n",
    "        ax_constr.fill_between(\n",
    "            x, c_min, c_max, color=color, alpha=0.15, label=titles[j]\n",
    "        )\n",
    "\n",
    "        # Plot mean curves with markers\n",
    "        for c_mean in constraints:\n",
    "            ax_constr.plot(x, c_mean, lw=1.8, color=color, alpha=0.7)\n",
    "            if eval_points is not None:\n",
    "                if isinstance(eval_points, int):\n",
    "                    idx = np.arange(0, len(c_mean), eval_points)\n",
    "                else:\n",
    "                    idx = np.array(eval_points)\n",
    "                    idx = idx[idx < len(c_mean)]\n",
    "                ax_constr.plot(\n",
    "                    x[idx],\n",
    "                    c_mean[idx],\n",
    "                    marker_styles[j],\n",
    "                    color=color,\n",
    "                    markersize=5,\n",
    "                    alpha=0.8,\n",
    "                )\n",
    "\n",
    "    # Threshold lines\n",
    "    for th in constraint_thresholds:\n",
    "        y = np.log(th) if log_constraints else th\n",
    "        ax_constr.axhline(y, color=\"red\", linestyle=\"--\", lw=1.4, label=\"Threshold\")\n",
    "\n",
    "    ax_constr.set_ylabel(\"Log Constraint\" if log_constraints else \"Constraint\")\n",
    "    ax_constr.set_xlabel(\"Epoch\")\n",
    "    ax_constr.set_title(\"Constraint Comparison\")\n",
    "    ax_constr.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "    ax_constr.legend(fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=[np.array(Adam_S_loss_log_plotting)],\n",
    "    constraints=[np.array(Adam_S_c_log_plotting).T],\n",
    "    losses_std=[np.array(Adam_S_loss_std_log_plotting)],\n",
    "    constraints_std=[np.array(Adam_S_c_std_log_plotting).T],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b932f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "thresholds = [fair_crit_bound]\n",
    "\n",
    "plot_losses_and_constraints_single_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    titles=[\"Unconstrained Adam\"],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce6b23",
   "metadata": {},
   "source": [
    "#### 2.1 SSL-ALM SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.algorithms import SSLALM\n",
    "from torch.nn import Sequential\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "optimizer = SSLALM(\n",
    "    params=model_con.parameters(),\n",
    "    m=1,  # number of constraints - one in our case\n",
    "    lr=0.1,  # primal variable lr\n",
    "    dual_lr=0.05,  # lr of a dual ALM variable\n",
    "    dual_bound=5,\n",
    "    rho=1,  # rho penalty in ALM parameter\n",
    "    mu=2,  # smoothing parameter\n",
    ")\n",
    "\n",
    "# add slack variables - to create the equality from the inequalities\n",
    "slack_vars = torch.zeros(1, requires_grad=True)\n",
    "optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75970986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSLALM_SGD_S_loss_log_plotting = []  # mean\n",
    "SSLALM_SGD_S_c_log_plotting = []  # mean\n",
    "SSLALM_SGD_S_loss_std_log_plotting = []  # std\n",
    "SSLALM_SGD_S_c_std_log_plotting = []  # std\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "\n",
    "        # calculate the fair constraint violation\n",
    "        fair_constraint = fair_loss + slack_vars[0] - fair_crit_bound\n",
    "        fair_constraint.backward(retain_graph=True)\n",
    "\n",
    "        # perform the dual step variable + save the dual grad for later\n",
    "        optimizer.dual_step(0, c_val=fair_constraint)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the fair loss violation for logging\n",
    "        c_log.append([fair_loss.detach().item()])\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "        # calculate primal loss and grad\n",
    "        loss = (\n",
    "            criterion(out, batch_label) + 0 * slack_vars[0]\n",
    "        )  # need to add 0*slack variables for autograd to work\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "        with torch.no_grad():\n",
    "            for s in slack_vars:\n",
    "                if s < 0:\n",
    "                    s.zero_()\n",
    "\n",
    "    optimizer.dual_lr *= 0.95\n",
    "    SSLALM_SGD_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSLALM_SGD_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSLALM_SGD_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    SSLALM_SGD_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fe6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSLALM_SGD_S_loss_log_plotting)]\n",
    "constraints += [np.array(SSLALM_SGD_S_c_log_plotting).T]\n",
    "losses_std += [np.array(SSLALM_SGD_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(SSLALM_SGD_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fd5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "thresholds = [fair_crit_bound]\n",
    "\n",
    "plot_losses_and_constraints_single_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    titles=[\"Unconstrained Adam\", \"SSL-ALM-SGD\"],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed004b",
   "metadata": {},
   "source": [
    "#### 2.2 SSL-ALM Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5223f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "    \n",
    "optimizer = SSLALM_Adam(\n",
    "    params=model_con.parameters(),\n",
    "    m=1,  # number of constraints - one in our case\n",
    "    lr=0.01,  # primal variable lr\n",
    "    dual_lr=0.05,  # lr of a dual ALM variable\n",
    "    dual_bound=5,\n",
    "    rho=1,  # rho penalty in ALM parameter\n",
    "    mu=2,  # smoothing parameter\n",
    ")\n",
    "\n",
    "# add slack variables - to create the equality from the inequalities    \n",
    "slack_vars = torch.zeros(1, requires_grad=True)\n",
    "optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSLALM_S_loss_log_plotting = []  # mean\n",
    "SSLALM_S_c_log_plotting = []  # mean\n",
    "SSLALM_S_loss_std_log_plotting = []  # std\n",
    "SSLALM_S_c_std_log_plotting = []  # std\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "\n",
    "        # calculate the fair constraint violation\n",
    "        fair_constraint = fair_loss + slack_vars[0] - fair_crit_bound\n",
    "        fair_constraint.backward(retain_graph=True)\n",
    "\n",
    "        # perform the dual step variable + save the dual grad for later\n",
    "        optimizer.dual_step(0, c_val=fair_constraint)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the fair loss violation for logging\n",
    "        c_log.append([fair_loss.detach().item()])\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "        # calculate primal loss and grad\n",
    "        loss = criterion(out, batch_label) + 0 * slack_vars[0]\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "        with torch.no_grad():\n",
    "            for s in slack_vars:\n",
    "                if s < 0:\n",
    "                    s.zero_()\n",
    "\n",
    "    optimizer.dual_lr *= 0.95\n",
    "    SSLALM_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSLALM_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSLALM_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    SSLALM_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497fd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSLALM_S_loss_log_plotting)]\n",
    "constraints += [np.array(SSLALM_S_c_log_plotting).T]\n",
    "losses_std += [np.array(SSLALM_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(SSLALM_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "thresholds = [fair_crit_bound]\n",
    "\n",
    "plot_losses_and_constraints_single_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    titles=[\"Unconstrained Adam\", \"SSL-ALM-SGD\", \"SSL-ALM-Adam\"],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077781bb",
   "metadata": {},
   "source": [
    "#### 3. Switching-Subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "# get the dataset\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=128, drop_last=True\n",
    ")\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "# define the criterion\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be83cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.algorithms import SSG\n",
    "from torch.nn import Sequential\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# same network size for all algorithms\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "optimizer = SSG(params=model_con.parameters(), m=1, lr=0.1, dual_lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e80eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSG_S_loss_log_plotting = []  # mean\n",
    "SSG_S_c_log_plotting = []  # mean\n",
    "SSG_S_loss_std_log_plotting = []  # std\n",
    "SSG_S_c_std_log_plotting = []  # std\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "        fair_constraint = torch.max(fair_loss - fair_crit_bound, torch.zeros(1))\n",
    "        fair_constraint.backward(retain_graph=True)\n",
    "\n",
    "        # compute the grad of the constraints\n",
    "        optimizer.dual_step(0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the constraint value\n",
    "        c_log.append([fair_loss.detach().item()])\n",
    "\n",
    "        # calculate loss and grad\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        optimizer.step(fair_constraint)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    SSG_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSG_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSG_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    SSG_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf100af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSG_S_loss_log_plotting)]\n",
    "constraints += [np.array(SSG_S_c_log_plotting).T]\n",
    "losses_std += [np.array(SSG_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(SSG_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2995877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "thresholds = [fair_crit_bound]\n",
    "\n",
    "plot_losses_and_constraints_single_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    titles=[\"Unconstrained Adam\", \"SSL-ALM-SGD\", \"SSL-ALM-Adam\", \"SSG\"],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ab741",
   "metadata": {},
   "source": [
    "#### 4. Cooper - Augmented Lagrangian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.loss import NormLoss\n",
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# get the dataset\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=128, drop_last=True\n",
    ")\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "# define the criterion\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create small FC network\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# define epochs + loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cooper\n",
    "\n",
    "# select device\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # define GPU if available\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "# define the problem\n",
    "class ACS_Deterministic_Constr_Fair_ALM(cooper.ConstrainedMinimizationProblem):\n",
    "    def __init__(self, formulation_type):\n",
    "        super().__init__()\n",
    "\n",
    "        # define the number of multipliers - same as the number of constraints\n",
    "        if not formulation_type.expects_multiplier:\n",
    "            self.multiplier = None\n",
    "        else:\n",
    "            self.multiplier = cooper.multipliers.DenseMultiplier(\n",
    "                num_constraints=1, device=DEVICE\n",
    "            )\n",
    "\n",
    "        # add the penalty coefficient - augmented lagrangian\n",
    "        if not formulation_type.expects_penalty_coefficient:\n",
    "            self.penalty = None\n",
    "        else:\n",
    "            self.penalty = cooper.penalty_coefficients.DensePenaltyCoefficient(\n",
    "                init=torch.tensor(1.0, device=DEVICE),\n",
    "            )\n",
    "\n",
    "        # inequality constraints on the norm of the matrix\n",
    "        self.constraint = cooper.Constraint(\n",
    "            constraint_type=cooper.ConstraintType.INEQUALITY,\n",
    "            formulation_type=formulation_type,\n",
    "            multiplier=self.multiplier,\n",
    "            penalty_coefficient=self.penalty,\n",
    "        )\n",
    "\n",
    "    # function that computes the constraints and the loss function\n",
    "    def compute_cmp_state(self, model, inputs, targets, batch_sens):\n",
    "        # compute the yhat loss\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, targets)\n",
    "\n",
    "        # compute fair metric\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "        fair_constraint = fair_loss - fair_crit_bound\n",
    "\n",
    "        # compute the constraint violation\n",
    "        constraint_state = cooper.ConstraintState(violation=fair_constraint)\n",
    "        observed_constraints = {self.constraint: constraint_state}\n",
    "\n",
    "        return cooper.CMPState(loss=loss, observed_constraints=observed_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6624c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the problem\n",
    "problem = ACS_Deterministic_Constr_Fair_ALM(cooper.formulations.AugmentedLagrangian)\n",
    "\n",
    "# flag of dual variables\n",
    "has_dual_variables = problem.multiplier is not None\n",
    "\n",
    "# create the primal optimizer\n",
    "primal_optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# create the dual optimizer\n",
    "if has_dual_variables:\n",
    "    dual_optimizer = torch.optim.SGD(problem.dual_parameters(), lr=0.05, maximize=True)\n",
    "\n",
    "    # combine the optimizers\n",
    "    constrained_optimizer = cooper.optim.SimultaneousOptimizer(\n",
    "        cmp=problem,\n",
    "        primal_optimizers=primal_optimizer,\n",
    "        dual_optimizers=dual_optimizer,\n",
    "    )\n",
    "else:\n",
    "    # Formulations without dual variables, such as the Quadratic Penalty\n",
    "    # formulation, do not require a dual optimizer\n",
    "    constrained_optimizer = cooper.optim.UnconstrainedOptimizer(\n",
    "        cmp=problem,\n",
    "        primal_optimizers=primal_optimizer,\n",
    "    )\n",
    "\n",
    "# Increase the penalty coefficient by `increment` if the constraint is violate by more\n",
    "# than `violation_tolerance`- keep this the same - since its not included in SSL-ALM implementation\n",
    "penalty_scheduler = cooper.penalty_coefficients.AdditivePenaltyCoefficientUpdater(\n",
    "    increment=0.0,\n",
    "    violation_tolerance=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "Cooper_S_c_log_plotting = []\n",
    "Cooper_S_loss_log_plotting = []\n",
    "Cooper_S_loss_std_log_plotting = []\n",
    "Cooper_S_c_std_log_plotting = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "    penalty_coefficient_values = []\n",
    "\n",
    "    # alloc norm values array\n",
    "    norm_values_c = []\n",
    "\n",
    "    # go through all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate the constraints - for logging purposes\n",
    "        out = model(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "        fair_constraint = fair_loss - fair_crit_bound\n",
    "        c_log.append([fair_loss.detach().item()])\n",
    "\n",
    "        # compute the loss - just for logging purposes\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss_log.append(loss.item())\n",
    "\n",
    "        # compute the states\n",
    "        compute_cmp_state_kwargs = {\n",
    "            \"model\": model,\n",
    "            \"inputs\": batch_input,\n",
    "            \"targets\": batch_label,\n",
    "            \"batch_sens\": batch_sens,\n",
    "        }\n",
    "        roll_out = constrained_optimizer.roll(\n",
    "            compute_cmp_state_kwargs=compute_cmp_state_kwargs\n",
    "        )\n",
    "\n",
    "        # Update the penalty coefficient\n",
    "        constraint_state = roll_out.cmp_state.observed_constraints[problem.constraint]\n",
    "        penalty_scheduler.update_penalty_coefficient_(\n",
    "            problem.constraint, constraint_state\n",
    "        )\n",
    "\n",
    "        # get the dual variables and the coefficients\n",
    "        multiplier_value = (\n",
    "            problem.multiplier.weight.detach().numpy() if has_dual_variables else None\n",
    "        )\n",
    "        penalty_coefficient_value = problem.constraint.penalty_coefficient().item()\n",
    "\n",
    "        # save the duals and penalty coefficients\n",
    "        duals_log.append(multiplier_value)\n",
    "        penalty_coefficient_values.append(penalty_coefficient_value)\n",
    "\n",
    "    # save the epoch values for plotting\n",
    "    Cooper_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    Cooper_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    Cooper_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    Cooper_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    # print out the epoch values\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}, \"\n",
    "        f\"penalty coefficients: {np.mean(penalty_coefficient_values, axis=0)}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(Cooper_S_loss_log_plotting)]\n",
    "constraints += [np.array(Cooper_S_c_log_plotting).T]\n",
    "losses_std += [np.array(Cooper_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(Cooper_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff35cbb",
   "metadata": {},
   "source": [
    "#### 5. PBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de481c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.optim.PBM import PBM\n",
    "\n",
    "# set the seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# create small FC network - same as the other algorithms\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "model = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size1, latent_size2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(latent_size2, 1),\n",
    ")\n",
    "\n",
    "# create the PBM optimizer\n",
    "optimizer = PBM(params=model.parameters(), m=1, lr=0.001, dual_beta=0.95, mu=0.1, penalty_update_m='CONST', barrier=\"quadratic_logarithmic\")\n",
    "\n",
    "# define the criterion\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic)\n",
    "fair_crit_bound = 0.2\n",
    "\n",
    "# alloc arrays for plotting\n",
    "SSG_S_loss_log_plotting = []  # mean\n",
    "SSG_S_c_log_plotting = []  # mean\n",
    "SSG_S_loss_std_log_plotting = []  # std\n",
    "SSG_S_c_std_log_plotting = []  # std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2585797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model(batch_input)\n",
    "        fair_loss = fair_criterion(out, batch_sens)\n",
    "        fair_constraint = fair_loss - fair_crit_bound\n",
    "\n",
    "        # compute the grad of the constraints\n",
    "        optimizer.dual_step(0, fair_constraint)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the constraint value\n",
    "        c_log.append([fair_loss.detach().item()])\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "        # calculate loss and grad\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    SSG_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSG_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSG_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    SSG_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.mean(c_log, axis=0)}, \"\n",
    "        f\"dual: {np.mean(duals_log, axis=0)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSG_S_loss_log_plotting)]\n",
    "constraints += [np.array(SSG_S_c_log_plotting).T]\n",
    "losses_std += [np.array(SSG_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(SSG_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49639a8f",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Now we plot all results in a single table to compare all algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "thresholds = [fair_crit_bound]\n",
    "\n",
    "plot_losses_and_constraints_single_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    titles=[\n",
    "        \"Unconstrained Adam\",\n",
    "        \"SSL-ALM-SGD\",\n",
    "        \"SSL-ALM-Adam\",\n",
    "        \"SSG\",\n",
    "        \"Cooper-ALM-IS\",\n",
    "        \"PBM\"\n",
    "    ],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38529fa8",
   "metadata": {},
   "source": [
    "It is interesting to see that in the stochastic case, the constraints are not as restrictive as in the deterministic case, directly implying the small difference in the loss between the Unconstrained Adam and the best constrained SSL-ALM-Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091c889",
   "metadata": {},
   "source": [
    "# Stochastic Constraints: ACSIncome - Equal Opportunity\n",
    "\n",
    "TODO: add test constr + loss\n",
    "\n",
    "To benchmark a more complicated problem, we define the regression problem for binary classification of income above a threshold as\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\min_{x \\in \\mathbb{R}^n} \\; \\mathbb{E}[f(x,\\xi)] \\\\\n",
    "& \\text{s.t.} \\quad \\mathbb{E}[c(x,\\zeta)] \\le 0, \\quad\n",
    "\\end{aligned}\n",
    "\n",
    "where the $f: \\mathbb{R}^n \\times  \\rightarrow \\mathbb{R}$ is a loss, the constraint $c : \\mathbb{R}^n \\times Z \\rightarrow \\mathbb{R}^m$ is the equal opportunity of the sensitive groups of $S = \\{$ man, woman, (married / divorced / single) $\\}$ of size $k$. That is, we aim to have an unbiased regression model $g$ that has the property of\n",
    "\n",
    "\\begin{aligned}\n",
    "\\forall S_k, S_j \\in S, S_j \\neq S_k :\\; | \\mathbb{E}\\big(g(x, \\zeta_i) = 1 \\mid S_k = 1\\big) - \\mathbb{E}\\big(g(x, \\zeta_i) = 1\\mid S_j = 1\\big)| \\leq \\epsilon.\n",
    "\\end{aligned}\n",
    "\n",
    "The number of constraints in this case is of order $\\mathcal{O}(k^2)$. Concretely, $k=6$ and so $m=30$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20474c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define the torch seed here\n",
    "seed_n = 1\n",
    "n_epochs = 60\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# log path file\n",
    "log_path = \"./data/logs/log_benchmark_stochastic_2.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# resave the path\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=[],\n",
    "    constraints=[],\n",
    "    losses_std=[],\n",
    "    constraints_std=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome, generate_categories\n",
    "from itertools import product\n",
    "\n",
    "# load folktables data\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "acs_data = data_source.get_data(states=[\"VA\"], download=True)\n",
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(\n",
    "    features=ACSIncome.features, definition_df=definition_df\n",
    ")\n",
    "df_feat, df_labels, _ = ACSIncome.df_to_pandas(\n",
    "    acs_data, categories=categories, dummies=True\n",
    ")\n",
    "\n",
    "# split the data on groups, labels and features - the features should not have the sensitive feature\n",
    "sens_cols = [\n",
    "    \"SEX_Female\",\n",
    "    \"SEX_Male\",\n",
    "    \"MAR_Divorced\",\n",
    "    \"MAR_Married\",\n",
    "    \"MAR_Never married or under 15 years old\",\n",
    "]\n",
    "features = df_feat.drop(columns=sens_cols).to_numpy(dtype=\"float\")\n",
    "groups = df_feat[sens_cols].to_numpy(dtype=\"float\")\n",
    "labels = df_labels.to_numpy(dtype=\"float\")\n",
    "\n",
    "# Split columns into sex and marital\n",
    "sex_cols = [\"SEX_Female\", \"SEX_Male\"]\n",
    "mar_cols = [\n",
    "    \"MAR_Divorced\",\n",
    "    \"MAR_Married\",\n",
    "    \"MAR_Never married or under 15 years old\",\n",
    "]\n",
    "\n",
    "# Convert each row to sex index and marital index\n",
    "sex_idx = df_feat[sex_cols].values.argmax(axis=1)\n",
    "mar_idx = df_feat[mar_cols].values.argmax(axis=1)\n",
    "\n",
    "# Number of unique combinations\n",
    "num_groups = len(sex_cols) * len(mar_cols)\n",
    "\n",
    "# Map each combination to a unique index\n",
    "group_indices = sex_idx * len(mar_cols) + mar_idx  # shape: (num_samples,)\n",
    "\n",
    "# One-hot encode the combinations\n",
    "groups_onehot = np.eye(num_groups)[group_indices]\n",
    "\n",
    "# Create dictionary mapping index to combination\n",
    "group_dict = {}\n",
    "for i, (s, m) in enumerate(product(sex_cols, mar_cols)):\n",
    "    group_dict[i] = f\"{s} + {m}\"\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    features, labels, groups_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20339fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the statistics\n",
    "for idx in group_dict:\n",
    "    print(f\"{group_dict[idx]}, : {(groups_onehot[:, idx] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "sens_train = torch.tensor(groups_train)\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "labels_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "sens_test = torch.tensor(groups_test)\n",
    "dataset_test = torch.utils.data.TensorDataset(features_test, sens_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic.linear_fractional import PositiveRate\n",
    "\n",
    "def positive_rate(out_batch, batch_sens, prob_f=torch.nn.functional.sigmoid):\n",
    "    \"\"\"\n",
    "    Calculates the positive rate vector based on the given outputs of the model for the given groups. \n",
    "    \n",
    "    \"\"\"\n",
    "    # compute the probabilities - using sigmoid (since that is used )\n",
    "    if prob_f is None: \n",
    "        preds = out_batch\n",
    "    else: \n",
    "        preds = prob_f( out_batch )\n",
    "    pr = PositiveRate()\n",
    "    probs_per_group = pr(preds, batch_sens)  # P(y=1|Sk = 1)\n",
    "    return probs_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP FOR BENCHMARK TASK - common elements for all optimizers\n",
    "from humancompatible.train.benchmark.algorithms.optim_wrapper import OptimLoopWrapper\n",
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "from torch.nn import Sequential\n",
    "import torch\n",
    "latent_size1 = 64\n",
    "latent_size2 = 32\n",
    "def create_model():\n",
    "    model = Sequential(\n",
    "        torch.nn.Linear(features_train.shape[1], latent_size1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(latent_size1, latent_size2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(latent_size2, 1),\n",
    "    )    \n",
    "    return model\n",
    "\n",
    "m = 30 # number of constraints\n",
    "torch.manual_seed(seed_n)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "bounds = [0.1]*m\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=120, drop_last=True,extend_groups=[0,2,3,5]\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, sampler=sampler)\n",
    "\n",
    "# function to calculate the constraints\n",
    "def groupwise_posrate_constraint(out, batch_sens):\n",
    "    pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "    # compute the equal opportunity constraint\n",
    "    constraints = []\n",
    "    current_constr = 0\n",
    "    for i in range(0, len(pos_rate_pergroup)):\n",
    "        for j in range(0, len(pos_rate_pergroup)):\n",
    "            if i != j:\n",
    "                constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j]\n",
    "                constraints.append(constr_ij)\n",
    "                current_constr += 1\n",
    "    return constraints\n",
    "\n",
    "# forward function: expects to have (model, batch) as arguments\n",
    "def fwd_unconstrained(model, batch):\n",
    "    batch_inputs, _, batch_labels = batch\n",
    "    out = model(batch_inputs)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    return loss\n",
    "\n",
    "def fwd_constrained(model, batch, slack_vars = None):\n",
    "    if slack_vars is not None:\n",
    "        with torch.no_grad():\n",
    "            for s in slack_vars:\n",
    "                if s < 0:\n",
    "                    s.zero_()\n",
    "    batch_inputs, batch_sens, batch_labels = batch\n",
    "    # for some unrelated reason theres an extra dimension in the batch?????\n",
    "    batch_inputs, batch_sens, batch_labels = batch_inputs[0], batch_sens[0], batch_labels[0]\n",
    "    out = model(batch_inputs)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    constraints = groupwise_posrate_constraint(out, batch_sens)\n",
    "    if slack_vars:\n",
    "        constraints_bounded_eq = [con - bounds[i] + slack_vars[i] for i, con in enumerate(constraints)]\n",
    "    else:\n",
    "        constraints_bounded_eq = [torch.max(con - bounds[i], torch.zeros_like(con)).to(torch.float) for i, con in enumerate(constraints)]\n",
    "    return loss, constraints_bounded_eq\n",
    "\n",
    "\n",
    "# ssw has an old api, so need to redefine some stuff\n",
    "def fwd_ssw(model, batch):\n",
    "    batch_inputs, batch_sens, batch_labels = batch\n",
    "    # for some unrelated reason theres an extra dimension in the batch?????\n",
    "    batch_inputs, batch_sens, batch_labels = batch_inputs[0], batch_sens[0], batch_labels[0]\n",
    "    out = model(batch_inputs)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    constraints = groupwise_posrate_constraint(out, batch_sens)\n",
    "    constraints_bounded_eq = torch.max(max([con - bounds[i] for i, con in enumerate(constraints)]), torch.zeros(1))\n",
    "    return loss, constraints_bounded_eq\n",
    "\n",
    "def train_iter_ssw(optimizer, model, batch):\n",
    "    loss, constraint = fwd_ssw(model, batch)\n",
    "    constraint.backward(retain_graph=True)\n",
    "    optimizer.dual_step(0)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step(constraint)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "@torch.inference_mode\n",
    "def eval(model, batch):\n",
    "    inputs, sens, labels = batch\n",
    "    out = model(inputs)\n",
    "    loss = criterion(out, labels)\n",
    "    constraints = groupwise_posrate_constraint(out, sens)\n",
    "    constraints = [c.detach().numpy().item() for c in constraints]\n",
    "    return loss.detach().numpy().item(), constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "model = create_model()\n",
    "# first backward pass sometimes takes ages\n",
    "model(features_train[0]).backward()\n",
    "model.zero_grad()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=0.001,\n",
    ")\n",
    "\n",
    "adam = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_unconstrained,\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, sens_train, labels_train), 'val': (features_test, sens_test, labels_test)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch=True\n",
    ")\n",
    "\n",
    "adam.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSLALM with Max\n",
    "from humancompatible.train.optim import SSLALM\n",
    "model = create_model()\n",
    "# first backward pass sometimes takes ages\n",
    "model(features_train[0]).backward()\n",
    "model.zero_grad()\n",
    "\n",
    "optimizer = SSLALM(params=model.parameters(), m=m, lr=0.01, dual_lr=0.05)\n",
    "\n",
    "alm = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_constrained,\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, sens_train, labels_train), 'val': (features_test, sens_test, labels_test)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch = False\n",
    ")\n",
    "\n",
    "alm.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSLALM-Adam with Max\n",
    "from humancompatible.train.optim import SSLALM_Adam\n",
    "model = create_model()\n",
    "model(features_train[0]).backward()\n",
    "model.zero_grad()\n",
    "\n",
    "optimizer = SSLALM_Adam(params=model.parameters(), m=m, lr=0.01, dual_lr=0.05)\n",
    "\n",
    "alm_adam = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_constrained,\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, sens_train, labels_train), 'val': (features_test, sens_test, labels_test)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch = False\n",
    ")\n",
    "\n",
    "alm_adam.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PBM with Max\n",
    "from humancompatible.train.optim.PBM import PBM\n",
    "model = create_model()\n",
    "model(features_train[0]).backward()\n",
    "\n",
    "optimizer = PBM(params=model.parameters(), m=m, lr=0.001, dual_beta=0.9, mu=0.1, penalty_update_m='CONST', barrier=\"quadratic_logarithmic\")\n",
    "\n",
    "pbm = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_constrained,\n",
    "    eval = eval,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, sens_train, labels_train), 'val': (features_test, sens_test, labels_test)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch = False\n",
    ")\n",
    "\n",
    "pbm.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSw with Max\n",
    "from humancompatible.train.optim import SSG\n",
    "model = create_model()\n",
    "model(features_train[0]).backward()\n",
    "\n",
    "optimizer = SSG(params=model.parameters(), m=1, lr=0.05)\n",
    "\n",
    "ssg = OptimLoopWrapper(\n",
    "    model = model,\n",
    "    fwd = fwd_ssw,\n",
    "    eval = eval,\n",
    "    train_iter=train_iter_ssw,\n",
    "    train_data = dataloader,\n",
    "    eval_data = {'train': (features_train, sens_train, labels_train), 'val': (features_test, sens_test, labels_test)},\n",
    "    optimizer = optimizer,\n",
    "    use_vanilla_torch = False\n",
    ")\n",
    "\n",
    "ssg.training_loop(epochs=n_epochs, max_iter=np.inf, max_runtime=np.inf, eval_every='epoch', save_every=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "algs = [adam, ssg, alm, alm_adam]\n",
    "f, axs = plt.subplots(nrows=2, ncols=len(algs), sharex=True, sharey='row')\n",
    "f.set_figwidth(len(algs)*4)\n",
    "for plt_idx, df in enumerate([pd.DataFrame(alg.history) for alg in algs]):\n",
    "    ax1 = axs[0][plt_idx]\n",
    "    ax2 = axs[1][plt_idx]\n",
    "    ax1.plot(df['train_loss'], color='red', ls='--')\n",
    "    for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_train')]:\n",
    "        ax2.plot(df[c], color='red', ls='-.', alpha=0.1)\n",
    "    for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_train')]:\n",
    "        ax2.plot(df[c], color='red', ls='-.', alpha=0.1)\n",
    "\n",
    "    ax1.plot(df['val_loss'], color='red', ls='--')\n",
    "    for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_val')]:\n",
    "        ax2.plot(df[c], color='blue', ls='-.', alpha=0.1)\n",
    "    for c in [c for c in df.columns if c.startswith('c_') and c.endswith('_val')]:\n",
    "        ax2.plot(df[c], color='blue', ls='-.', alpha=0.1)\n",
    "\n",
    "    ax2.hlines((bounds[0], -bounds[0]), 0, 60, color='black', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df8bbe",
   "metadata": {},
   "source": [
    "#### 1. SSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# get the dataset\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=120, drop_last=True\n",
    ")\n",
    "# create a dataloader from the sampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaff5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.optim.ssw import SSG\n",
    "from torch.nn import Sequential\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# same network size for all algorithms\n",
    "hsize1 = 64\n",
    "hsize2 = 32\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "number_of_constraints = 30\n",
    "threshold = 0.1\n",
    "\n",
    "optimizer = SSG(params=model_con.parameters(), m=1, lr=0.01, dual_lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ec69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSG_S_loss_log_plotting = []  # mean\n",
    "SSG_S_c_log_plotting = []  # mean\n",
    "SSG_S_loss_std_log_plotting = []  # std\n",
    "SSG_S_c_std_log_plotting = []  # std\n",
    "\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # prepare the max of the constraints\n",
    "        max_norm_viol = torch.zeros(1)\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij - threshold\n",
    "\n",
    "                    # save the max\n",
    "                    max_norm_viol = torch.max(max_norm_viol, constr_ij)\n",
    "\n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().numpy() + threshold)\n",
    "\n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        # calculate the Jacobian of the max-violating norm constraint\n",
    "        max_norm_viol.backward(retain_graph=True)\n",
    "\n",
    "        # save the gradient of the constraint\n",
    "        optimizer.dual_step(0)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step(max_norm_viol)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "\n",
    "\n",
    "    SSG_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSG_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSG_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    SSG_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.max(np.abs(np.mean(c_log, axis=0)))}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ad2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSG_S_loss_log_plotting)]\n",
    "constraints += [np.array(SSG_S_c_log_plotting).T]\n",
    "losses_std += [np.array(SSG_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(SSG_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17c5f7",
   "metadata": {},
   "source": [
    "#### 2. SSLALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# get the dataset\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=120, drop_last=True\n",
    ")\n",
    "# create a dataloader from the sampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "hsize1 = 128\n",
    "hsize2 = 64\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "# 10*10* 2 constraint - per subgroup inequality + 2 per inequality\n",
    "number_of_constraints = 30\n",
    "\n",
    "optimizer = SSLALM_Adam(\n",
    "    params=model_con.parameters(),\n",
    "    m=number_of_constraints,  # number of constraints - one in our case\n",
    "    lr=0.01,  # primal variable lr\n",
    "    dual_lr=0.05,  # lr of a dual ALM variable\n",
    "    dual_bound=5,\n",
    "    rho=1,  # rho penalty in ALM parameter\n",
    "    mu=2,  # smoothing parameter\n",
    ")\n",
    "\n",
    "# define the constraint violation band\n",
    "threshold = 0.1\n",
    "\n",
    "# add slack variables - to create the equality from the inequalities    \n",
    "slack_vars = torch.zeros(number_of_constraints, requires_grad=True)\n",
    "optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0128b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "SSLALM_S_loss_log_plotting = []  # mean\n",
    "SSLALM_S_c_log_plotting = []  # mean\n",
    "SSLALM_S_loss_std_log_plotting = []  # std\n",
    "SSLALM_S_c_std_log_plotting = []  # std\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij + slack_vars[current_constr] - threshold\n",
    "\n",
    "                    # perform the dual step on the constraint\n",
    "                    constr_ij.backward(retain_graph=True)\n",
    "\n",
    "                    # perform the dual step variable + save the dual grad for later\n",
    "                    optimizer.dual_step(current_constr, c_val=constr_ij)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().numpy() - slack_vars[current_constr].detach().numpy() + threshold)\n",
    "\n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        # calculate primal loss and grad\n",
    "        loss = 0.0\n",
    "        for i in range(0, number_of_constraints): # this is purely for pytorch not to complain about slack variables not being in the loss\n",
    "            loss += 0*slack_vars[i]\n",
    "\n",
    "        loss += criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "        # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "        with torch.no_grad():\n",
    "            for s in slack_vars:\n",
    "                if s < 0:\n",
    "                    s.zero_()\n",
    "\n",
    "    SSLALM_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    SSLALM_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    SSLALM_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    SSLALM_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.max(np.abs(np.mean(c_log, axis=0)))}, \"\n",
    "        f\"dual: {np.max(np.mean(duals_log, axis=0).mean())}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(SSLALM_S_loss_log_plotting)]\n",
    "constraints += [np.array(SSLALM_S_c_log_plotting).T]\n",
    "losses_std += [np.array(SSLALM_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(SSLALM_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00878ddc",
   "metadata": {},
   "source": [
    "#### 2. PBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# get the dataset\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=120, drop_last=True\n",
    ")\n",
    "# create a dataloader from the sampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "from humancompatible.train.optim.PBM import PBM\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "hsize1 = 128\n",
    "hsize2 = 64\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "# 5*6 constraint - per subgroup inequality + 2 per inequality\n",
    "number_of_constraints = 30\n",
    "\n",
    "optimizer = PBM(params=model_con.parameters(), m=number_of_constraints, lr=0.001, dual_beta=0.9, mu=0.1, penalty_update_m='CONST', barrier=\"quadratic_logarithmic\")\n",
    "\n",
    "# define the constraint violation band\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ae788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "PBM_S_loss_log_plotting = []  # mean\n",
    "PBM_S_c_log_plotting = []  # mean\n",
    "PBM_S_loss_std_log_plotting = []  # std\n",
    "PBM_S_c_std_log_plotting = []  # std\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij - threshold\n",
    "                    \n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().numpy() + threshold)\n",
    "\n",
    "                    # perform the dual step variable + save the dual grad for later\n",
    "                    optimizer.dual_step(current_constr, c_val=constr_ij)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        # compute the augemented objective loss\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "\n",
    "    PBM_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    PBM_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    PBM_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    PBM_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.max(np.abs(np.mean(c_log, axis=0)))}, \"\n",
    "        f\"dual: {np.max(np.mean(duals_log, axis=0).mean())}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d757027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(PBM_S_loss_log_plotting)]\n",
    "constraints += [np.array(PBM_S_c_log_plotting).T]\n",
    "losses_std += [np.array(PBM_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(PBM_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7d72b",
   "metadata": {},
   "source": [
    "#### 4. Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# get the dataset\n",
    "dataset = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=120, drop_last=True\n",
    ")\n",
    "# create a dataloader from the sampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "from humancompatible.train.optim.PBM import PBM\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "hsize1 = 128\n",
    "hsize2 = 64\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ")\n",
    "\n",
    "# 5*6 constraint - per subgroup inequality + 2 per inequality\n",
    "number_of_constraints = 30\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model_con.parameters(), lr=0.001)\n",
    "\n",
    "# define the constraint violation band\n",
    "threshold = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "adam_S_loss_log_plotting = []  # mean\n",
    "adam_S_c_log_plotting = []  # mean\n",
    "adam_S_loss_std_log_plotting = []  # std\n",
    "adam_S_c_std_log_plotting = []  # std\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader:\n",
    "\n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij - threshold\n",
    "                    \n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().numpy() + threshold)\n",
    "\n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        # compute the augemented objective loss\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().numpy())\n",
    "\n",
    "\n",
    "    adam_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    adam_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    adam_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "    adam_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss: {np.mean(loss_log)}, \"\n",
    "        f\"constraints: {np.max(np.abs(np.mean(c_log, axis=0)))}, \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecca513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "\n",
    "# append\n",
    "losses += [np.array(adam_S_loss_log_plotting)]\n",
    "constraints += [np.array(adam_S_c_log_plotting).T]\n",
    "losses_std += [np.array(adam_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(adam_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e00b0f",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd225107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "thresholds = [threshold]\n",
    "\n",
    "plot_losses_and_constraints_single_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    titles=[\n",
    "        \"SSW\",\n",
    "        \"SSL-ALM-Adam\",\n",
    "        \"PBM\",\n",
    "        \"Unconstrained Adam\"\n",
    "    ],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c469f",
   "metadata": {},
   "source": [
    "# Stochastic Constraints: Dutch Dataset - Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff953b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -e ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define the torch seed here\n",
    "seed_n = 1\n",
    "n_epochs = 20\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# log path file\n",
    "log_path = \"./data/logs/log_benchmark_stochastic_dutch.npz\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# define network size\n",
    "hsize1 = 128\n",
    "hsize2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# resave the path\n",
    "# np.savez(\n",
    "#     log_path,\n",
    "#     losses=[],\n",
    "#     constraints=[],\n",
    "#     losses_std=[],\n",
    "#     constraints_std=[],\n",
    "#     losses_t=[],\n",
    "#     constraints_t=[],\n",
    "#     losses_std_t=[],\n",
    "#     constraints_std_t=[],\n",
    "#     times=[]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.fairness.utils.dataset_loader import get_data_dutch\n",
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "import torch\n",
    "\n",
    "# get the data\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test, group_names_dict = get_data_dutch(test_size=0.2, seed_n = 42, drop_small_groups=True, print_stats=True)\n",
    "\n",
    "# TODO: remove\n",
    "# X_train = X_train[:10_000]\n",
    "# y_train = y_train[:10_000]\n",
    "# groups_train = groups_train[:10_000]\n",
    "\n",
    "# X_test = X_test[:10_000]\n",
    "# y_test = y_test[:10_000]\n",
    "# groups_test = groups_test[:10_000]\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(y_train, dtype=torch.float32).reshape((-1, 1))\n",
    "sens_train = torch.tensor(groups_train)\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, labels_train)\n",
    "\n",
    "# make into a pytorch dataset, remove the sensitive attribute\n",
    "features_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "labels_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32).reshape((-1, 1))\n",
    "sens_test = torch.tensor(groups_test)\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# get the dataset\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "dataset_test = torch.utils.data.TensorDataset(features_test, sens_test, labels_test)\n",
    "\n",
    "# create a balanced sampling - needed for an unbiased gradient\n",
    "sampler_train = BalancedBatchSampler(\n",
    "    group_onehot=sens_train, batch_size=72, drop_last=True\n",
    ")\n",
    "\n",
    "sampler_test = BalancedBatchSampler(\n",
    "    group_onehot=sens_test, batch_size=252*4, drop_last=True\n",
    ")\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_sampler=sampler_train, num_workers=8) #balanced\n",
    "# dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=256, shuffle=True, num_workers=8) # unbalanced\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_sampler=sampler_test, num_workers=8) # balanced\n",
    "# dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=256, shuffle=True, num_workers=8) # unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic.linear_fractional import PositiveRate\n",
    "\n",
    "def positive_rate(out_batch, batch_sens, prob_f=torch.nn.functional.sigmoid):\n",
    "    \"\"\"\n",
    "    Calculates the positive rate vector based on the given outputs of the model for the given groups. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # compute the probabilities - using sigmoid (since that is used )\n",
    "    if prob_f is None: \n",
    "        preds = out_batch\n",
    "    else: \n",
    "        preds = prob_f( out_batch )\n",
    "\n",
    "    pr = PositiveRate()\n",
    "    probs_per_group = pr(preds, batch_sens)  # P(y=1|Sk = 1)\n",
    "\n",
    "    return probs_per_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2e178",
   "metadata": {},
   "source": [
    "#### 1. Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ce402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define criterion here\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ").to(device)\n",
    "\n",
    "# get the number of subgroups\n",
    "number_of_subgroups = len(group_names_dict.keys())\n",
    "\n",
    "# per each pair of subgroup - 1x inequality \n",
    "number_of_constraints = number_of_subgroups * (number_of_subgroups - 1)\n",
    "\n",
    "print(\"Number of constraints in total: \", number_of_constraints)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model_con.parameters(), lr=0.001)\n",
    "\n",
    "# define the constraint violation band\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d525673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader_test):\n",
    "\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # go though all data\n",
    "        for batch_input, batch_sens, batch_label in dataloader_test:\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            batch_sens = batch_sens.to(device)\n",
    "            batch_label = batch_label.to(device)\n",
    "            \n",
    "            # calculate constraints and constraint grads\n",
    "            out = model(batch_input)\n",
    "\n",
    "            # compute per group positive rate\n",
    "            pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "            # prepare counter + array of constr for this batch\n",
    "            current_constr = 0\n",
    "            c_log.append([])\n",
    "\n",
    "            # compute the equal opportunity constraint\n",
    "            for i in range(0, len(pos_rate_pergroup)):\n",
    "                for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                    # calculate the constraint only for different subgroups\n",
    "                    if i != j:\n",
    "\n",
    "                        # the constraint with the slack variables\n",
    "                        constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                        constr_ij = constr_ij - threshold\n",
    "                        \n",
    "                        # save the value of the constraint\n",
    "                        c_log[-1].append(constr_ij.detach().cpu().numpy() + threshold)\n",
    "\n",
    "                        # iterate the constraint counter\n",
    "                        current_constr += 1\n",
    "\n",
    "            # compute the augemented objective loss\n",
    "            loss = criterion(out, batch_label)\n",
    "\n",
    "            # save the logs\n",
    "            loss_log.append(loss.detach().cpu().numpy())\n",
    "    \n",
    "    return loss_log, c_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f986993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "adam_S_loss_log_plotting = []  # mean\n",
    "adam_S_c_log_plotting = []  # mean\n",
    "adam_S_loss_std_log_plotting = []  # std\n",
    "adam_S_c_std_log_plotting = []  # std\n",
    "\n",
    "test_S_loss_log_plotting = []  # mean\n",
    "test_S_c_log_plotting = []  # mean\n",
    "test_S_loss_std_log_plotting = []  # std\n",
    "test_S_c_std_log_plotting = []  # std\n",
    "\n",
    "losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "losses_train, c_train = test_model(model_con, dataloader_train)\n",
    "\n",
    "adam_S_loss_log_plotting.append(np.mean(losses_train))\n",
    "adam_S_c_log_plotting.append(np.mean(c_train, axis=0))\n",
    "adam_S_loss_std_log_plotting.append(np.std(losses_train, axis=0))\n",
    "adam_S_c_std_log_plotting.append(np.std(c_train, axis=0))\n",
    "\n",
    "test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "\n",
    "# print the initial test loss\n",
    "print(\"Losses: \", np.mean(losses_train), np.mean(losses_test))\n",
    "print(\"Constraints: \", np.mean(c_train), np.mean(c_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Calculate the start time\n",
    "starttime = time.time()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs-1):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader_train:\n",
    "\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_sens = batch_sens.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        \n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij - threshold\n",
    "                    \n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().cpu().numpy() + threshold)\n",
    "\n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        # compute the augemented objective loss\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    adam_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    adam_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    adam_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "    adam_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "\n",
    "    # compute the test\n",
    "    losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "\n",
    "    test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "    test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "    test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "    test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss ({np.mean(loss_log):.4f}/{np.mean(losses_test):.4f}):\"\n",
    "        f\"constraints (train/test): ({np.max(np.abs(np.mean(c_log, axis=0))):.4f}/{np.max(np.abs(np.mean(c_test, axis=0))):.4f}), \"\n",
    "    )\n",
    "\n",
    "\n",
    "# Calculate the end time and time taken\n",
    "endtime = time.time()\n",
    "seconds_elapsed = endtime - starttime\n",
    "\n",
    "# per epoch time\n",
    "seconds_elapsed = seconds_elapsed / (n_epochs-1)\n",
    "\n",
    "formatted_time = time.strftime('%H:%M:%S', time.gmtime(seconds_elapsed))\n",
    "print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "losses_t = list(np.load(log_path)[\"losses_t\"])\n",
    "constraints_t = list(np.load(log_path)[\"constraints_t\"])\n",
    "losses_std_t = list(np.load(log_path)[\"losses_std_t\"])\n",
    "constraints_std_t = list(np.load(log_path)[\"constraints_std_t\"])\n",
    "times = list(np.load(log_path)[\"times\"])\n",
    "\n",
    "# append time\n",
    "times += [seconds_elapsed]\n",
    "\n",
    "# append\n",
    "losses += [np.array(adam_S_loss_log_plotting)]\n",
    "constraints += [np.array(adam_S_c_log_plotting).T]\n",
    "losses_std += [np.array(adam_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(adam_S_c_std_log_plotting).T]\n",
    "\n",
    "losses_t += [np.array(test_S_loss_log_plotting)]\n",
    "constraints_t += [np.array(test_S_c_log_plotting).T]\n",
    "losses_std_t += [np.array(test_S_loss_std_log_plotting)]\n",
    "constraints_std_t += [np.array(test_S_c_std_log_plotting).T]\n",
    "\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    "    losses_t=losses_t,\n",
    "    constraints_t=constraints_t,\n",
    "    losses_std_t=losses_std_t,\n",
    "    constraints_std_t=constraints_std_t,\n",
    "    times=times\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c3f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses_and_constraints_stochastic(\n",
    "    train_losses_list,\n",
    "    train_losses_std_list,\n",
    "    train_constraints_list,\n",
    "    train_constraints_std_list,\n",
    "    constraint_thresholds,\n",
    "    test_losses_list=None,\n",
    "    test_losses_std_list=None,\n",
    "    test_constraints_list=None,\n",
    "    test_constraints_std_list=None,\n",
    "    titles=None,\n",
    "    eval_points=1,\n",
    "    std_multiplier=2,\n",
    "    log_constraints=False,\n",
    "    mode=\"train\",  # \"train\" or \"train_test\"\n",
    "    times=[], # second per epoch\n",
    "    plot_time_instead_epochs=False\n",
    "):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "        \"train\"       -> only training plots\n",
    "        \"train_test\"  -> training + test side by side\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Color palette (Tableau 10) ---\n",
    "    colors = [\n",
    "        \"#4E79A7\",\n",
    "        \"#F28E2B\",\n",
    "        \"#E15759\",\n",
    "        \"#76B7B2\",\n",
    "        \"#59A14F\",\n",
    "        \"#EDC948\",\n",
    "        \"#B07AA1\",\n",
    "        \"#FF9DA7\",\n",
    "        \"#9C755F\",\n",
    "        \"#BAB0AB\",\n",
    "    ]\n",
    "\n",
    "    marker_styles = [\"o\", \"s\", \"D\", \"^\", \"v\", \"<\", \">\", \"P\", \"X\", \"*\"]\n",
    "\n",
    "    num_algos = len(train_losses_list)\n",
    "    if titles is None:\n",
    "        titles = [f\"Algorithm {i + 1}\" for i in range(num_algos)]\n",
    "\n",
    "    constraint_thresholds = np.atleast_1d(constraint_thresholds)\n",
    "\n",
    "    # --- Layout ---\n",
    "    ncols = 1 if mode == \"train\" else 2\n",
    "    fig, axes = plt.subplots(2, ncols, figsize=(9 * ncols, 10), sharex=\"col\")\n",
    "\n",
    "    if ncols == 1:\n",
    "        axes = np.array([[axes[0]], [axes[1]]])\n",
    "\n",
    "    # ======================================================\n",
    "    # Helper plotting functions\n",
    "    # ======================================================\n",
    "\n",
    "    def plot_loss(ax, losses_list, losses_std_list, title_suffix):\n",
    "        for j, (loss, loss_std) in enumerate(zip(losses_list, losses_std_list)):\n",
    "            x = np.arange(len(loss))\n",
    "            color = colors[j % len(colors)]\n",
    "            upper = loss + std_multiplier * loss_std\n",
    "            lower = loss - std_multiplier * loss_std\n",
    "\n",
    "            minutes = round(times[j] // 60)\n",
    "            seconds = round(times[j] % 60)\n",
    "\n",
    "            if plot_time_instead_epochs:\n",
    "                x *= round(times[j])\n",
    "\n",
    "            ax.plot(x, loss, lw=2.2, color=color, label=titles[j] + f\"; TPE: {minutes}m:{seconds}s\")\n",
    "            ax.fill_between(x, lower, upper, color=color, alpha=0.15)\n",
    "\n",
    "            if eval_points is not None:\n",
    "                idx = (\n",
    "                    np.arange(0, len(loss), eval_points)\n",
    "                    if isinstance(eval_points, int)\n",
    "                    else np.array(eval_points)\n",
    "                )\n",
    "                idx = idx[idx < len(loss)]\n",
    "                ax.plot(\n",
    "                    x[idx],\n",
    "                    loss[idx],\n",
    "                    marker_styles[j % len(marker_styles)],\n",
    "                    color=color,\n",
    "                    markersize=6,\n",
    "                    alpha=0.8,\n",
    "                )\n",
    "\n",
    "        ax.set_title(f\"Loss ({title_suffix})\")\n",
    "        ax.set_ylabel(\"Mean Loss\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        ax.legend(fontsize=9)\n",
    "\n",
    "    def plot_constraints(ax, constraints_list, constraints_std_list, title_suffix):\n",
    "        for j, (constraints, constraints_std) in enumerate(\n",
    "            zip(constraints_list, constraints_std_list)\n",
    "        ):\n",
    "            color = colors[j % len(colors)]\n",
    "            constraints = np.asarray(constraints)\n",
    "            constraints_std = np.asarray(constraints_std)\n",
    "\n",
    "            x = np.arange(constraints.shape[1])\n",
    "\n",
    "            c_min = np.min(constraints - std_multiplier * constraints_std, axis=0)\n",
    "            c_max = np.max(constraints + std_multiplier * constraints_std, axis=0)\n",
    "\n",
    "            ax.fill_between(x, c_min, c_max, color=color, alpha=0.1)\n",
    "\n",
    "            for c_mean in constraints:\n",
    "\n",
    "                if plot_time_instead_epochs:\n",
    "                    x *= round(times[j])\n",
    "\n",
    "                ax.plot(x, c_mean, lw=1.8, color=color, alpha=0.3)\n",
    "\n",
    "                if eval_points is not None:\n",
    "                    idx = (\n",
    "                        np.arange(0, len(c_mean), eval_points)\n",
    "                        if isinstance(eval_points, int)\n",
    "                        else np.array(eval_points)\n",
    "                    )\n",
    "                    idx = idx[idx < len(c_mean)]\n",
    "                    ax.plot(\n",
    "                        x[idx],\n",
    "                        c_mean[idx],\n",
    "                        marker_styles[j % len(marker_styles)],\n",
    "                        color=color,\n",
    "                        markersize=5,\n",
    "                        alpha=0.3,\n",
    "                    )\n",
    "\n",
    "        for th in constraint_thresholds:\n",
    "            y = np.log(th) if log_constraints else th\n",
    "            ax.axhline(y, color=\"red\", linestyle=\"--\", lw=1.4, label=\"Threshold\")\n",
    "\n",
    "        ax.set_title(f\"Constraint ({title_suffix})\")\n",
    "        ax.set_ylabel(\"Log Constraint\" if log_constraints else \"Constraint\")\n",
    "\n",
    "        if plot_time_instead_epochs:\n",
    "            ax.set_xlabel(\"Time (m)\")\n",
    "        else: \n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "        ax.legend(fontsize=9)\n",
    "\n",
    "    # ======================================================\n",
    "    # TRAIN PLOTS\n",
    "    # ======================================================\n",
    "\n",
    "    plot_loss(axes[0, 0], train_losses_list, train_losses_std_list, \"Train\")\n",
    "    plot_constraints(\n",
    "        axes[1, 0],\n",
    "        train_constraints_list,\n",
    "        train_constraints_std_list,\n",
    "        \"Train\",\n",
    "    )\n",
    "\n",
    "    # ======================================================\n",
    "    # TEST PLOTS\n",
    "    # ======================================================\n",
    "\n",
    "    if mode == \"train_test\":\n",
    "        plot_loss(axes[0, 1], test_losses_list, test_losses_std_list, \"Test\")\n",
    "        plot_constraints(\n",
    "            axes[1, 1],\n",
    "            test_constraints_list,\n",
    "            test_constraints_std_list,\n",
    "            \"Test\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14453117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "losses_t = np.load(log_path)[\"losses_t\"]\n",
    "losses_std_t = np.load(log_path)[\"losses_std_t\"]\n",
    "constraints_t = np.load(log_path)[\"constraints_t\"]\n",
    "constraints_std_t = np.load(log_path)[\"constraints_std_t\"]\n",
    "times = np.load(log_path)[\"times\"]\n",
    "thresholds = [threshold]\n",
    "\n",
    "plot_losses_and_constraints_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    test_losses_list=losses_t,\n",
    "    test_losses_std_list=losses_std_t,\n",
    "    test_constraints_list=constraints_t,\n",
    "    test_constraints_std_list=constraints_std_t,\n",
    "    titles=[\n",
    "        \"Unconstrained Adam\"\n",
    "    ],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    "    mode='train_test', # change this to 'train', to ignore the test\n",
    "    times=times\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6743ab",
   "metadata": {},
   "source": [
    "#### 2. SSW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ecea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os\n",
    "from humancompatible.train.optim.ssw import SSG\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ").to(device)\n",
    "\n",
    "# get the number of subgroups\n",
    "number_of_subgroups = len(group_names_dict.keys())\n",
    "\n",
    "# per each pair of subgroup - 1x inequality \n",
    "number_of_constraints = number_of_subgroups * (number_of_subgroups - 1)\n",
    "\n",
    "print(\"Number of constraints in total: \", number_of_constraints)\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_sampler=sampler_train)\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_sampler=sampler_test)\n",
    "\n",
    "optimizer = SSG(params=model_con.parameters(), m=1, lr=0.01, dual_lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa275d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "ssw_S_loss_log_plotting = []  # mean\n",
    "ssw_S_c_log_plotting = []  # mean\n",
    "ssw_S_loss_std_log_plotting = []  # std\n",
    "ssw_S_c_std_log_plotting = []  # std\n",
    "\n",
    "test_S_loss_log_plotting = []  # mean\n",
    "test_S_c_log_plotting = []  # mean\n",
    "test_S_loss_std_log_plotting = []  # std\n",
    "test_S_c_std_log_plotting = []  # std\n",
    "\n",
    "losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "losses_train, c_train = test_model(model_con, dataloader_train)\n",
    "\n",
    "ssw_S_loss_log_plotting.append(np.mean(losses_train))\n",
    "ssw_S_c_log_plotting.append(np.mean(c_train, axis=0))\n",
    "ssw_S_loss_std_log_plotting.append(np.std(losses_train, axis=0))\n",
    "ssw_S_c_std_log_plotting.append(np.std(c_train, axis=0))\n",
    "\n",
    "test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "\n",
    "# print the initial test loss\n",
    "print(\"Losses: \", np.mean(losses_train), np.mean(losses_test))\n",
    "print(\"Constraints: \", np.mean(c_train), np.mean(c_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9812a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the start time\n",
    "starttime = time.time()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs-1):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader_train:\n",
    "\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_sens = batch_sens.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        \n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # prepare the max of the constraints\n",
    "        max_norm_viol = torch.zeros(1, device=device)\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij - threshold\n",
    "\n",
    "                    # save the max\n",
    "                    max_norm_viol = torch.max(max_norm_viol, constr_ij)\n",
    "\n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().cpu().numpy() + threshold)\n",
    "\n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        # calculate the Jacobian of the max-violating norm constraint\n",
    "        max_norm_viol.backward(retain_graph=True)\n",
    "\n",
    "        # save the gradient of the constraint\n",
    "        optimizer.dual_step(0)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step(max_norm_viol)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    ssw_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    ssw_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    ssw_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "    ssw_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "\n",
    "    # compute the test\n",
    "    losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "\n",
    "    test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "    test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "    test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "    test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss ({np.mean(loss_log):.4f}/{np.mean(losses_test):.4f}):\"\n",
    "        f\"constraints (train/test): ({np.max(np.abs(np.mean(c_log, axis=0))):.4f}/{np.max(np.abs(np.mean(c_test, axis=0))):.4f}), \"\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "# Calculate the end time and time taken\n",
    "endtime = time.time()\n",
    "seconds_elapsed = endtime - starttime\n",
    "\n",
    "# per epoch time\n",
    "seconds_elapsed = seconds_elapsed / (n_epochs-1)\n",
    "\n",
    "formatted_time = time.strftime('%H:%M:%S', time.gmtime(seconds_elapsed))\n",
    "print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "losses_t = list(np.load(log_path)[\"losses_t\"])\n",
    "constraints_t = list(np.load(log_path)[\"constraints_t\"])\n",
    "losses_std_t = list(np.load(log_path)[\"losses_std_t\"])\n",
    "constraints_std_t = list(np.load(log_path)[\"constraints_std_t\"])\n",
    "times = list(np.load(log_path)[\"times\"])\n",
    "\n",
    "# append time\n",
    "times += [seconds_elapsed]\n",
    "\n",
    "# append\n",
    "losses += [np.array(ssw_S_loss_log_plotting)]\n",
    "constraints += [np.array(ssw_S_c_log_plotting).T]\n",
    "losses_std += [np.array(ssw_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(ssw_S_c_std_log_plotting).T]\n",
    "\n",
    "losses_t += [np.array(test_S_loss_log_plotting)]\n",
    "constraints_t += [np.array(test_S_c_log_plotting).T]\n",
    "losses_std_t += [np.array(test_S_loss_std_log_plotting)]\n",
    "constraints_std_t += [np.array(test_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    "    losses_t=losses_t,\n",
    "    constraints_t=constraints_t,\n",
    "    losses_std_t=losses_std_t,\n",
    "    constraints_std_t=constraints_std_t,\n",
    "    times=times\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f775cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "losses_t = np.load(log_path)[\"losses_t\"]\n",
    "losses_std_t = np.load(log_path)[\"losses_std_t\"]\n",
    "constraints_t = np.load(log_path)[\"constraints_t\"]\n",
    "constraints_std_t = np.load(log_path)[\"constraints_std_t\"]\n",
    "times = np.load(log_path)[\"times\"]\n",
    "thresholds = [threshold]\n",
    "\n",
    "plot_losses_and_constraints_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    test_losses_list=losses_t,\n",
    "    test_losses_std_list=losses_std_t,\n",
    "    test_constraints_list=constraints_t,\n",
    "    test_constraints_std_list=constraints_std_t,\n",
    "    titles=[\n",
    "        \"Unconstrained Adam\",\n",
    "        \"SSW\",\n",
    "        \"SSLALM-Adam\",\n",
    "        \"PBM\"\n",
    "    ],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    "    mode='train_test', # change this to 'train', to ignore the test\n",
    "    times=times\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b872f2",
   "metadata": {},
   "source": [
    "#### 3. SSLALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe73d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ").to(device)\n",
    "\n",
    "# get the number of subgroups\n",
    "number_of_subgroups = len(group_names_dict.keys())\n",
    "\n",
    "# per each pair of subgroup - 1x inequality \n",
    "number_of_constraints = number_of_subgroups * (number_of_subgroups - 1)\n",
    "\n",
    "print(\"Number of constraints in total: \", number_of_constraints)\n",
    "\n",
    "optimizer = SSLALM_Adam(\n",
    "    params=model_con.parameters(),\n",
    "    m=number_of_constraints,  # number of constraints - one in our case\n",
    "    lr=0.01,  # primal variable lr\n",
    "    dual_lr=0.05,  # lr of a dual ALM variable\n",
    "    dual_bound=5,\n",
    "    rho=1,  # rho penalty in ALM parameter\n",
    "    mu=2,  # smoothing parameter\n",
    "    device=device,\n",
    "    joint_backward_pass=True\n",
    ")\n",
    "\n",
    "\n",
    "# add slack variables - to create the equality from the inequalities    \n",
    "slack_vars = torch.zeros(number_of_constraints, requires_grad=True, device=device)\n",
    "optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})\n",
    "\n",
    "# define the constraint violation band\n",
    "threshold = 0.1\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_sampler=sampler_train)\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_sampler=sampler_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "sslalm_S_loss_log_plotting = []  # mean\n",
    "sslalm_S_c_log_plotting = []  # mean\n",
    "sslalm_S_loss_std_log_plotting = []  # std\n",
    "sslalm_S_c_std_log_plotting = []  # std\n",
    "\n",
    "test_S_loss_log_plotting = []  # mean\n",
    "test_S_c_log_plotting = []  # mean\n",
    "test_S_loss_std_log_plotting = []  # std\n",
    "test_S_c_std_log_plotting = []  # std\n",
    "\n",
    "losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "losses_train, c_train = test_model(model_con, dataloader_train)\n",
    "\n",
    "sslalm_S_loss_log_plotting.append(np.mean(losses_train))\n",
    "sslalm_S_c_log_plotting.append(np.mean(c_train, axis=0))\n",
    "sslalm_S_loss_std_log_plotting.append(np.std(losses_train, axis=0))\n",
    "sslalm_S_c_std_log_plotting.append(np.std(c_train, axis=0))\n",
    "\n",
    "test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "\n",
    "# print the initial test loss\n",
    "print(\"Losses: \", np.mean(losses_train), np.mean(losses_test))\n",
    "print(\"Constraints: \", np.mean(c_train), np.mean(c_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d888fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Calculate the start time\n",
    "starttime = time.time()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs-1):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader_train:\n",
    "\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_sens = batch_sens.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        \n",
    "        # calculate constraints and constraint gradslagrange_fn\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        constraints = torch.zeros(len(slack_vars))\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij + slack_vars[current_constr] - threshold\n",
    "\n",
    "                    # perform the dual step on the constraint\n",
    "                    # constr_ij.backward(retain_graph=True)\n",
    "\n",
    "                    # perform the dual step variable + save the dual grad for later\n",
    "                    optimizer.dual_step(current_constr, c_val=constr_ij)\n",
    "                    # optimizer.zero_grad()\n",
    "                    constraints[current_constr] = constr_ij\n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().cpu().numpy() - slack_vars[current_constr].detach().cpu().numpy() + threshold)\n",
    "                    \n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        # calculate primal loss and grad\n",
    "        loss = 0.0\n",
    "        for i in range(0, number_of_constraints): # this is purely for pytorch not to complain about slack variables not being in the loss\n",
    "            loss += 0*slack_vars[i]\n",
    "\n",
    "        # loss += criterion(out, batch_label)\n",
    "        # loss.backward()\n",
    "        loss = criterion(out, batch_label)\n",
    "        lagr_fn = loss + optimizer._dual_vars @ constraints + torch.linalg.norm(constraints, ord=2)\n",
    "        lagr_fn.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().cpu().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach().cpu())\n",
    "\n",
    "        # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "        with torch.no_grad():\n",
    "            for s in slack_vars:\n",
    "                if s < 0:\n",
    "                    s.zero_()\n",
    "\n",
    "    sslalm_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    sslalm_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    sslalm_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "    sslalm_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "\n",
    "    # compute the test\n",
    "    losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "\n",
    "    test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "    test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "    test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "    test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss ({np.mean(loss_log):.4f}/{np.mean(losses_test):.4f}):\"\n",
    "        f\"constraints (train/test): ({np.max(np.abs(np.mean(c_log, axis=0))):.4f}/{np.max(np.abs(np.mean(c_test, axis=0))):.4f}), \"\n",
    "        f\"dual: {np.max(np.mean(duals_log, axis=0).mean())}\"\n",
    "    )\n",
    "\n",
    "# Calculate the end time and time taken\n",
    "endtime = time.time()\n",
    "seconds_elapsed = endtime - starttime\n",
    "\n",
    "# per epoch time\n",
    "seconds_elapsed = seconds_elapsed / (n_epochs-1)\n",
    "\n",
    "formatted_time = time.strftime('%H:%M:%S', time.gmtime(seconds_elapsed))\n",
    "print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "losses_t = list(np.load(log_path)[\"losses_t\"])\n",
    "constraints_t = list(np.load(log_path)[\"constraints_t\"])\n",
    "losses_std_t = list(np.load(log_path)[\"losses_std_t\"])\n",
    "constraints_std_t = list(np.load(log_path)[\"constraints_std_t\"])\n",
    "times = list(np.load(log_path)[\"times\"])\n",
    "\n",
    "# append time\n",
    "times += [seconds_elapsed]\n",
    "\n",
    "# append\n",
    "losses += [np.array(sslalm_S_loss_log_plotting)]\n",
    "constraints += [np.array(sslalm_S_c_log_plotting).T]\n",
    "losses_std += [np.array(sslalm_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(sslalm_S_c_std_log_plotting).T]\n",
    "\n",
    "losses_t += [np.array(test_S_loss_log_plotting)]\n",
    "constraints_t += [np.array(test_S_c_log_plotting).T]\n",
    "losses_std_t += [np.array(test_S_loss_std_log_plotting)]\n",
    "constraints_std_t += [np.array(test_S_c_std_log_plotting).T]\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    "    losses_t=losses_t,\n",
    "    constraints_t=constraints_t,\n",
    "    losses_std_t=losses_std_t,\n",
    "    constraints_std_t=constraints_std_t,\n",
    "    times=times\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "losses_t = np.load(log_path)[\"losses_t\"]\n",
    "losses_std_t = np.load(log_path)[\"losses_std_t\"]\n",
    "constraints_t = np.load(log_path)[\"constraints_t\"]\n",
    "constraints_std_t = np.load(log_path)[\"constraints_std_t\"]\n",
    "times = np.load(log_path)[\"times\"]\n",
    "thresholds = [threshold]\n",
    "\n",
    "plot_losses_and_constraints_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    test_losses_list=losses_t,\n",
    "    test_losses_std_list=losses_std_t,\n",
    "    test_constraints_list=constraints_t,\n",
    "    test_constraints_std_list=constraints_std_t,\n",
    "    titles=[\n",
    "        \"Unconstrained Adam\",\n",
    "        \"SSW\",\n",
    "        \"SSLALM-Adam\",\n",
    "        \"PBM\"\n",
    "    ],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    "    mode='train_test', # change this to 'train', to ignore the test\n",
    "    times=times\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688427d",
   "metadata": {},
   "source": [
    "### 4. PBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from humancompatible.train.optim.PBM import PBM\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "model_con = Sequential(\n",
    "    torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize1, hsize2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hsize2, 1),\n",
    ").to(device)\n",
    "\n",
    "# get the number of subgroups\n",
    "number_of_subgroups = len(group_names_dict.keys())\n",
    "\n",
    "# per each pair of subgroup - 1x inequality \n",
    "number_of_constraints = number_of_subgroups * (number_of_subgroups - 1)\n",
    "\n",
    "print(\"Number of constraints in total: \", number_of_constraints)\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_sampler=sampler_train, num_workers=8)\n",
    "# dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "# create a dataloader from the sampler\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_sampler=sampler_test, num_workers=8)\n",
    "# dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "optimizer = PBM(params=model_con.parameters(), m=number_of_constraints, lr=0.001, dual_beta=0.9, mu=0.1, \n",
    "                epoch_len=len(dataloader_train), init_dual=0.01, penalty_update_m='DIMINISH', p_lb=0.1,\n",
    "                barrier=\"quadratic_logarithmic\", device=device)\n",
    "\n",
    "# define the constraint violation band\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alloc arrays for plotting\n",
    "pbm_S_loss_log_plotting = []  # mean\n",
    "pbm_S_c_log_plotting = []  # mean\n",
    "pbm_S_loss_std_log_plotting = []  # std\n",
    "pbm_S_c_std_log_plotting = []  # std\n",
    "\n",
    "test_S_loss_log_plotting = []  # mean\n",
    "test_S_c_log_plotting = []  # mean\n",
    "test_S_loss_std_log_plotting = []  # std\n",
    "test_S_c_std_log_plotting = []  # std\n",
    "\n",
    "losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "losses_train, c_train = test_model(model_con, dataloader_train)\n",
    "\n",
    "pbm_S_loss_log_plotting.append(np.mean(losses_train))\n",
    "pbm_S_c_log_plotting.append(np.mean(c_train, axis=0))\n",
    "pbm_S_loss_std_log_plotting.append(np.std(losses_train, axis=0))\n",
    "pbm_S_c_std_log_plotting.append(np.std(c_train, axis=0))\n",
    "\n",
    "test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "\n",
    "# print the initial test loss\n",
    "print(\"Losses: \", np.mean(losses_train), np.mean(losses_test))\n",
    "print(\"Constraints: \", np.mean(c_train), np.mean(c_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee92eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Calculate the start time\n",
    "starttime = time.time()\n",
    "\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs-1):\n",
    "    # alloc the logging arrays for the batch\n",
    "    loss_log = []\n",
    "    c_log = []\n",
    "    duals_log = []\n",
    "\n",
    "    # go though all data\n",
    "    for batch_input, batch_sens, batch_label in dataloader_train:\n",
    "\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_sens = batch_sens.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "        \n",
    "        # calculate constraints and constraint grads\n",
    "        out = model_con(batch_input)\n",
    "\n",
    "        # compute per group positive rate\n",
    "        pos_rate_pergroup = positive_rate(out, batch_sens, prob_f=torch.nn.functional.sigmoid)\n",
    "\n",
    "        # prepare counter + array of constr for this batch\n",
    "        current_constr = 0\n",
    "        c_log.append([])\n",
    "\n",
    "        # compute the equal opportunity constraint\n",
    "        for i in range(0, len(pos_rate_pergroup)):\n",
    "            for j in range(0, len(pos_rate_pergroup)):\n",
    "\n",
    "                # calculate the constraint only for different subgroups\n",
    "                if i != j:\n",
    "\n",
    "                    # the constraint with the slack variables\n",
    "                    constr_ij = pos_rate_pergroup[i] - pos_rate_pergroup[j] \n",
    "                    constr_ij = constr_ij - threshold\n",
    "\n",
    "                    # perform the dual step variable + save the dual grad for later\n",
    "                    optimizer.dual_step(current_constr, c_val=constr_ij)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # save the value of the constraint\n",
    "                    c_log[-1].append(constr_ij.detach().cpu().numpy() + threshold)\n",
    "                    \n",
    "                    # iterate the constraint counter\n",
    "                    current_constr += 1\n",
    "\n",
    "        loss = criterion(out, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # save the logs\n",
    "        loss_log.append(loss.detach().cpu().numpy())\n",
    "        duals_log.append(optimizer._dual_vars.detach().cpu())\n",
    "\n",
    "    pbm_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "    pbm_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "    pbm_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "    pbm_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "\n",
    "    # compute the test\n",
    "    losses_test, c_test = test_model(model_con, dataloader_test)\n",
    "\n",
    "    test_S_loss_log_plotting.append(np.mean(losses_test))\n",
    "    test_S_c_log_plotting.append(np.mean(c_test, axis=0))\n",
    "    test_S_loss_std_log_plotting.append(np.std(losses_test, axis=0))\n",
    "    test_S_c_std_log_plotting.append(np.std(c_test, axis=0))\n",
    "    \n",
    "    # compute the largest violations\n",
    "    argmax = np.argsort(np.abs(np.mean(c_log, axis=0)))[::-1]\n",
    "    n_max = 5\n",
    "    print(f'argmax constraints ({n_max}): {argmax[:n_max]}')\n",
    "    print(f'max constraints ({n_max}): {np.abs(np.mean(c_log, axis=0))[argmax[:n_max]]}')\n",
    "    print(f'duals ({n_max}): {np.mean(duals_log, axis=0)[argmax[:n_max]]}')\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, \"\n",
    "        f\"loss ({np.mean(loss_log):.4f}/{np.mean(losses_test):.4f}):\"\n",
    "        f\"constraints (train/test): ({np.max(np.abs(np.mean(c_log, axis=0))):.4f}/{np.max(np.abs(np.mean(c_test, axis=0))):.4f}), \"\n",
    "        f\"dual: {np.max(np.mean(duals_log, axis=0))}\"\n",
    "    )\n",
    "\n",
    "    print(optimizer.p)\n",
    "\n",
    "\n",
    "# Calculate the end time and time taken\n",
    "endtime = time.time()\n",
    "seconds_elapsed = endtime - starttime\n",
    "\n",
    "# per epoch time\n",
    "seconds_elapsed = seconds_elapsed / (n_epochs-1)\n",
    "\n",
    "formatted_time = time.strftime('%H:%M:%S', time.gmtime(seconds_elapsed))\n",
    "print(formatted_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prior and append\n",
    "losses = list(np.load(log_path)[\"losses\"])\n",
    "constraints = list(np.load(log_path)[\"constraints\"])\n",
    "losses_std = list(np.load(log_path)[\"losses_std\"])\n",
    "constraints_std = list(np.load(log_path)[\"constraints_std\"])\n",
    "losses_t = list(np.load(log_path)[\"losses_t\"])\n",
    "constraints_t = list(np.load(log_path)[\"constraints_t\"])\n",
    "losses_std_t = list(np.load(log_path)[\"losses_std_t\"])\n",
    "constraints_std_t = list(np.load(log_path)[\"constraints_std_t\"])\n",
    "times = list(np.load(log_path)[\"times\"])\n",
    "\n",
    "# append time\n",
    "times += [seconds_elapsed]\n",
    "\n",
    "# append\n",
    "losses += [np.array(pbm_S_loss_log_plotting)]\n",
    "constraints += [np.array(pbm_S_c_log_plotting).T]\n",
    "losses_std += [np.array(pbm_S_loss_std_log_plotting)]\n",
    "constraints_std += [np.array(pbm_S_c_std_log_plotting).T]\n",
    "\n",
    "losses_t += [np.array(test_S_loss_log_plotting)]\n",
    "constraints_t += [np.array(test_S_c_log_plotting).T]\n",
    "losses_std_t += [np.array(test_S_loss_std_log_plotting)]\n",
    "constraints_std_t += [np.array(test_S_c_std_log_plotting).T]\n",
    "\n",
    "\n",
    "# save the computed data\n",
    "np.savez(\n",
    "    log_path,\n",
    "    losses=losses,\n",
    "    constraints=constraints,\n",
    "    losses_std=losses_std,\n",
    "    constraints_std=constraints_std,\n",
    "    losses_t=losses_t,\n",
    "    constraints_t=constraints_t,\n",
    "    losses_std_t=losses_std_t,\n",
    "    constraints_std_t=constraints_std_t,\n",
    "    times=times\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything into a single graph\n",
    "losses = np.load(log_path)[\"losses\"]\n",
    "losses_std = np.load(log_path)[\"losses_std\"]\n",
    "constraints = np.load(log_path)[\"constraints\"]\n",
    "constraints_std = np.load(log_path)[\"constraints_std\"]\n",
    "losses_t = np.load(log_path)[\"losses_t\"]\n",
    "losses_std_t = np.load(log_path)[\"losses_std_t\"]\n",
    "constraints_t = np.load(log_path)[\"constraints_t\"]\n",
    "constraints_std_t = np.load(log_path)[\"constraints_std_t\"]\n",
    "times = np.load(log_path)[\"times\"]\n",
    "thresholds = [threshold]\n",
    "\n",
    "plot_losses_and_constraints_stochastic(\n",
    "    losses,\n",
    "    losses_std,\n",
    "    constraints,\n",
    "    constraints_std,\n",
    "    thresholds,\n",
    "    test_losses_list=losses_t,\n",
    "    test_losses_std_list=losses_std_t,\n",
    "    test_constraints_list=constraints_t,\n",
    "    test_constraints_std_list=constraints_std_t,\n",
    "    titles=[\n",
    "        \"Unconstrained Adam\",\n",
    "        \"SSW\",\n",
    "        \"SSLALM-Adam\",\n",
    "        \"PBM\"\n",
    "    ],\n",
    "    log_constraints=False,\n",
    "    std_multiplier=1,\n",
    "    mode='train_test', # change this to 'train', to ignore the test\n",
    "    times=times,\n",
    "    plot_time_instead_epochs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f75c2b",
   "metadata": {},
   "source": [
    "# Importance Sampling\n",
    "\n",
    "We now demonstrate the need for having samples of each subgroup in the training batch. This is needed for the model to improve the constraint loss. Without a sample of the subgroup in the batch, the training has ill-behaviour as can be seen on the next two cases. We choose our sensitive groups to be SEX (M/F) + Marriage status (Married, Divorced, Widowed, Separated, Never married). That is, $2\\cdot5$ subgroups in total, although each subgroup has a different ratio in the population, making it unsuitable for a simple i.i.d sampling. To showcase the ill-behaviour, we test one algorithm with the following changes  \n",
    "\n",
    "1. Train a model using the SSL-ALM with balanced sampling over groups (each subgroups has equal number of samples in each batch),\n",
    "2. Train a model using the SSL-ALM without a balanced sampling over groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# define the torch seed here\n",
    "seed_n = 1\n",
    "n_epochs = 20\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# log path file\n",
    "log_path = \"./data/logs/log_benchmark_stochastic_importance_sampling.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb243b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome, generate_categories\n",
    "from itertools import product\n",
    "\n",
    "# load folktables data\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "acs_data = data_source.get_data(states=[\"VA\"], download=True)\n",
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(\n",
    "    features=ACSIncome.features, definition_df=definition_df\n",
    ")\n",
    "df_feat, df_labels, _ = ACSIncome.df_to_pandas(\n",
    "    acs_data, categories=categories, dummies=True\n",
    ")\n",
    "\n",
    "# split the data on groups, labels and features - the features should not have the sensitive feature\n",
    "sens_cols = [\n",
    "    \"SEX_Female\",\n",
    "    \"SEX_Male\",\n",
    "    \"MAR_Divorced\",\n",
    "    \"MAR_Married\",\n",
    "    \"MAR_Separated\",\n",
    "    \"MAR_Widowed\",\n",
    "    \"MAR_Never married or under 15 years old\",\n",
    "]\n",
    "features = df_feat.drop(columns=sens_cols).to_numpy(dtype=\"float\")\n",
    "groups = df_feat[sens_cols].to_numpy(dtype=\"float\")\n",
    "labels = df_labels.to_numpy(dtype=\"float\")\n",
    "\n",
    "# Split columns into sex and marital\n",
    "sex_cols = [\"SEX_Female\", \"SEX_Male\"]\n",
    "mar_cols = [\n",
    "    \"MAR_Divorced\",\n",
    "    \"MAR_Married\",\n",
    "    \"MAR_Separated\",\n",
    "    \"MAR_Widowed\",\n",
    "    \"MAR_Never married or under 15 years old\",\n",
    "]\n",
    "\n",
    "# Convert each row to sex index and marital index\n",
    "sex_idx = df_feat[sex_cols].values.argmax(axis=1)\n",
    "mar_idx = df_feat[mar_cols].values.argmax(axis=1)\n",
    "\n",
    "# Number of unique combinations\n",
    "num_groups = len(sex_cols) * len(mar_cols)\n",
    "\n",
    "# Map each combination to a unique index\n",
    "group_indices = sex_idx * len(mar_cols) + mar_idx  # shape: (num_samples,)\n",
    "\n",
    "# One-hot encode the combinations\n",
    "groups_onehot = np.eye(num_groups)[group_indices]\n",
    "\n",
    "# Create dictionary mapping index to combination\n",
    "group_dict = {}\n",
    "for i, (s, m) in enumerate(product(sex_cols, mar_cols)):\n",
    "    group_dict[i] = f\"{s} + {m}\"\n",
    "\n",
    "# set the same seed for fair comparisons\n",
    "torch.manual_seed(seed_n)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n",
    "    features, labels, groups_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the statistics\n",
    "for idx in group_dict:\n",
    "    print(f\"{group_dict[idx]}, : {(groups_onehot[:, idx] == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36f60a",
   "metadata": {},
   "source": [
    "Now we create 3 dataloaders:\n",
    "\n",
    "- train unbalanced dataloader - unbalanced in the sense that each batch is not guaranteed to have the same number of samples of sensitive subgroups - leading to a biased gradient of the constraint\n",
    "- train balanaced dataloader\n",
    "- test dataloader - to see how the two above compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from humancompatible.train.fairness.utils import BalancedBatchSampler\n",
    "\n",
    "# define the batch sizes testing array\n",
    "batch_sizes = [10, 20, 40, 80, 160]\n",
    "\n",
    "# samplers array\n",
    "unbalanced_samplers = []\n",
    "balanced_samplers = []\n",
    "\n",
    "# tensor the train data\n",
    "features_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "sens_train = torch.tensor(groups_train)\n",
    "\n",
    "# tensor the test data\n",
    "features_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "labels_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "sens_test = torch.tensor(groups_test)\n",
    "\n",
    "# create a train dataset\n",
    "dataset_train = torch.utils.data.TensorDataset(features_train, sens_train, labels_train)\n",
    "\n",
    "# create test dataset\n",
    "dataset_test = torch.utils.data.TensorDataset(features_test, sens_test, labels_test)\n",
    "\n",
    "# create samplers for each batchsize\n",
    "for batch_size in batch_sizes:\n",
    "    # create the unbalanced dataloader\n",
    "    unbalanced_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    unbalanced_samplers += [unbalanced_dataloader]\n",
    "\n",
    "    # create the balanced dataloader\n",
    "    sampler = BalancedBatchSampler(\n",
    "        group_onehot=sens_train, batch_size=batch_size, drop_last=True\n",
    "    )\n",
    "    balanced_sampler = torch.utils.data.DataLoader(dataset_train, batch_sampler=sampler)\n",
    "    balanced_samplers += [balanced_sampler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b33f5",
   "metadata": {},
   "source": [
    "Define the fairness constraint to be the same as in the stochastic benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ecda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.statistic import Accuracy\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "# define the fairness criterion\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "statistic = PositiveRate()\n",
    "fair_criterion = NormLoss(statistic=statistic, p=torch.inf)\n",
    "fair_crit_bound = 0.15  # define the bound on the criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91fafb",
   "metadata": {},
   "source": [
    "Define the test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_fairness(model_to_test, fair_criterion, loss_criterion):\n",
    "    \"\"\"Test the model fairness on the test dataset\"\"\"\n",
    "\n",
    "    fair_constr = []\n",
    "    fair_constr_subgroups = [[] for _ in range(10)]\n",
    "    fair_constr_subgroups_ret = [() for _ in range(10)]\n",
    "    losses = []\n",
    "\n",
    "    test_sampler = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1024, shuffle=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # go though all test data\n",
    "        for batch_input, batch_sens, batch_labels in test_sampler:\n",
    "            # compute the fairness loss\n",
    "            out = model_to_test(batch_input)\n",
    "            fair_loss = fair_criterion(out, batch_sens)  # sigmoid is implicit here\n",
    "\n",
    "            # compute the fairness per group\n",
    "            preds = torch.nn.functional.sigmoid(out)\n",
    "            pr = PositiveRate()\n",
    "            probs_per_group = pr(preds, batch_sens)  # P(y=1|Sk = 1)\n",
    "            overall = PositiveRate().overall_statistic(preds)  # take the P(y=1)\n",
    "\n",
    "            # compute the posrate for the group\n",
    "            for idx, prob_per_group in enumerate(probs_per_group):\n",
    "                posrate_group = torch.abs(prob_per_group / overall - 1.0)\n",
    "                fair_constr_subgroups[idx] += [posrate_group.detach().item()]\n",
    "\n",
    "            # save the data\n",
    "            fair_constr += [fair_loss.detach().item()]\n",
    "\n",
    "            # print(fair_constr)\n",
    "            # print(a)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = loss_criterion(out, batch_labels)\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "        # np the losses and\n",
    "        fair_constr = np.array(fair_constr)\n",
    "        losses = np.array(losses)\n",
    "\n",
    "        # compute the mean + std over the dataset of posrate\n",
    "        for idx in range(0, len(fair_constr_subgroups)):\n",
    "            subgroup_posrates = np.array(fair_constr_subgroups[idx])\n",
    "            fair_constr_subgroups_ret[idx] = (\n",
    "                subgroup_posrates.mean(),\n",
    "                subgroup_posrates.std(),\n",
    "            )\n",
    "\n",
    "    return fair_constr.mean(), fair_constr_subgroups_ret, losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47114586",
   "metadata": {},
   "source": [
    "### Positive Rate\n",
    "\n",
    "The first constraint scenario we test, is a positive rate among all subgroups. We choose the infinite norm of the violation vector. Although, such loss works with the importance sampling, the i.i.d sampling starts to completely ignore the constraint penalty and only focuses on the loss function, making the constraint useless. It is only when the batch size starts to grow in size, the optimization start focusing on the constraint. \n",
    "\n",
    "#### 1. Balanced PosRate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "import copy\n",
    "\n",
    "# alloc arrays for plotting\n",
    "balanced_losses_train_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_const_train_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_constr_test_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_test_loss_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_constr_subgroup_posrate = [\n",
    "    [[] for _ in range(10)] for _ in range(len(batch_sizes))\n",
    "]  # B x SUBGROUPS x EPOCHS\n",
    "\n",
    "for batch_size_idx, balanced_sampler in enumerate(balanced_samplers):\n",
    "    print(f\"BATCH SIZE TESTING: {batch_sizes[batch_size_idx]}\\n\")\n",
    "\n",
    "    ################################### INIT THE MODEL ############################################\n",
    "\n",
    "    # set the same seed for fair comparisons\n",
    "    torch.manual_seed(seed_n)\n",
    "\n",
    "    hsize1 = 64\n",
    "    hsize2 = 32\n",
    "    model_con = Sequential(\n",
    "        torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize1, hsize2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize2, 1),\n",
    "    )\n",
    "\n",
    "    optimizer = SSLALM_Adam(\n",
    "        params=model_con.parameters(),\n",
    "        m=1,  # number of constraints - one in our case\n",
    "        lr=0.05,  # primal variable lr\n",
    "        dual_lr=0.08,  # lr of a dual ALM variable\n",
    "        dual_bound=5,\n",
    "        rho=1,  # rho penalty in ALM parameter\n",
    "        mu=2,  # smoothing parameter\n",
    "    )\n",
    "\n",
    "    # add slack variables - to create the equality from the inequalities\n",
    "    slack_vars = torch.zeros(1, requires_grad=True)\n",
    "    optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})\n",
    "\n",
    "    ################################### TRAIN/TEST THE MODEL ############################################\n",
    "\n",
    "    # alloc arrays for plotting\n",
    "    SSLALM_S_loss_log_plotting = []  # mean\n",
    "    SSLALM_S_c_log_plotting = []  # mean\n",
    "    SSLALM_S_loss_std_log_plotting = []  # std\n",
    "    SSLALM_S_c_std_log_plotting = []  # std\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # alloc the logging arrays for the batch\n",
    "        loss_log = []\n",
    "        c_log = []\n",
    "        duals_log = []\n",
    "\n",
    "        # go though all data\n",
    "        for batch_input, batch_sens, batch_label in balanced_sampler:\n",
    "            # calculate constraints and constraint grads\n",
    "            out = model_con(batch_input)\n",
    "            fair_loss = fair_criterion(out, batch_sens)\n",
    "\n",
    "            # calculate the fair constraint violation\n",
    "            fair_constraint = fair_loss + slack_vars[0] - fair_crit_bound\n",
    "            fair_constraint.backward(retain_graph=True)\n",
    "\n",
    "            # perform the dual step variable + save the dual grad for later\n",
    "            optimizer.dual_step(0, c_val=fair_constraint)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the fair loss violation for logging\n",
    "            c_log.append([fair_loss.detach().item()])\n",
    "            duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "            # calculate primal loss and grad\n",
    "            loss = criterion(out, batch_label) + 0 * slack_vars[0]\n",
    "            loss.backward()\n",
    "            loss_log.append(loss.detach().numpy())\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "            with torch.no_grad():\n",
    "                for s in slack_vars:\n",
    "                    if s < 0:\n",
    "                        s.zero_()\n",
    "\n",
    "        # test the model fairness\n",
    "        test_constr, subgroups_constr_epoch, test_loss = test_model_fairness(\n",
    "            model_con, fair_criterion, criterion\n",
    "        )\n",
    "\n",
    "        # save the data\n",
    "        balanced_constr_test_posrate[batch_size_idx] += [test_constr]\n",
    "        balanced_test_loss_posrate[batch_size_idx] += [test_loss]\n",
    "\n",
    "        # for each group update the current epoch pos rate for that group\n",
    "        for group_idx in range(\n",
    "            0, len(balanced_constr_subgroup_posrate[batch_size_idx])\n",
    "        ):\n",
    "            balanced_constr_subgroup_posrate[batch_size_idx][group_idx] += [\n",
    "                subgroups_constr_epoch[group_idx]\n",
    "            ]\n",
    "\n",
    "        optimizer.dual_lr *= 0.95\n",
    "        SSLALM_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "        SSLALM_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "        SSLALM_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "        SSLALM_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"train/test loss: {np.mean(loss_log)}, / {test_loss}, \"\n",
    "            f\"Test Constr: {test_constr}, \"\n",
    "            f\"Train Constr: {np.mean(c_log, axis=0)}, \"\n",
    "        )\n",
    "\n",
    "        # print out the groups\n",
    "        for idx, (subgroup_epoch_constr_mean, subgroup_epoch_constr_std) in enumerate(\n",
    "            subgroups_constr_epoch\n",
    "        ):\n",
    "            print(\n",
    "                f\"{group_dict[idx]}: {subgroup_epoch_constr_mean}, {subgroup_epoch_constr_std}\"\n",
    "            )\n",
    "\n",
    "        print(f\"dual: {np.mean(duals_log, axis=0)}\")\n",
    "\n",
    "    # save the data after training\n",
    "    balanced_losses_train_posrate[batch_size_idx] = copy.deepcopy(\n",
    "        SSLALM_S_loss_log_plotting\n",
    "    )\n",
    "    balanced_const_train_posrate[batch_size_idx] = copy.deepcopy(\n",
    "        SSLALM_S_c_log_plotting\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38001951",
   "metadata": {},
   "source": [
    "#### 2. Unbalanced PosRate\n",
    "Train-test the SSL-ALM-Adam with the unbalanced sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "import copy\n",
    "\n",
    "# alloc arrays for plotting\n",
    "unbalanced_losses_train_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_const_train_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_constr_test_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_test_loss_posrate = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_constr_subgroup_posrate = [\n",
    "    [[] for _ in range(10)] for _ in range(len(batch_sizes))\n",
    "]  # B x SUBGROUPS x EPOCHS\n",
    "\n",
    "for batch_size_idx, unbalanced_sampler in enumerate(unbalanced_samplers):\n",
    "    print(f\"BATCH SIZE TESTING: {batch_sizes[batch_size_idx]}\\n\")\n",
    "\n",
    "    ################################### INIT THE MODEL ############################################\n",
    "\n",
    "    # set the same seed for fair comparisons\n",
    "    torch.manual_seed(seed_n)\n",
    "\n",
    "    hsize1 = 64\n",
    "    hsize2 = 32\n",
    "    model_con = Sequential(\n",
    "        torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize1, hsize2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize2, 1),\n",
    "    )\n",
    "\n",
    "    optimizer = SSLALM_Adam(\n",
    "        params=model_con.parameters(),\n",
    "        m=1,  # number of constraints - one in our case\n",
    "        lr=0.05,  # primal variable lr\n",
    "        dual_lr=0.08,  # lr of a dual ALM variable\n",
    "        dual_bound=5,\n",
    "        rho=1,  # rho penalty in ALM parameter\n",
    "        mu=2,  # smoothing parameter\n",
    "    )\n",
    "\n",
    "    # add slack variables - to create the equality from the inequalities\n",
    "    slack_vars = torch.zeros(1, requires_grad=True)\n",
    "    optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})\n",
    "\n",
    "    ################################### TRAIN/TEST THE MODEL ############################################\n",
    "\n",
    "    # alloc arrays for plotting\n",
    "    SSLALM_S_loss_log_plotting = []  # mean\n",
    "    SSLALM_S_c_log_plotting = []  # mean\n",
    "    SSLALM_S_loss_std_log_plotting = []  # std\n",
    "    SSLALM_S_c_std_log_plotting = []  # std\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # alloc the logging arrays for the batch\n",
    "        loss_log = []\n",
    "        c_log = []\n",
    "        duals_log = []\n",
    "\n",
    "        # go though all data\n",
    "        for batch_input, batch_sens, batch_label in unbalanced_sampler:\n",
    "            # calculate constraints and constraint grads\n",
    "            out = model_con(batch_input)\n",
    "            fair_loss = fair_criterion(out, batch_sens)\n",
    "\n",
    "            # calculate the fair constraint violation\n",
    "            fair_constraint = fair_loss + slack_vars[0] - fair_crit_bound\n",
    "            fair_constraint.backward(retain_graph=True)\n",
    "\n",
    "            # perform the dual step variable + save the dual grad for later\n",
    "            optimizer.dual_step(0, c_val=fair_constraint)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the fair loss violation for logging\n",
    "            c_log.append([fair_loss.detach().item()])\n",
    "            duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "            # calculate primal loss and grad\n",
    "            loss = criterion(out, batch_label) + 0 * slack_vars[0]\n",
    "            loss.backward()\n",
    "            loss_log.append(loss.detach().numpy())\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "            with torch.no_grad():\n",
    "                for s in slack_vars:\n",
    "                    if s < 0:\n",
    "                        s.zero_()\n",
    "\n",
    "        # test the model fairness\n",
    "        test_constr, subgroups_constr_epoch, test_loss = test_model_fairness(\n",
    "            model_con, fair_criterion, criterion\n",
    "        )\n",
    "\n",
    "        # save the data\n",
    "        unbalanced_constr_test_posrate[batch_size_idx] += [test_constr]\n",
    "        unbalanced_test_loss_posrate[batch_size_idx] += [test_loss]\n",
    "\n",
    "        # for each group update the current epoch pos rate for that group\n",
    "        for group_idx in range(\n",
    "            0, len(unbalanced_constr_subgroup_posrate[batch_size_idx])\n",
    "        ):\n",
    "            unbalanced_constr_subgroup_posrate[batch_size_idx][group_idx] += [\n",
    "                subgroups_constr_epoch[group_idx]\n",
    "            ]\n",
    "\n",
    "        optimizer.dual_lr *= 0.95\n",
    "        SSLALM_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "        SSLALM_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "        SSLALM_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "        SSLALM_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"train/test loss: {np.mean(loss_log)}, / {test_loss}, \"\n",
    "            f\"Test Constr: {test_constr}, \"\n",
    "            f\"Train Constr: {np.mean(c_log, axis=0)}, \"\n",
    "        )\n",
    "\n",
    "        # print out the groups\n",
    "        for idx, (subgroup_epoch_constr_mean, subgroup_epoch_constr_std) in enumerate(\n",
    "            subgroups_constr_epoch\n",
    "        ):\n",
    "            print(\n",
    "                f\"{group_dict[idx]}: {subgroup_epoch_constr_mean}, {subgroup_epoch_constr_std}\"\n",
    "            )\n",
    "\n",
    "        print(f\"dual: {np.mean(duals_log, axis=0)}\")\n",
    "\n",
    "    # save the data after training\n",
    "    unbalanced_losses_train_posrate[batch_size_idx] = copy.deepcopy(\n",
    "        SSLALM_S_loss_log_plotting\n",
    "    )\n",
    "    unbalanced_const_train_posrate[batch_size_idx] = copy.deepcopy(\n",
    "        SSLALM_S_c_log_plotting\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e865c1",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_all_results(\n",
    "    batch_sizes,\n",
    "    unbalanced_losses_train,\n",
    "    unbalanced_losses_test,\n",
    "    balanced_losses_train,\n",
    "    balanced_losses_test,\n",
    "    unbalanced_const_train,\n",
    "    balanced_const_train,\n",
    "    unbalanced_constr_test,\n",
    "    balanced_constr_test,\n",
    "    unbalanced_constr_subgroup,\n",
    "    balanced_constr_subgroup,\n",
    "    group_dict,\n",
    "    threshold_value=0.0,\n",
    "    per_group_threshold=False,\n",
    "    save_path=None,\n",
    "):\n",
    "    num_batches = len(batch_sizes)\n",
    "    num_groups = len(group_dict)\n",
    "\n",
    "    total_rows = 2 + num_groups  # Row0 losses, Row1 overall, then subgroup rows\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=total_rows,\n",
    "        ncols=num_batches,\n",
    "        figsize=(5 * num_batches, 3 * total_rows),\n",
    "        sharex=False,\n",
    "    )\n",
    "\n",
    "    if num_batches == 1:\n",
    "        axes = axes.reshape(total_rows, 1)\n",
    "\n",
    "    color_u = \"red\"\n",
    "    color_b = \"blue\"\n",
    "    markers = [\"o\", \"s\", \"^\", \"D\", \"v\", \"<\", \">\", \"p\", \"x\", \"*\"]\n",
    "\n",
    "    # -------- Helper: mean + std shading -------- #\n",
    "    def plot_mean_std(ax, data, label, color, marker=None, ls=\"-\", lw=1.8):\n",
    "        means = np.array([m for m, _ in data])\n",
    "        stds = np.array([s for _, s in data])\n",
    "        x = np.arange(len(means))\n",
    "\n",
    "        ax.plot(\n",
    "            x,\n",
    "            means,\n",
    "            color=color,\n",
    "            linestyle=ls,\n",
    "            marker=marker,\n",
    "            markersize=3,\n",
    "            linewidth=lw,\n",
    "            label=label,\n",
    "        )\n",
    "\n",
    "        ax.fill_between(x, means - stds, means + stds, color=color, alpha=0.15)\n",
    "\n",
    "    # =====================================================\n",
    "    #                  ROW 0  LOSSES\n",
    "    # =====================================================\n",
    "    ax = axes[0, 0]\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    for b, batch in enumerate(batch_sizes):\n",
    "        ax = axes[0, b]\n",
    "\n",
    "        ax.plot(\n",
    "            unbalanced_losses_train[b],\n",
    "            color=color_u,\n",
    "            ls=\"-\",\n",
    "            lw=2,\n",
    "            label=\"Unbalanced Train Loss\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            unbalanced_losses_test[b],\n",
    "            color=color_u,\n",
    "            ls=\"--\",\n",
    "            lw=2,\n",
    "            label=\"Unbalanced Test Loss\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            balanced_losses_train[b],\n",
    "            color=color_b,\n",
    "            ls=\"-\",\n",
    "            lw=2,\n",
    "            label=\"Balanced Train Loss\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            balanced_losses_test[b],\n",
    "            color=color_b,\n",
    "            ls=\"--\",\n",
    "            lw=2,\n",
    "            label=\"Balanced Test Loss\",\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Batch size: {batch}\")\n",
    "        ax.grid(True)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    # =====================================================\n",
    "    #       ROW 1  OVERALL TRAIN/TEST CONSTRAINTS\n",
    "    # =====================================================\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    ax.set_ylabel(\"Overall Constraint\")\n",
    "\n",
    "    for b in range(num_batches):\n",
    "        ax = axes[1, b]\n",
    "\n",
    "        ax.plot(\n",
    "            unbalanced_const_train[b],\n",
    "            color=color_u,\n",
    "            ls=\"-\",\n",
    "            lw=2,\n",
    "            label=\"Unbalanced Train\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            unbalanced_constr_test[b],\n",
    "            color=color_u,\n",
    "            ls=\"--\",\n",
    "            lw=2,\n",
    "            label=\"Unbalanced Test\",\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            balanced_const_train[b], color=color_b, ls=\"-\", lw=2, label=\"Balanced Train\"\n",
    "        )\n",
    "        ax.plot(\n",
    "            balanced_constr_test[b], color=color_b, ls=\"--\", lw=2, label=\"Balanced Test\"\n",
    "        )\n",
    "\n",
    "        ax.axhline(threshold_value, color=\"black\", linestyle=\":\", linewidth=2)\n",
    "        ax.grid(True)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    # =====================================================\n",
    "    #   INSERT SEPARATOR LINE BETWEEN ROW 1 AND SUBGROUP ROWS\n",
    "    # =====================================================\n",
    "    separator_y = 1 - (2 / total_rows)\n",
    "\n",
    "    fig.add_artist(\n",
    "        plt.Line2D(\n",
    "            [0.02, 0.995],  # horizontal across figure\n",
    "            [separator_y - 0.005, separator_y - 0.005],\n",
    "            transform=fig.transFigure,\n",
    "            color=\"black\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # =====================================================\n",
    "    #         ROWS 2+  SUBGROUP CONSTRAINTS ONLY (mean  std)\n",
    "    # =====================================================\n",
    "\n",
    "    if per_group_threshold:\n",
    "        threshold_value = threshold_value / num_groups\n",
    "\n",
    "    for g in range(num_groups):\n",
    "        group_name = group_dict[g]\n",
    "        row = 2 + g\n",
    "\n",
    "        # replace long name\n",
    "        group_name = group_name.replace(\"or under 15 years old\", \"\")\n",
    "\n",
    "        ax = axes[row, 0]\n",
    "        ax.set_ylabel(f\"{group_name}\\nConstraint\")\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            ax = axes[row, b]\n",
    "            marker = markers[g % len(markers)]\n",
    "\n",
    "            # Only individual subgroup curves (no train/test distinction)\n",
    "            plot_mean_std(\n",
    "                ax,\n",
    "                unbalanced_constr_subgroup[b][g],\n",
    "                f\"Unbalanced  {group_name}\",\n",
    "                color_u,\n",
    "                marker=marker,\n",
    "            )\n",
    "            plot_mean_std(\n",
    "                ax,\n",
    "                balanced_constr_subgroup[b][g],\n",
    "                f\"Balanced  {group_name}\",\n",
    "                color_b,\n",
    "                marker=marker,\n",
    "            )\n",
    "\n",
    "            if per_group_threshold:\n",
    "                ax.axhline(\n",
    "                    threshold_value,\n",
    "                    color=\"black\",\n",
    "                    linestyle=\":\",\n",
    "                    linewidth=1.5,\n",
    "                    label=\"Average Group-wise Threshold\",\n",
    "                )\n",
    "            else:\n",
    "                ax.axhline(\n",
    "                    threshold_value,\n",
    "                    color=\"black\",\n",
    "                    linestyle=\":\",\n",
    "                    linewidth=1.5,\n",
    "                    label=\"Threshold\",\n",
    "                )\n",
    "            ax.grid(True)\n",
    "            ax.legend(fontsize=8)\n",
    "\n",
    "            if g == num_groups - 1:\n",
    "                ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_path, dpi=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92227f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(\n",
    "    batch_sizes,\n",
    "    unbalanced_losses_train_posrate,\n",
    "    unbalanced_test_loss_posrate,\n",
    "    balanced_losses_train_posrate,\n",
    "    balanced_test_loss_posrate,\n",
    "    unbalanced_const_train_posrate,\n",
    "    balanced_const_train_posrate,\n",
    "    unbalanced_constr_test_posrate,\n",
    "    balanced_constr_test_posrate,\n",
    "    unbalanced_constr_subgroup_posrate,\n",
    "    balanced_constr_subgroup_posrate,\n",
    "    group_dict,\n",
    "    threshold_value=fair_crit_bound,\n",
    "    save_path=\"./data/figs/IS_posrate_infnorm.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc69fd",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "The second constraint scenario we test, is the accuracy among all subgroups. We choose the Manhattan norm of the violation vector. Although, such loss works with the importance sampling, the i.i.d sampling struggles to decrease the constraint penalty while also having a trouble to decrease the primal loss function. \n",
    "\n",
    "#### 1.b Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import PositiveRate\n",
    "from fairret.statistic import Accuracy\n",
    "from fairret.loss import NormLoss\n",
    "\n",
    "# define the fairness criterion\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "statistic = Accuracy()\n",
    "fair_criterion = NormLoss(statistic=statistic, p=1)\n",
    "fair_crit_bound = 0.3  # define the bound on the criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_fairness(model_to_test, fair_criterion, loss_criterion):\n",
    "    \"\"\"Test the model fairness on the test dataset\"\"\"\n",
    "\n",
    "    fair_constr = []\n",
    "    fair_constr_subgroups = [[] for _ in range(10)]\n",
    "    fair_constr_subgroups_ret = [() for _ in range(10)]\n",
    "    losses = []\n",
    "\n",
    "    test_sampler = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=2048 * 2, shuffle=False\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # go though all test data\n",
    "        for batch_input, batch_sens, batch_labels in test_sampler:\n",
    "            # compute the fairness loss\n",
    "            out = model_to_test(batch_input)\n",
    "            fair_loss = fair_criterion(\n",
    "                out, batch_sens, batch_labels\n",
    "            )  # sigmoid is implicit here\n",
    "\n",
    "            # compute the fairness per group\n",
    "            preds = torch.nn.functional.sigmoid(out)\n",
    "            pr = Accuracy()\n",
    "            probs_per_group = pr(preds, batch_sens, batch_labels)  # P(y=1|Sk = 1)\n",
    "            overall = Accuracy().overall_statistic(\n",
    "                preds, batch_labels\n",
    "            )  # take the P(y=1)\n",
    "\n",
    "            # compute the posrate for the group\n",
    "            for idx, prob_per_group in enumerate(probs_per_group):\n",
    "                posrate_group = torch.abs(prob_per_group / overall - 1.0)\n",
    "                fair_constr_subgroups[idx] += [posrate_group.detach().item()]\n",
    "\n",
    "            # save the data\n",
    "            fair_constr += [fair_loss.detach().item()]\n",
    "\n",
    "            # print(fair_constr)\n",
    "            # print(a)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = loss_criterion(out, batch_labels)\n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "        # np the losses and\n",
    "        fair_constr = np.array(fair_constr)\n",
    "        losses = np.array(losses)\n",
    "\n",
    "        # compute the mean + std over the dataset of posrate\n",
    "        for idx in range(0, len(fair_constr_subgroups)):\n",
    "            subgroup_acc = np.array(fair_constr_subgroups[idx])\n",
    "            fair_constr_subgroups_ret[idx] = (subgroup_acc.mean(), subgroup_acc.std())\n",
    "\n",
    "    return fair_constr.mean(), fair_constr_subgroups_ret, losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b68cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "import copy\n",
    "\n",
    "# alloc arrays for plotting\n",
    "balanced_losses_train_acc = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_const_train_acc = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_constr_test_acc = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_test_loss_acc = [[] for _ in range(len(batch_sizes))]\n",
    "balanced_constr_subgroup_acc = [\n",
    "    [[] for _ in range(10)] for _ in range(len(batch_sizes))\n",
    "]  # B x SUBGROUPS x EPOCHS\n",
    "\n",
    "for batch_size_idx, balanced_sampler in enumerate(balanced_samplers):\n",
    "    print(f\"BATCH SIZE TESTING: {batch_sizes[batch_size_idx]}\\n\")\n",
    "\n",
    "    ################################### INIT THE MODEL ############################################\n",
    "\n",
    "    # set the same seed for fair comparisons\n",
    "    torch.manual_seed(seed_n)\n",
    "\n",
    "    hsize1 = 64\n",
    "    hsize2 = 32\n",
    "    model_con = Sequential(\n",
    "        torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize1, hsize2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize2, 1),\n",
    "    )\n",
    "\n",
    "    optimizer = SSLALM_Adam(\n",
    "        params=model_con.parameters(),\n",
    "        m=1,  # number of constraints - one in our case\n",
    "        lr=0.05,  # primal variable lr\n",
    "        dual_lr=0.08,  # lr of a dual ALM variable\n",
    "        dual_bound=5,\n",
    "        rho=1,  # rho penalty in ALM parameter\n",
    "        mu=2,  # smoothing parameter\n",
    "    )\n",
    "\n",
    "    # add slack variables - to create the equality from the inequalities\n",
    "    slack_vars = torch.zeros(1, requires_grad=True)\n",
    "    optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})\n",
    "\n",
    "    ################################### TRAIN/TEST THE MODEL ############################################\n",
    "\n",
    "    # alloc arrays for plotting\n",
    "    SSLALM_S_loss_log_plotting = []  # mean\n",
    "    SSLALM_S_c_log_plotting = []  # mean\n",
    "    SSLALM_S_loss_std_log_plotting = []  # std\n",
    "    SSLALM_S_c_std_log_plotting = []  # std\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # alloc the logging arrays for the batch\n",
    "        loss_log = []\n",
    "        c_log = []\n",
    "        duals_log = []\n",
    "\n",
    "        # go though all data\n",
    "        for batch_input, batch_sens, batch_label in balanced_sampler:\n",
    "            # calculate constraints and constraint grads\n",
    "            out = model_con(batch_input)\n",
    "            fair_loss = fair_criterion(out, batch_sens, batch_label)\n",
    "\n",
    "            # calculate the fair constraint violation\n",
    "            fair_constraint = fair_loss + slack_vars[0] - fair_crit_bound\n",
    "            fair_constraint.backward(retain_graph=True)\n",
    "\n",
    "            # perform the dual step variable + save the dual grad for later\n",
    "            optimizer.dual_step(0, c_val=fair_constraint)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the fair loss violation for logging\n",
    "            c_log.append([fair_loss.detach().item()])\n",
    "            duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "            # calculate primal loss and grad\n",
    "            loss = criterion(out, batch_label) + 0 * slack_vars[0]\n",
    "            loss.backward()\n",
    "            loss_log.append(loss.detach().numpy())\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "            with torch.no_grad():\n",
    "                for s in slack_vars:\n",
    "                    if s < 0:\n",
    "                        s.zero_()\n",
    "\n",
    "        # test the model fairness\n",
    "        test_constr, subgroups_constr_epoch, test_loss = test_model_fairness(\n",
    "            model_con, fair_criterion, criterion\n",
    "        )\n",
    "\n",
    "        # save the data\n",
    "        balanced_constr_test_acc[batch_size_idx] += [test_constr]\n",
    "        balanced_test_loss_acc[batch_size_idx] += [test_loss]\n",
    "\n",
    "        # for each group update the current epoch pos rate for that group\n",
    "        for group_idx in range(0, len(balanced_constr_subgroup_acc[batch_size_idx])):\n",
    "            balanced_constr_subgroup_acc[batch_size_idx][group_idx] += [\n",
    "                subgroups_constr_epoch[group_idx]\n",
    "            ]\n",
    "\n",
    "        optimizer.dual_lr *= 0.95\n",
    "        SSLALM_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "        SSLALM_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "        SSLALM_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "        SSLALM_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"train/test loss: {np.mean(loss_log)}, / {test_loss}, \"\n",
    "            f\"Test Constr: {test_constr}, \"\n",
    "            f\"Train Constr: {np.mean(c_log, axis=0)}, \"\n",
    "        )\n",
    "\n",
    "        # print out the groups\n",
    "        for idx, (subgroup_epoch_constr_mean, subgroup_epoch_constr_std) in enumerate(\n",
    "            subgroups_constr_epoch\n",
    "        ):\n",
    "            print(\n",
    "                f\"{group_dict[idx]}: {subgroup_epoch_constr_mean}, {subgroup_epoch_constr_std}\"\n",
    "            )\n",
    "\n",
    "        print(f\"dual: {np.mean(duals_log, axis=0)}\")\n",
    "\n",
    "    # save the data after training\n",
    "    balanced_losses_train_acc[batch_size_idx] = copy.deepcopy(\n",
    "        SSLALM_S_loss_log_plotting\n",
    "    )\n",
    "    balanced_const_train_acc[batch_size_idx] = copy.deepcopy(SSLALM_S_c_log_plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539f9e3",
   "metadata": {},
   "source": [
    "#### 2.b Unbalanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.humancompatible.train.optim.ssl_alm_adam import SSLALM_Adam\n",
    "import copy\n",
    "\n",
    "# alloc arrays for plotting\n",
    "unbalanced_losses_train_acc = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_const_train_acc = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_constr_test_acc = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_test_loss_acc = [[] for _ in range(len(batch_sizes))]\n",
    "unbalanced_constr_subgroup_acc = [\n",
    "    [[] for _ in range(10)] for _ in range(len(batch_sizes))\n",
    "]  # B x SUBGROUPS x EPOCHS\n",
    "\n",
    "for batch_size_idx, unbalanced_sampler in enumerate(unbalanced_samplers):\n",
    "    print(f\"BATCH SIZE TESTING: {batch_sizes[batch_size_idx]}\\n\")\n",
    "\n",
    "    ################################### INIT THE MODEL ############################################\n",
    "\n",
    "    # set the same seed for fair comparisons\n",
    "    torch.manual_seed(seed_n)\n",
    "\n",
    "    hsize1 = 64\n",
    "    hsize2 = 32\n",
    "    model_con = Sequential(\n",
    "        torch.nn.Linear(features_train.shape[1], hsize1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize1, hsize2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hsize2, 1),\n",
    "    )\n",
    "\n",
    "    optimizer = SSLALM_Adam(\n",
    "        params=model_con.parameters(),\n",
    "        m=1,  # number of constraints - one in our case\n",
    "        lr=0.05,  # primal variable lr\n",
    "        dual_lr=0.08,  # lr of a dual ALM variable\n",
    "        dual_bound=5,\n",
    "        rho=1,  # rho penalty in ALM parameter\n",
    "        mu=2,  # smoothing parameter\n",
    "    )\n",
    "\n",
    "    # add slack variables - to create the equality from the inequalities\n",
    "    slack_vars = torch.zeros(1, requires_grad=True)\n",
    "    optimizer.add_param_group(param_group={\"params\": slack_vars, \"name\": \"slack\"})\n",
    "\n",
    "    ################################### TRAIN/TEST THE MODEL ############################################\n",
    "\n",
    "    # alloc arrays for plotting\n",
    "    SSLALM_S_loss_log_plotting = []  # mean\n",
    "    SSLALM_S_c_log_plotting = []  # mean\n",
    "    SSLALM_S_loss_std_log_plotting = []  # std\n",
    "    SSLALM_S_c_std_log_plotting = []  # std\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # alloc the logging arrays for the batch\n",
    "        loss_log = []\n",
    "        c_log = []\n",
    "        duals_log = []\n",
    "\n",
    "        # go though all data\n",
    "        for batch_input, batch_sens, batch_label in unbalanced_sampler:\n",
    "            # calculate constraints and constraint grads\n",
    "            out = model_con(batch_input)\n",
    "            fair_loss = fair_criterion(out, batch_sens, batch_label)\n",
    "\n",
    "            # calculate the fair constraint violation\n",
    "            fair_constraint = fair_loss + slack_vars[0] - fair_crit_bound\n",
    "            fair_constraint.backward(retain_graph=True)\n",
    "\n",
    "            # perform the dual step variable + save the dual grad for later\n",
    "            optimizer.dual_step(0, c_val=fair_constraint)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the fair loss violation for logging\n",
    "            c_log.append([fair_loss.detach().item()])\n",
    "            duals_log.append(optimizer._dual_vars.detach())\n",
    "\n",
    "            # calculate primal loss and grad\n",
    "            loss = criterion(out, batch_label) + 0 * slack_vars[0]\n",
    "            loss.backward()\n",
    "            loss_log.append(loss.detach().numpy())\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # slack variables must be non-negative. this is the \"projection\" step from the SSL-ALM paper\n",
    "            with torch.no_grad():\n",
    "                for s in slack_vars:\n",
    "                    if s < 0:\n",
    "                        s.zero_()\n",
    "\n",
    "        # test the model fairness\n",
    "        test_constr, subgroups_constr_epoch, test_loss = test_model_fairness(\n",
    "            model_con, fair_criterion, criterion\n",
    "        )\n",
    "\n",
    "        # save the data\n",
    "        unbalanced_constr_test_acc[batch_size_idx] += [test_constr]\n",
    "        unbalanced_test_loss_acc[batch_size_idx] += [test_loss]\n",
    "\n",
    "        # for each group update the current epoch pos rate for that group\n",
    "        for group_idx in range(0, len(unbalanced_constr_subgroup_acc[batch_size_idx])):\n",
    "            unbalanced_constr_subgroup_acc[batch_size_idx][group_idx] += [\n",
    "                subgroups_constr_epoch[group_idx]\n",
    "            ]\n",
    "\n",
    "        optimizer.dual_lr *= 0.95\n",
    "        SSLALM_S_c_log_plotting.append(np.mean(c_log, axis=0))\n",
    "        SSLALM_S_loss_log_plotting.append(np.mean(loss_log))\n",
    "        SSLALM_S_c_std_log_plotting.append(np.std(c_log, axis=0))\n",
    "        SSLALM_S_loss_std_log_plotting.append(np.std(loss_log, axis=0))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, \"\n",
    "            f\"train/test loss: {np.mean(loss_log)}, / {test_loss}, \"\n",
    "            f\"Test Constr: {test_constr}, \"\n",
    "            f\"Train Constr: {np.mean(c_log, axis=0)}, \"\n",
    "        )\n",
    "\n",
    "        # print out the groups\n",
    "        for idx, (subgroup_epoch_constr_mean, subgroup_epoch_constr_std) in enumerate(\n",
    "            subgroups_constr_epoch\n",
    "        ):\n",
    "            print(\n",
    "                f\"{group_dict[idx]}: {subgroup_epoch_constr_mean}, {subgroup_epoch_constr_std}\"\n",
    "            )\n",
    "\n",
    "        print(f\"dual: {np.mean(duals_log, axis=0)}\")\n",
    "\n",
    "    # save the data after training\n",
    "    unbalanced_losses_train_acc[batch_size_idx] = copy.deepcopy(\n",
    "        SSLALM_S_loss_log_plotting\n",
    "    )\n",
    "    unbalanced_const_train_acc[batch_size_idx] = copy.deepcopy(SSLALM_S_c_log_plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e21d64",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3384e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(\n",
    "    batch_sizes,\n",
    "    unbalanced_losses_train_acc,\n",
    "    unbalanced_test_loss_acc,\n",
    "    balanced_losses_train_acc,\n",
    "    balanced_test_loss_acc,\n",
    "    unbalanced_const_train_acc,\n",
    "    balanced_const_train_acc,\n",
    "    unbalanced_constr_test_acc,\n",
    "    balanced_constr_test_acc,\n",
    "    unbalanced_constr_subgroup_acc,\n",
    "    balanced_constr_subgroup_acc,\n",
    "    group_dict,\n",
    "    threshold_value=fair_crit_bound,  # the expected loss per subgroup\n",
    "    per_group_threshold=True,\n",
    "    save_path=\"./data/figs/IS_acc_manhattannorm.png\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
