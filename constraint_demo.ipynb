{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efc20cc",
   "metadata": {},
   "source": [
    "This notebook will demonstrate the `FairnessConstraint` class and how you can use it to **add a constraint**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ccbc7",
   "metadata": {},
   "source": [
    "**Fairness constraints** typically involve working with numerous **protected groups** - as such, **sampling** data to evaluate them can sometimes be tricky. Ideally, each minibatch should **contain samples from each of the protected groups** ...\n",
    "\n",
    "`FairnessConstraint` provides the functionality to sample for and evaluate user-defined fairness constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe05ba",
   "metadata": {},
   "source": [
    "A toy example: we have a dataset of 10 samples with 2 groups. We want to add an \"equal loss\" constraint on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58feeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.constraints.constraint import FairnessConstraint\n",
    "from src.constraints.constraint_fns import one_sided_loss_constr\n",
    "\n",
    "# the toy dataset\n",
    "features = torch.arange(0,10)\n",
    "# encode group membership of each sample\n",
    "group_membership = torch.repeat_interleave(torch.tensor([0,1]), 5)\n",
    "labels = group_membership\n",
    "dataset = torch.utils.data.TensorDataset(features, labels)\n",
    "\n",
    "# For each subgroup, FairnessConstraint needs a list of indices of samples belonging to that subgroup\n",
    "group_indices = [(group_membership == gr_idx).nonzero(as_tuple=True)[0] for gr_idx in group_membership.unique()]\n",
    "\n",
    "c = FairnessConstraint(\n",
    "    dataset=dataset,\n",
    "    group_indices=group_indices,\n",
    "    fn=one_sided_loss_constr,\n",
    "    batch_size=2,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e2463",
   "metadata": {},
   "source": [
    "The constructor creates a `DataLoader` for each of the subgroup. Now, when you call `sample_loader()`, it will return a `(features: torch.Tensor, labels: torch.Tensor)` tuple for each subrgoup, with specified batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "058dfa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([0]), tensor([0])], [tensor([8]), tensor([1])]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sample_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff4a55",
   "metadata": {},
   "source": [
    "When one of the dataloaders runs out, it is reset just like during normal PyTorch training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284acab",
   "metadata": {},
   "source": [
    "What about the constraint itself?\n",
    "The constraint formulation is defined by the `fn` argument. When you call the `eval(net, sample)` method of a `FairnessConstraint`, it just passes those arguments plus kwargs to the constraint function and returns the result. It is expected that the constraint function takes the **model** and the **minibatch** in format specified above as the arguments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
