{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "from utils.load_folktables import prepare_folktables, prepare_folktables_multattr\n",
    "from src.constraints.constraint_fns import *\n",
    "from fairret.statistic import *\n",
    "from utils.network import SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents some useful plots based on the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparation**\n",
    "\n",
    "**Load the Folktables dataset for the selected state and prepare it for usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"income\"\n",
    "# TASK = 'employment'\n",
    "STATE = \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    group_ind_train,\n",
    "    sep_group_ind_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    group_ind_test,\n",
    "    sep_group_ind_test,\n",
    ") = prepare_folktables_multattr(\n",
    "    TASK,\n",
    "    state=STATE.upper(),\n",
    "    random_state=42,\n",
    "    onehot=False,\n",
    "    download=True,\n",
    "    sens_cols=[\n",
    "        \"MAR\",\n",
    "        # 'RAC1P',\n",
    "        # 'SEX',\n",
    "    ],\n",
    "    binarize=[None],\n",
    "    stratify=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() and False else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = tensor(y_train, dtype=torch.float, device=device)\n",
    "\n",
    "X_test_tensor = tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = tensor(y_test, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load saved models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "constraints = {\n",
    "    'abs_loss_equality': 0.01,\n",
    "    # \"loss_equality\": 0.005,\n",
    "    \"unconstrained\": None,\n",
    "}\n",
    "\n",
    "dict_alg_names = {\n",
    "    \"StochasticGhost\": \"StGh\",\n",
    "    \"ALM\": \"ALM\",\n",
    "    \"SSLALM\": \"SSL-ALM\",\n",
    "    \"SSG\": \"SSw\",\n",
    "    \"SGD\": \"SGD\",\n",
    "    \"fairret\": \"SGD-Fairret\",\n",
    "}\n",
    "\n",
    "DATASET = TASK + \"_\" + STATE\n",
    "loaded_models = []\n",
    "\n",
    "for constr, cb in constraints.items():\n",
    "    DIRECTORY_PATH = (\n",
    "        \"./utils/saved_models/\" + DATASET + \"/\" + constr + \"/\" + ((f\"{cb:.0E}\" + \"/\") if cb is not None else '')\n",
    "    )\n",
    "    FILE_EXT = \".pt\"\n",
    "\n",
    "    directory_path = DIRECTORY_PATH\n",
    "    print(f\"Looking for models in: {directory_path}\")\n",
    "    try:\n",
    "        file_list = os.listdir(directory_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Not found\")\n",
    "        continue\n",
    "    model_files = [file for file in file_list if file.endswith(FILE_EXT)]\n",
    "    for model_file in model_files:\n",
    "        if model_file.split(\"_\")[0] not in dict_alg_names.keys():\n",
    "            continue\n",
    "        model_name = model_file\n",
    "        model = SimpleNet(X_test.shape[1], 1, torch.float32).to(device)\n",
    "        print(model_file)\n",
    "        try:\n",
    "            model.load_state_dict(\n",
    "                torch.load(\n",
    "                    directory_path + model_name, weights_only=False, map_location=device\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "        loaded_models.append((model_file, model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate test set statistics for the models - AUC, constraint satisfaction, loss, etc.. and aggregate per algorithm:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stats import make_model_stats_table\n",
    "from utils.stats import aggregate_model_stats_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "bin_dfs = []\n",
    "\n",
    "for group_idx_1, group_idx_2 in list(combinations(group_ind_train, 2)):\n",
    "    X_train_1, y_train_1 = X_train_tensor[group_idx_1], y_train_tensor[group_idx_1]\n",
    "    X_train_2, y_train_2 = X_train_tensor[group_idx_2], y_train_tensor[group_idx_2]\n",
    "    table = make_model_stats_table(\n",
    "        X_train_1, y_train_1, X_train_2, y_train_2, loaded_models\n",
    "    )\n",
    "    table.index = table.Algorithm.apply(lambda x: dict_alg_names[x.split(\"_\")[0]])\n",
    "    table.drop(\"Algorithm\", axis=1, inplace=True)\n",
    "    bin_dfs.append(table)\n",
    "    \n",
    "df_train = pd.concat(bin_dfs, axis=0, keys=range(len(bin_dfs)), names=[\"constraint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = aggregate_model_stats_table(\n",
    "    df_train, \"mean\", agg_cols=[\"constraint\", \"Algorithm\"]\n",
    ")\n",
    "train_df_std = aggregate_model_stats_table(\n",
    "    df_train, [\"mean\", \"std\"], agg_cols=[\"constraint\", \"Algorithm\"]\n",
    ")\n",
    "train_df_std.drop(\"Algname\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in test_df.index:\n",
    "#     alg_name = model_name.split(\"_\")[0]\n",
    "#     os.makedirs(os.path.dirname(f\"./plots/{alg_name}/{DATASET}/\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import spider_line\n",
    "\n",
    "\n",
    "f = spider_line(train_df, yticks=[0,0.1,0.2,0.5])\n",
    "# f = spider_line(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of predictions by group:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_by_alg = {alg: {} for alg in set([model_name.split(\"_\")[0] for model_name, _ in loaded_models])}\n",
    "\n",
    "\n",
    "for i, group in enumerate(group_ind_test):\n",
    "    for model_name, model in loaded_models:\n",
    "        alg = model_name.split(\"_\")[0]\n",
    "\n",
    "        preds = torch.nn.functional.sigmoid(model(X_test_tensor[group])).detach().numpy().squeeze()\n",
    "        try:\n",
    "            predictions_by_alg[alg][i].append(preds)\n",
    "        except:\n",
    "            predictions_by_alg[alg][i] = [preds]\n",
    "\n",
    "    predictions_by_alg[alg][i] = np.concatenate(predictions_by_alg[alg][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "\n",
    "for alg, predictions in predictions_by_alg.items():\n",
    "    sns.kdeplot(\n",
    "        predictions,\n",
    "        fill=True,\n",
    "        alpha=0.1,\n",
    "        bw_adjust=0.4,\n",
    "        ax=ax,\n",
    "        clip=[0,1],\n",
    "        common_norm=False)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(0, 10)\n",
    "    plt.xlabel(\"Predictions\", fontsize=20)\n",
    "    plt.ylabel(\"Density\", fontsize=20)\n",
    "    plt.title(alg)\n",
    "    alg_name = (\n",
    "        \"sslalm_aug\"\n",
    "        if model_name.startswith(\"sslalm_mu0\")\n",
    "        else model_name.split(\"_\")[0]\n",
    "    )\n",
    "    # plt.savefig(f\"./plots/{alg_name}/{DATASET}/dist\")\n",
    "    # plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We choose one model per algorithm to make some useful plots**\n",
    "\n",
    "For now, choose the model with the highest mean AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by = \"auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "algs = df_train.index.get_level_values('Algorithm').unique()\n",
    "for alg in algs:\n",
    "    alg_df = df_train.xs(alg, level=1)\n",
    "    if select_by == \"auc\":\n",
    "        model = loaded_models[alg_df.AUC_M.idxmax()]\n",
    "    elif select_by == \"wd\":\n",
    "        model = loaded_models[alg_df.Wd.idxmin()]\n",
    "    best_models[alg] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subgroup ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPR-FPR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_pr(ax, predictions, targets, sensitive_value):\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f\"group={sensitive_value}, AUC = {roc_auc:.2f}\")\n",
    "    tpr_minus_fpr = tpr - fpr\n",
    "    # Find the threshold that maximizes TPR - FPR difference\n",
    "    optimal_threshold_index = np.argmax(tpr_minus_fpr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(\n",
    "        fpr[optimal_threshold_index],\n",
    "        tpr[optimal_threshold_index],\n",
    "        # c=\"blue\" if sensitive_value == sensitive_value_0 else \"red\",\n",
    "        label=f\"Optimal Threshold {sensitive_value} {optimal_threshold:.2f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(20)\n",
    "    f.set_figheight(20)\n",
    "    ax = f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        for i,group in enumerate(group_ind_test):\n",
    "            predictions = model(X_test_tensor[group])\n",
    "            # Plot ROC for sensitive attribute A=0\n",
    "            plot_roc_curve_pr(\n",
    "                ax, predictions, y_test[group], sensitive_value=i\n",
    "            )\n",
    "            ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "            ax.set_xlabel(\"False Positive Rate\", fontsize=24)\n",
    "            ax.set_ylabel(\"True Positive Rate\", fontsize=24)\n",
    "            ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TNR-FNR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_nr(ax, predictions, targets, sensitive_value):\n",
    "    # Convert PyTorch tensors to numpy arrays\n",
    "    # predictions = predictions.detach().numpy()\n",
    "    # targets = targets.numpy()\n",
    "\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    fnr = 1 - tpr\n",
    "    tnr = 1 - fpr\n",
    "    roc_auc = auc(tnr, fnr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(tnr, fnr, label=f\"Sensitive={sensitive_value}, AUC = {roc_auc:.2f}\")\n",
    "\n",
    "    tnr_minus_fnr = tnr - fnr\n",
    "\n",
    "    # Find the threshold that maximizes tnr - fnr difference\n",
    "    optimal_threshold_index = np.argmax(tnr_minus_fnr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(\n",
    "        fnr[optimal_threshold_index],\n",
    "        tpr[optimal_threshold_index],\n",
    "        # c=\"blue\" if sensitive_value == sensitive_value_0 else \"red\",\n",
    "        label=f\"Optimal Threshold {sensitive_value} {optimal_threshold:.2f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(20)\n",
    "    f.set_figheight(20)\n",
    "    ax = f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        for i,group in enumerate(group_ind_test):\n",
    "            predictions = model(X_test_tensor[group])\n",
    "            # Plot ROC for sensitive attribute A=0\n",
    "            plot_roc_curve_nr(\n",
    "                ax, predictions, y_test[group], sensitive_value=i\n",
    "            )\n",
    "            ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "            ax.set_xlabel(\"False Negative Rate\", fontsize=24)\n",
    "            ax.set_ylabel(\"True Negative Rate\", fontsize=24)\n",
    "            ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
