{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "from utils.load_folktables import prepare_folktables_multattr\n",
    "from humancompatible.train.fairness.constraints.constraint_fns import *\n",
    "from fairret.statistic import *\n",
    "from utils.network import SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents some useful plots based on the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparation**\n",
    "\n",
    "**Load the Folktables dataset for the selected state and prepare it for usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"income\"\n",
    "STATE = \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_cols=[\n",
    "    \"MAR\",\n",
    "    # \"SEX\",\n",
    "    # 'RAC1P',\n",
    "    ]\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    group_ind_train,\n",
    "    group_onehot_train,\n",
    "    sep_group_ind_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    group_ind_test,\n",
    "    group_onehot_test,\n",
    "    sep_group_ind_test,\n",
    "    group_order\n",
    ") = prepare_folktables_multattr(\n",
    "    TASK,\n",
    "    state=STATE.upper(),\n",
    "    random_state=42,\n",
    "    onehot=False,\n",
    "    download=True,\n",
    "    sens_cols=sens_cols,\n",
    "    binarize=[None],\n",
    "    stratify=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_onehot_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_codes = {\n",
    "    \"MAR\": {0: \"OTHER\", 1: \"Mar\", 2: \"Wid\", 3: \"Div\", 4: \"Sep\", 5:\"Nev\"},\n",
    "    \"SEX\": {0: \"OTHER\", 1: \"M\", 2: \"F\"},\n",
    "    \"RAC1P\": {0: \"OTHER\", 1: \"W\", 2: \"B\", 3: \"AI\", 4: \"AN\", 5: \"AIAN\", 6: \"A\", 7: \"PA\", 8: \"OT\", 9: \"TW\"}\n",
    "}\n",
    "groups_sep = [[int(g) for g in gr.split('_')] for gr in group_order]\n",
    "group_names = [\n",
    "    [\n",
    "        group_codes[sens_cols[i]][c]\n",
    "        for i, c in enumerate(gc)\n",
    "    ]\n",
    "    for gc in groups_sep]\n",
    "group_names = ['_'.join(g) for g in group_names]\n",
    "group_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() and False else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = tensor(y_train, dtype=torch.float, device=device)\n",
    "\n",
    "X_test_tensor = tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = tensor(y_test, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_tensor) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load saved models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "constraints = {\n",
    "    # \"loss_equality\": 0.005,\n",
    "    # \"unconstrained\": 0.005,\n",
    "    # \"unconstrained\": 0.03,\n",
    "    \"abs_diff_pr\": 0.05,\n",
    "}\n",
    "\n",
    "dict_alg_names = {\n",
    "    \"StochasticGhost\": \"Ghost\",\n",
    "    \"SSLALM\": \"SSLALM\",\n",
    "    \"SSG\": \"SSw\",\n",
    "    # \"SGD\": \"SGD\",\n",
    "    \"Adam\": \"Adam\",\n",
    "    \"fairret\": \"SGD-Fairret\",\n",
    "    \"TorchSSLALM\": \"SSLALM\",\n",
    "    \"TorchSSG\": \"SSG\"\n",
    "}\n",
    "\n",
    "DATASET = TASK + \"_\" + STATE\n",
    "loaded_models = []\n",
    "\n",
    "for constr, cb in constraints.items():\n",
    "    DIRECTORY_PATH = (\n",
    "        \"./utils/saved_models/\" + DATASET + \"/\" + constr + \"/\" + ((f\"{cb:.0E}\" + \"/\") if cb is not None else '')\n",
    "    )\n",
    "    FILE_EXT = \".pt\"\n",
    "\n",
    "    directory_path = DIRECTORY_PATH\n",
    "    print(f\"Looking for models in: {directory_path}\")\n",
    "    try:\n",
    "        file_list = os.listdir(directory_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Not found\")\n",
    "        continue\n",
    "    model_files = [file for file in file_list if file.endswith(FILE_EXT)]\n",
    "    for model_file in model_files:\n",
    "        if model_file.split(\"_\")[0] not in dict_alg_names.keys():\n",
    "            continue\n",
    "        model_name = model_file\n",
    "        model = SimpleNet(X_test.shape[1], 1, torch.float32).to(device)\n",
    "        print(model_file)\n",
    "        try:\n",
    "            model.load_state_dict(\n",
    "                torch.load(\n",
    "                    directory_path + model_name, weights_only=True, map_location=device\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "        loaded_models.append((model_file, model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate test set statistics for the models - AUC, constraint satisfaction, loss, etc.. and aggregate per algorithm:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stats import make_pairwise_constraint_stats_table, aggregate_model_stats_table, make_groupwise_stats_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_stats = make_groupwise_stats_table(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    loaded_models\n",
    "    ).drop('Model',axis=1).groupby('Algorithm').agg('mean')\n",
    "\n",
    "groupwise_stats = []\n",
    "\n",
    "for group_ind in group_ind_train:\n",
    "    groupwise_stats.append(\n",
    "        make_groupwise_stats_table(\n",
    "            X_train_tensor[group_ind],\n",
    "            y_train_tensor[group_ind],\n",
    "            loaded_models\n",
    "        ).drop('Model',axis=1).groupby('Algorithm').agg('mean')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupwise_dev = []\n",
    "\n",
    "for group_stats in groupwise_stats:\n",
    "    diff = group_stats - full_data_stats\n",
    "    diff = diff.add_suffix('_dev')\n",
    "    diff['Sp'] = abs(diff['tpr_dev']) + abs(diff['fpr_dev'])\n",
    "    diff['Ind'] = abs(diff['ppv_dev']) + abs(diff['fomr_dev'])\n",
    "    diff['Sf'] = abs(diff['pr_dev'])\n",
    "    diff['Ina'] = 1 - group_stats['acc']\n",
    "    groupwise_dev.append(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stats = pd.concat(groupwise_stats, keys=group_names, names=['group'])\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "con = pd.concat(groupwise_dev, keys=group_names, names=['group'])\n",
    "con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "bin_dfs = []\n",
    "\n",
    "for group_idx_1, group_idx_2 in list(combinations(group_ind_train, 2)):\n",
    "    X_train_1, y_train_1 = X_train_tensor[group_idx_1], y_train_tensor[group_idx_1]\n",
    "    X_train_2, y_train_2 = X_train_tensor[group_idx_2], y_train_tensor[group_idx_2]\n",
    "    table = make_pairwise_constraint_stats_table(\n",
    "        X_train_1, y_train_1, X_train_2, y_train_2, loaded_models\n",
    "    )\n",
    "    table.index = table.Algorithm.apply(lambda x: dict_alg_names[x.split(\"_\")[0]])\n",
    "    table.drop(\"Algorithm\", axis=1, inplace=True)\n",
    "    bin_dfs.append(table)\n",
    "    \n",
    "df_train = pd.concat(bin_dfs, axis=0, keys=range(len(bin_dfs)), names=[\"constraint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = aggregate_model_stats_table(\n",
    "    df_train, \"mean\", agg_cols=[\"constraint\", \"Algorithm\"]\n",
    ")\n",
    "train_df_std = aggregate_model_stats_table(\n",
    "    df_train, [\"mean\", \"std\"], agg_cols=[\"constraint\", \"Algorithm\"]\n",
    ")\n",
    "train_df_std.drop(\"Algname\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import spider_line\n",
    "cr = con.reset_index()\n",
    "cr_alg = cr[cr['Algorithm'] == 'TorchSSLALM_0.05']\n",
    "cr_alg.index = cr_alg.group\n",
    "\n",
    "f = spider_line(cr_alg, yticks=[0,0.1,0.2,0.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of predictions by group:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_by_alg = {alg: {} for alg in set([model_name.split(\"_\")[0] for model_name, _ in loaded_models])}\n",
    "\n",
    "\n",
    "for i, group in enumerate(group_ind_test):\n",
    "    for model_name, model in loaded_models:\n",
    "        alg = model_name.split(\"_\")[0]\n",
    "\n",
    "        preds = torch.nn.functional.sigmoid(model(X_test_tensor[group])).detach().numpy().squeeze()\n",
    "        try:\n",
    "            predictions_by_alg[alg][i].append(preds)\n",
    "        except:\n",
    "            predictions_by_alg[alg][i] = [preds]\n",
    "\n",
    "for alg in predictions_by_alg.keys():\n",
    "    for i in predictions_by_alg[alg].keys():\n",
    "        predictions_by_alg[alg][i] = np.concatenate(predictions_by_alg[alg][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = {}\n",
    "\n",
    "for alg, pred_dict in predictions_by_alg.items():\n",
    "    preds = []\n",
    "    groups = []\n",
    "    for group, group_preds in pred_dict.items():\n",
    "        preds.extend(group_preds)\n",
    "        groups.extend([group]*len(group_preds))\n",
    "    \n",
    "    pred_dfs[alg] = (\n",
    "        pd.DataFrame({'pred': preds, 'group': groups})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "for i, (alg, predictions) in enumerate(pred_dfs.items()):\n",
    "    ax = axs[i]\n",
    "    predictions.group = predictions.group.apply(lambda x: group_names[x])\n",
    "    sns.kdeplot(\n",
    "        predictions,\n",
    "        x='pred',\n",
    "        hue='group',\n",
    "        palette=sns.color_palette(\"husl\", 5),\n",
    "        fill=True,\n",
    "        alpha=0.1,\n",
    "        bw_adjust=0.4,\n",
    "        ax=ax,\n",
    "        clip=[0,1],\n",
    "        common_norm=False)\n",
    "    ax.vlines(0.5,0.,10, ls='--',color='black')\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_xlabel(\"Predictions\", fontsize=20)\n",
    "    ax.set_ylabel(\"Density\", fontsize=20)\n",
    "    ax.set_title(alg)\n",
    "\n",
    "fig.set_figwidth(30)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We choose one model per algorithm to make some useful plots**\n",
    "\n",
    "For now, choose the model with the highest mean AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.index.get_level_values('Algorithm').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by = \"AUC_M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "algs = df_train.index.get_level_values('Algorithm').unique()\n",
    "for alg in algs:\n",
    "    alg_df = df_train.xs(alg, level=1).reset_index()\n",
    "    best_model_name = alg_df[['Model', select_by]].groupby('Model').mean()[select_by].idxmax()\n",
    "    model = [(name, model) for name, model in loaded_models if name == best_model_name][0]\n",
    "    best_models[alg] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subgroup ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPR-FPR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_pr(ax, predictions, targets, sensitive_value):\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f\"group={sensitive_value}, AUC = {roc_auc:.2f}\")\n",
    "    tpr_minus_fpr = tpr - fpr\n",
    "    # Find the threshold that maximizes TPR - FPR difference\n",
    "    optimal_threshold_index = np.argmax(tpr_minus_fpr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(\n",
    "        fpr[optimal_threshold_index],\n",
    "        tpr[optimal_threshold_index],\n",
    "        # c=\"blue\" if sensitive_value == sensitive_value_0 else \"red\",\n",
    "        label=f\"Optimal Threshold {sensitive_value} {optimal_threshold:.2f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(10)\n",
    "    f.set_figheight(10)\n",
    "    ax = f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        for i,group in enumerate(group_ind_test):\n",
    "            predictions = model(X_test_tensor[group])\n",
    "            # Plot ROC for sensitive attribute A=0\n",
    "            plot_roc_curve_pr(\n",
    "                ax, predictions, y_test[group], sensitive_value=i\n",
    "            )\n",
    "            ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "            ax.set_xlabel(\"False Positive Rate\", fontsize=24)\n",
    "            ax.set_ylabel(\"True Positive Rate\", fontsize=24)\n",
    "            ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TNR-FNR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_nr(ax, predictions, targets, sensitive_value):\n",
    "    # Convert PyTorch tensors to numpy arrays\n",
    "    # predictions = predictions.detach().numpy()\n",
    "    # targets = targets.numpy()\n",
    "\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    fnr = 1 - tpr\n",
    "    tnr = 1 - fpr\n",
    "    roc_auc = auc(tnr, fnr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(tnr, fnr, label=f\"Sensitive={sensitive_value}, AUC = {roc_auc:.2f}\")\n",
    "\n",
    "    tnr_minus_fnr = tnr - fnr\n",
    "\n",
    "    # Find the threshold that maximizes tnr - fnr difference\n",
    "    optimal_threshold_index = np.argmax(tnr_minus_fnr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(\n",
    "        fnr[optimal_threshold_index],\n",
    "        tpr[optimal_threshold_index],\n",
    "        # c=\"blue\" if sensitive_value == sensitive_value_0 else \"red\",\n",
    "        label=f\"Optimal Threshold {sensitive_value} {optimal_threshold:.2f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(10)\n",
    "    f.set_figheight(10)\n",
    "    ax = f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        for i,group in enumerate(group_ind_test):\n",
    "            predictions = model(X_test_tensor[group])\n",
    "            # Plot ROC for sensitive attribute A=0\n",
    "            plot_roc_curve_nr(\n",
    "                ax, predictions, y_test[group], sensitive_value=i\n",
    "            )\n",
    "            ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "            ax.set_xlabel(\"False Negative Rate\", fontsize=24)\n",
    "            ax.set_ylabel(\"True Negative Rate\", fontsize=24)\n",
    "            ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
