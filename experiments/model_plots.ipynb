{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "from utils.load_folktables import prepare_folktables\n",
    "from src.constraints.constraint_fns import *\n",
    "from fairret.statistic import *\n",
    "from utils.network import SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents some useful plots based on the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparation**\n",
    "\n",
    "**Load the Folktables dataset for the selected state and prepare it for usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"income\"\n",
    "# TASK = 'employment'\n",
    "STATE = \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    [w_idx_train, nw_idx_train],\n",
    "    X_test,\n",
    "    y_test,\n",
    "    [w_idx_test, nw_idx_test],\n",
    ") = prepare_folktables(\n",
    "    TASK,\n",
    "    state=STATE,\n",
    "    random_state=42,\n",
    "    make_unbalanced=False,\n",
    "    onehot=False,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "sensitive_value_0 = \"white\"\n",
    "sensitive_value_1 = \"non-white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() and False else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = tensor(y_train, dtype=torch.float, device=device)\n",
    "\n",
    "X_test_tensor = tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = tensor(y_test, dtype=torch.float, device=device)\n",
    "\n",
    "X_train_w = X_train_tensor[w_idx_train]\n",
    "y_train_w = y_train_tensor[w_idx_train]\n",
    "X_train_nw = X_train_tensor[nw_idx_train]\n",
    "y_train_nw = y_train_tensor[nw_idx_train]\n",
    "\n",
    "X_test_w = X_test_tensor[w_idx_test]\n",
    "y_test_w = y_test_tensor[w_idx_test]\n",
    "X_test_nw = X_test_tensor[nw_idx_test]\n",
    "y_test_nw = y_test_tensor[nw_idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"w, nw, total\")\n",
    "print(\"train\")\n",
    "print(len(y_train_w), len(y_train_nw), len(y_train))\n",
    "print(\n",
    "    sum(y_train_w == 1) / len(y_train_w),\n",
    "    sum(y_train_nw == 1) / len(y_train_nw),\n",
    "    sum(y_train_tensor == 1) / len(y_train_tensor),\n",
    ")\n",
    "print(\"test\")\n",
    "print(len(y_test_w), len(y_test_nw), len(y_test))\n",
    "print(\n",
    "    sum(y_test_w == 1) / len(y_test_w),\n",
    "    sum(y_test_nw == 1) / len(y_test_nw),\n",
    "    sum(y_test_tensor == 1) / len(y_test_tensor),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load saved models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to load models from\n",
    "\n",
    "LOSS_BOUND = 0.005\n",
    "DATASET = TASK + \"_\" + STATE\n",
    "constraint = \"eq_loss\"\n",
    "DIRECTORY_PATH = (\n",
    "    \"./utils/saved_models/\"\n",
    "    + DATASET\n",
    "    + \"/\"\n",
    "    + constraint\n",
    "    + \"/\"\n",
    "    + f\"{LOSS_BOUND:.0e}\"\n",
    "    + \"/\"\n",
    ")\n",
    "FILE_EXT = \".pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = []\n",
    "directory_path = DIRECTORY_PATH\n",
    "file_list = os.listdir(directory_path)\n",
    "model_files = [file for file in file_list if file.endswith(FILE_EXT)]\n",
    "for model_file in model_files:\n",
    "    model_name = model_file\n",
    "    model = SimpleNet(X_test.shape[1], 1, torch.float32).to(device)\n",
    "    print(model_file, end=\"\\r\")\n",
    "    try:\n",
    "        model.load_state_dict(\n",
    "            torch.load(\n",
    "                directory_path + model_name, weights_only=False, map_location=device\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        continue\n",
    "    model_file = str.join(\"\", model_file.split(\"_trial\")[:-1])\n",
    "    loaded_models.append((model_file, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate test set statistics for the models - AUC, constraint satisfaction, loss, etc.. and aggregate per algorithm:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stats import make_model_stats_table\n",
    "from utils.stats import aggregate_model_stats_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_train = make_model_stats_table(\n",
    "    X_train_w, y_train_w, X_train_nw, y_train_nw, loaded_models\n",
    ")\n",
    "\n",
    "train_df = aggregate_model_stats_table(res_df_train, \"mean\")\n",
    "train_df_std = aggregate_model_stats_table(res_df_train, [\"mean\", \"std\"])\n",
    "train_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_test = make_model_stats_table(\n",
    "    X_test_w, y_test_w, X_test_nw, y_test_nw, loaded_models\n",
    ")\n",
    "\n",
    "test_df = aggregate_model_stats_table(res_df_test, \"mean\")\n",
    "test_df_std = aggregate_model_stats_table(res_df_test, [\"mean\", \"std\"])\n",
    "test_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in test_df.index:\n",
    "    alg_name = (\n",
    "        \"sslalm_aug\"\n",
    "        if model_name.startswith(\"sslalm_mu0\")\n",
    "        else model_name.split(\"_\")[0]\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(f\"./plots/{alg_name}/{DATASET}/\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import spider_line\n",
    "\n",
    "\n",
    "f = spider_line(train_df)\n",
    "f = spider_line(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of predictions by group:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_0 = {}\n",
    "predictions_1 = {}\n",
    "\n",
    "for model_name, model in loaded_models:\n",
    "    preds_0 = torch.nn.functional.sigmoid(model(X_test_w)).detach().numpy()\n",
    "    preds_1 = torch.nn.functional.sigmoid(model(X_test_nw)).detach().numpy()\n",
    "    try:\n",
    "        predictions_0[model_name].append(preds_0)\n",
    "        predictions_1[model_name].append(preds_1)\n",
    "    except:\n",
    "        predictions_0[model_name] = [preds_0]\n",
    "        predictions_1[model_name] = [preds_1]\n",
    "\n",
    "for name in np.unique([name for name, _ in loaded_models]):\n",
    "    predictions_0[name] = np.concatenate(predictions_0[name])\n",
    "    predictions_1[name] = np.concatenate(predictions_1[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for model_name in np.unique([name for name, _ in loaded_models]):\n",
    "    # predictions_0 = torch.nn.functional.sigmoid(model(X_test_w)).detach().numpy()\n",
    "    # predictions_1 = torch.nn.functional.sigmoid(model(X_test_nw)).detach().numpy()\n",
    "\n",
    "    sns.kdeplot(\n",
    "        predictions_0[model_name].squeeze(),\n",
    "        label=sensitive_value_0,\n",
    "        color=\"blue\",\n",
    "        fill=True,\n",
    "        bw_adjust=0.4,\n",
    "    )  # ,clip=[0,1],common_norm=True)\n",
    "    sns.kdeplot(\n",
    "        predictions_1[model_name].squeeze(),\n",
    "        label=sensitive_value_1,\n",
    "        color=\"red\",\n",
    "        fill=True,\n",
    "        bw_adjust=0.4,\n",
    "    )  # ,clip=[0,1],common_norm=True)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(0, 22)\n",
    "    plt.xlabel(\"Predictions\", fontsize=20)\n",
    "    plt.ylabel(\"Density\", fontsize=20)\n",
    "    # plt.title(model_name, fontsize=10)\n",
    "    # plt.title(alg)\n",
    "    # print(alg)\n",
    "    alg_name = (\n",
    "        \"sslalm_aug\"\n",
    "        if model_name.startswith(\"sslalm_mu0\")\n",
    "        else model_name.split(\"_\")[0]\n",
    "    )\n",
    "    plt.savefig(f\"./plots/{alg_name}/{DATASET}/dist\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We choose one model per algorithm to make some useful plots**\n",
    "\n",
    "For now, choose the model with the highest mean AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by = \"auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "algs = res_df_test.Algorithm.unique()\n",
    "for alg in algs:\n",
    "    alg_df = res_df_test[res_df_test.Algorithm == alg]\n",
    "    if select_by == \"auc\":\n",
    "        model = loaded_models[alg_df.AUC_M.idxmax()]\n",
    "    elif select_by == \"wd\":\n",
    "        model = loaded_models[alg_df.Wd.idxmin()]\n",
    "    best_models[alg] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subgroup ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPR-FPR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_pr(ax, predictions, targets, sensitive_value):\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f\"Sensitive={sensitive_value}, AUC = {roc_auc:.2f}\")\n",
    "    tpr_minus_fpr = tpr - fpr\n",
    "    # Find the threshold that maximizes TPR - FPR difference\n",
    "    optimal_threshold_index = np.argmax(tpr_minus_fpr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(\n",
    "        fpr[optimal_threshold_index],\n",
    "        tpr[optimal_threshold_index],\n",
    "        c=\"blue\" if sensitive_value == sensitive_value_0 else \"red\",\n",
    "        label=f\"Optimal Threshold {sensitive_value} {optimal_threshold:.2f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    ax = f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        predictions_0 = model(X_test_w)\n",
    "        predictions_1 = model(X_test_nw)\n",
    "        # Plot ROC for sensitive attribute A=0\n",
    "        plot_roc_curve_pr(\n",
    "            ax, predictions_0, y_test_w, sensitive_value=sensitive_value_0\n",
    "        )\n",
    "        # Plot ROC for sensitive attribute A=1\n",
    "        plot_roc_curve_pr(\n",
    "            ax, predictions_1, y_test_nw, sensitive_value=sensitive_value_1\n",
    "        )\n",
    "        ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "        ax.set_xlabel(\"False Positive Rate\", fontsize=24)\n",
    "        ax.set_ylabel(\"True Positive Rate\", fontsize=24)\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TNR-FNR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_nr(ax, predictions, targets, sensitive_value):\n",
    "    # Convert PyTorch tensors to numpy arrays\n",
    "    # predictions = predictions.detach().numpy()\n",
    "    # targets = targets.numpy()\n",
    "\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    fnr = 1 - tpr\n",
    "    tnr = 1 - fpr\n",
    "    roc_auc = auc(tnr, fnr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(tnr, fnr, label=f\"Sensitive={sensitive_value}, AUC = {roc_auc:.2f}\")\n",
    "\n",
    "    tnr_minus_fnr = tnr - fnr\n",
    "\n",
    "    # Find the threshold that maximizes TPR - FPR difference\n",
    "    optimal_threshold_index = np.argmax(tnr_minus_fnr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(\n",
    "        tnr[optimal_threshold_index],\n",
    "        fnr[optimal_threshold_index],\n",
    "        c=\"blue\" if sensitive_value == sensitive_value_0 else \"red\",\n",
    "        label=f\"Optimal Threshold {sensitive_value} {optimal_threshold:.2f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    ax = f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        predictions_0 = model(X_test_w)\n",
    "        predictions_1 = model(X_test_nw)\n",
    "        # Plot ROC for sensitive attribute A=0\n",
    "        plot_roc_curve_nr(\n",
    "            ax, predictions_0, y_test_w, sensitive_value=sensitive_value_0\n",
    "        )\n",
    "        # Plot ROC for sensitive attribute A=1\n",
    "        plot_roc_curve_nr(\n",
    "            ax, predictions_1, y_test_nw, sensitive_value=sensitive_value_1\n",
    "        )\n",
    "        ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "        ax.set_xlabel(\"False Negative Rate\", fontsize=24)\n",
    "        ax.set_ylabel(\"True Negative Rate\", fontsize=24)\n",
    "        ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humancompatible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
