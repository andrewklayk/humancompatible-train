{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "from utils.load_folktables import prepare_folktables_multattr\n",
    "from humancompatible.train.benchmark.constraints.constraint_fns import *\n",
    "from fairret.statistic import *\n",
    "from utils.network import SimpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.io.arff import loadarff\n",
    "\n",
    "# raw_data = loadarff('utils/raw_data/dutch_census_2001.arff')\n",
    "# df_data = pd.DataFrame(raw_data[0])\n",
    "# df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents some useful plots based on the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparation**\n",
    "\n",
    "**Load the Folktables dataset for the selected state and prepare it for usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"income\"\n",
    "STATE = \"VA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_cols=[\n",
    "    \"MAR\",\n",
    "    # \"SEX\",\n",
    "    #'RAC1P',\n",
    "    ]\n",
    "\n",
    "(\n",
    "    (X_train, X_val, X_test),\n",
    "    (y_train, y_val, y_test),\n",
    "    (group_ind_train, group_ind_val, group_ind_test),\n",
    "    (group_onehot_train, group_onehot_val, group_onehot_test),\n",
    "    group_order\n",
    ") = prepare_folktables_multattr(\n",
    "    TASK,\n",
    "    state=STATE.upper(),\n",
    "    random_state=42,\n",
    "    onehot=False,\n",
    "    download=False,\n",
    "    sens_cols=sens_cols,\n",
    "    # binarize=[None],\n",
    "    stratify=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_onehot_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_codes = {\n",
    "    \"MAR\": {0: \"OTHER\", 1: \"Mar\", 2: \"Wid\", 3: \"Div\", 4: \"Sep\", 5:\"Nev\"},\n",
    "    \"SEX\": {0: \"OTHER\", 1: \"M\", 2: \"F\"},\n",
    "    \"RAC1P\": {0: \"OTHER\", 1: \"W\", 2: \"B\", 3: \"AI\", 4: \"AN\", 5: \"AIAN\", 6: \"A\", 7: \"PA\", 8: \"OT\", 9: \"TW\"}\n",
    "}\n",
    "groups_sep = [[int(g) for g in gr.split('_')] for gr in group_order]\n",
    "group_names = [\n",
    "    [\n",
    "        group_codes[sens_cols[i]][c]\n",
    "        for i, c in enumerate(gc)\n",
    "    ]\n",
    "    for gc in groups_sep]\n",
    "group_names = ['_'.join(g) for g in group_names]\n",
    "group_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() and False else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = tensor(y_train, dtype=torch.float, device=device)\n",
    "\n",
    "X_test_tensor = tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = tensor(y_test, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_tensor) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load saved models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "constraints = {\n",
    "    \"abs_loss_equality\": 0.05,\n",
    "    # \"unconstrained\": 0.005,\n",
    "    # \"unconstrained\": 0.03,\n",
    "    # \"abs_diff_pr\": 0.05,\n",
    "}\n",
    "\n",
    "dict_alg_names = {\n",
    "    \"SGD\": \"SGD\",\n",
    "    \"SGD+Reg\": \"SGD+Reg\",\n",
    "    \"SSG\": \"SSw\",\n",
    "    \"SSLALM\": \"SSLALM\",\n",
    "    \"StochasticGhost\": \"Ghost\",\n",
    "    # \"Adam\": \"Adam\",\n",
    "    # \"fairret\": \"SGD-Fairret\",\n",
    "    # \"TorchSSLALM\": \"SSLALM\",\n",
    "    # \"TorchSSG\": \"SSG\"\n",
    "}\n",
    "\n",
    "DATASET = TASK + \"_\" + STATE\n",
    "loaded_models = []\n",
    "\n",
    "for constr, cb in constraints.items():\n",
    "    DIRECTORY_PATH = (\n",
    "        \"./utils/saved_models/\" + DATASET + \"/\" + constr + \"/\" + ((f\"{cb:.0E}\" + \"/\") if cb is not None else '')\n",
    "    )\n",
    "    FILE_EXT = \".pt\"\n",
    "\n",
    "    directory_path = DIRECTORY_PATH\n",
    "    print(f\"Looking for models in: {directory_path}\")\n",
    "    try:\n",
    "        file_list = os.listdir(directory_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Not found\")\n",
    "        continue\n",
    "    model_files = [file for file in file_list if file.endswith(FILE_EXT)]\n",
    "    for model_file in model_files:\n",
    "        if model_file.split(\"_\")[0] not in dict_alg_names.keys():\n",
    "            continue\n",
    "        model_name = model_file\n",
    "        model = SimpleNet(X_test.shape[1], 1, torch.float32).to(device)\n",
    "        print(model_file)\n",
    "        try:\n",
    "            model.load_state_dict(\n",
    "                torch.load(\n",
    "                    directory_path + model_name, weights_only=True, map_location=device\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "        loaded_models.append((model_file, model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate test set statistics for the models - AUC, constraint satisfaction, loss, etc.. and aggregate per algorithm:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stats import make_pairwise_constraint_stats_table, aggregate_model_stats_table, make_groupwise_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_tensor\n",
    "y = y_test_tensor\n",
    "g = group_ind_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_statistics = make_groupwise_stats_table(\n",
    "    X,\n",
    "    y,\n",
    "    loaded_models\n",
    "    ).drop('Model',axis=1).groupby('Algorithm').agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupwise statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data_stats = make_groupwise_stats_table(\n",
    "#     X_train_tensor,\n",
    "#     y_train_tensor,\n",
    "#     loaded_models\n",
    "#     ).drop('Model',axis=1).groupby('Algorithm').agg('mean')\n",
    "\n",
    "# X = X_train_tensor\n",
    "# y = y_train_tensor\n",
    "# g = group_ind_train\n",
    "\n",
    "groupwise_stats = []\n",
    "groupwise_stats_inv = []\n",
    "\n",
    "for group_ind in g:\n",
    "    groupwise_stats.append(\n",
    "        make_groupwise_stats_table(\n",
    "            X[group_ind],\n",
    "            y[group_ind],\n",
    "            loaded_models\n",
    "        ).drop('Model',axis=1).groupby('Algorithm').agg('mean')\n",
    "    )\n",
    "    except_group_ind = np.delete(np.arange(len(X)), group_ind, 0)\n",
    "    groupwise_stats_inv.append(\n",
    "        make_groupwise_stats_table(\n",
    "            X[except_group_ind],\n",
    "            y[except_group_ind],\n",
    "            loaded_models\n",
    "        ).drop('Model',axis=1).groupby('Algorithm').agg('mean')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stats = pd.concat(groupwise_stats, keys=group_names, names=['group'])\n",
    "inv_stats = pd.concat(groupwise_stats_inv, keys=group_names, names=['group'])\n",
    "sep = {}\n",
    "sep_total = 0\n",
    "ind = {}\n",
    "ind_total = 0\n",
    "sf = {}\n",
    "sf_total = 0\n",
    "\n",
    "gn = [g for g in group_names]\n",
    "\n",
    "for group_idx, group in enumerate(gn):\n",
    "    sep[group] = 0\n",
    "    ind[group] = 0\n",
    "    sf[group] = 0\n",
    "    for alt_group in [g for g in gn if g != group]:\n",
    "        sep[group] += abs(stats.loc[group]['tpr'] - stats.loc[alt_group]['tpr']) + abs(stats.loc[group]['fpr'] - stats.loc[alt_group]['fpr'])\n",
    "        ind[group] += abs(stats.loc[group]['pr'] - stats.loc[alt_group]['pr'])\n",
    "        sf[group] += abs(stats.loc[group]['ppv'] - stats.loc[alt_group]['ppv']) + abs(stats.loc[group]['fomr'] - stats.loc[alt_group]['fomr'])\n",
    "    sep_total += sep[group]\n",
    "    ind_total += ind[group]\n",
    "    sf_total += sf[group]\n",
    "\n",
    "sep = pd.concat(sep, keys=gn, names=['group'])\n",
    "ind = pd.concat(ind, keys=gn, names=['group'])\n",
    "sf = pd.concat(sf, keys=gn, names=['group'])\n",
    "\n",
    "stats['Sp'] = sep\n",
    "stats['Ind'] = ind\n",
    "stats['Sf'] = sf\n",
    "\n",
    "# stats['Sp']= abs(stats['tpr'] - inv_stats['tpr']) + abs(stats['fpr'] - inv_stats['fpr'])\n",
    "# stats['Ind'] = abs(stats['pr'] - inv_stats['pr'])\n",
    "# stats['Sf'] = abs(stats['ppv'] - inv_stats['ppv']) + abs(stats['fomr'] - inv_stats['fomr'])\n",
    "\n",
    "stats['Ina'] = 1 - stats['acc']\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import spider_line\n",
    "cr = stats.loc['Mar']\n",
    "# cr = sts\n",
    "# cr = stats.groupby('Algorithm').mean()\n",
    "# cr_alg = cr[cr['Algorithm'] == 'SGD_0.05']\n",
    "# cr_alg.index = cr_alg.group\n",
    "\n",
    "f = spider_line(cr, yticks=np.arange(0, 0.5, 0.1))\n",
    "f.set_figwidth(10)\n",
    "f.set_figheight(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of predictions by group:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X_train_tensor\n",
    "# y = y_train_tensor\n",
    "\n",
    "X = X_test_tensor\n",
    "y = y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_by_alg = {alg: {} for alg in set([model_name.split(\"_\")[0] for model_name, _ in loaded_models])}\n",
    "\n",
    "for i, group in enumerate(group_ind_test):\n",
    "    for model_name, model in loaded_models:\n",
    "        alg = model_name.split(\"_\")[0]\n",
    "\n",
    "        preds = torch.nn.functional.sigmoid(model(X[group])).detach().numpy().squeeze()\n",
    "        try:\n",
    "            predictions_by_alg[alg][i].append(preds)\n",
    "        except:\n",
    "            predictions_by_alg[alg][i] = [preds]\n",
    "\n",
    "for alg in predictions_by_alg.keys():\n",
    "    for i in predictions_by_alg[alg].keys():\n",
    "        predictions_by_alg[alg][i] = np.concatenate(predictions_by_alg[alg][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = {}\n",
    "\n",
    "for alg, pred_dict in predictions_by_alg.items():\n",
    "    preds = []\n",
    "    groups = []\n",
    "    for group, group_preds in pred_dict.items():\n",
    "        preds.extend(group_preds)\n",
    "        groups.extend([group]*len(group_preds))\n",
    "    \n",
    "    pred_dfs[alg] = (\n",
    "        pd.DataFrame({'pred': preds, 'group': groups})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5)\n",
    "\n",
    "for i, (alg, predictions) in enumerate(pred_dfs.items()):\n",
    "    ax = axs[i]\n",
    "    predictions.group = predictions.group.apply(lambda x: group_names[x])\n",
    "    sns.kdeplot(\n",
    "        predictions,\n",
    "        x='pred',\n",
    "        hue='group',\n",
    "        palette=sns.color_palette(\"husl\", 5),\n",
    "        fill=True,\n",
    "        alpha=0.1,\n",
    "        bw_adjust=0.4,\n",
    "        ax=ax,\n",
    "        clip=[0,1],\n",
    "        common_norm=False)\n",
    "    ax.vlines(0.5,0.,10, ls='--',color='black')\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_xlabel(\"Predictions\", fontsize=20)\n",
    "    ax.set_ylabel(\"Density\", fontsize=20)\n",
    "    ax.set_title(alg)\n",
    "\n",
    "fig.set_figwidth(30)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We choose one model per algorithm to make some useful plots**\n",
    "\n",
    "Choose the model with the highest mean AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve, accuracy_score\n",
    "\n",
    "# X = X_train_tensor\n",
    "# y = y_train_tensor\n",
    "\n",
    "X = X_test_tensor\n",
    "y = y_test_tensor\n",
    "\n",
    "alg_list = [str.join(\"\", model_name.split(\"_trial\")[:-1]) for model_name, _ in loaded_models]\n",
    "# model_df = pd.DataFrame(columns=['alg', 'auc', 'model'])\n",
    "model_df = []\n",
    "for model_name, model in loaded_models:\n",
    "    alg = str.join(\"\", model_name.split(\"_trial\")[:-1])\n",
    "    with torch.inference_mode():\n",
    "        preds = model(X)\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "        y.cpu().numpy(), preds.cpu().numpy()\n",
    "    )\n",
    "    model_auc = auc(fpr, tpr)\n",
    "    model_df.append({'alg': alg, 'auc': model_auc, 'model': model})\n",
    "\n",
    "model_df = pd.DataFrame(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_idx = model_df.groupby('alg')['auc'].idxmax()\n",
    "best_models = model_df.iloc[models_idx][['alg', 'model']]\n",
    "best_models.index = best_models['alg']\n",
    "best_models.drop('alg', inplace=True, axis=1)\n",
    "best_models = best_models['model'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subgroup ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPR-FPR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_pr(ax, predictions, targets, sensitive_value):\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f\"group={sensitive_value}, AUC = {roc_auc:.2f}\")\n",
    "    tpr_minus_fpr = tpr - fpr\n",
    "    # Find the threshold that maximizes TPR - FPR difference\n",
    "    optimal_threshold_index = np.argmax(tpr_minus_fpr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(\n",
    "        fpr[optimal_threshold_index],\n",
    "        tpr[optimal_threshold_index],\n",
    "        # c=\"blue\" if sensitive_value == sensitive_value_0 else \"red\",\n",
    "        label=f\"Optimal Threshold {sensitive_value} {optimal_threshold:.2f}\",\n",
    "    )\n",
    "\n",
    "\n",
    "for alg, model in best_models.items():\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(10)\n",
    "    f.set_figheight(10)\n",
    "    ax = f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        for i,group in enumerate(group_ind_test):\n",
    "            predictions = model(X_test_tensor[group])\n",
    "            # Plot ROC for sensitive attribute A=0\n",
    "            plot_roc_curve_pr(\n",
    "                ax, predictions, y_test[group], sensitive_value=i\n",
    "            )\n",
    "            ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")\n",
    "            ax.set_xlabel(\"False Positive Rate\", fontsize=24)\n",
    "            ax.set_ylabel(\"True Positive Rate\", fontsize=24)\n",
    "            ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
