{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.plotting import plot_time, plot_sep, plot_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6]})\n",
    "a = pd.DataFrame({'a': [7,8,9]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"income\"\n",
    "STATE = \"VA\"\n",
    "DATASET = TASK + \"_\" + STATE\n",
    "DATASET = TASK + \"_\" + STATE\n",
    "loaded_models = []\n",
    "\n",
    "experiments_to_read = {\n",
    "    'SGD': {\n",
    "        'abs_loss_equality': 0.05,\n",
    "        \"abs_diff_pr\": 0.05\n",
    "    },\n",
    "\n",
    "\n",
    "    'SGD+Reg': {\n",
    "        'abs_loss_equality': 0.05,\n",
    "        \"abs_diff_pr\": 0.05\n",
    "    },\n",
    "    # 'sgd_0.': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sgd_0.3': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sgd_0.4': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sgd_0.5': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sgd_1.0': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "\n",
    "    # 'sslalm_005005': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sslalm_005001': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sslalm_001005': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sslalm_001001': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "\n",
    "    # 'ssg_start_cc': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cc10': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cc01': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cc11': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_dd10': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cd00': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cd01': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cd11': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cd12': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    # 'ssg_start_cd15': {\n",
    "    #     \"abs_loss_equality\": 0.05,\n",
    "    # },\n",
    "    \n",
    "    'SSG': {\n",
    "        \"abs_loss_equality\": 0.05,\n",
    "        # \"loss_equality\": 0.05,\n",
    "        \"abs_diff_pr\": 0.05\n",
    "    },\n",
    "    'SSLALM': {\n",
    "        \"abs_loss_equality\": 0.05,\n",
    "        # \"loss_equality\": 0.05,\n",
    "        \"abs_diff_pr\": 0.05\n",
    "    },\n",
    "    'StochasticGhost': {\n",
    "        \"abs_loss_equality\": 0.05,\n",
    "        # \"loss_equality\": 0.05,\n",
    "        \"abs_diff_pr\": 0.05\n",
    "    },\n",
    "    # 'ghost_z005_g005_a03_b10': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z001_g001_a03_b10': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z001_g001_a03_b20': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z001_g001_a03_b1': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z001_g001_a03_b1_r05': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z001_g0005_a03_b1_r01': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z001_g0005_a03_b1_r08': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z005_g0005_a03_b1_r01': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'ghost_z005_g0005_a03_b1_r08': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    'ghost_210': {\n",
    "        'abs_loss_equality': 0.05,\n",
    "        \"abs_diff_pr\": 0.05\n",
    "    },\n",
    "    # 'sgd_0.3': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sgd_0.4': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sgd_0.5': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "    # 'sgd_1.0': {\n",
    "    #     'abs_loss_equality': 0.05,\n",
    "    #     \"abs_diff_pr\": 0.05\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "path_to_experiments = '.'\n",
    "experiments_to_read = ['CURRENT_DATE_TIME']\n",
    "\n",
    "alg_states = {}\n",
    "full_eval_train = {}\n",
    "full_eval_val = {}\n",
    "full_eval_test = {}\n",
    "\n",
    "for exp_name in experiments_to_read:\n",
    "    exp_path = os.path.join(path_to_experiments, exp_name)\n",
    "    alg_states[exp_name] = {}\n",
    "    full_eval_train[exp_name] = {}\n",
    "    full_eval_val[exp_name] = {}\n",
    "    full_eval_test[exp_name] = {}\n",
    "    # load config for the current experiment (get algorithm name, constraints name, etc)\n",
    "    with open(os.path.join(exp_path, 'config.yaml')) as stream:\n",
    "        exp_config = yaml.safe_load(stream)\n",
    "    for run_dir in os.listdir(exp_path):\n",
    "        run_dir_path = os.path.join(exp_name, run_dir)\n",
    "        if not os.path.isdir(run_dir_path):\n",
    "            continue\n",
    "        alg_states_path = os.path.join(run_dir_path, 'alg_states.csv')\n",
    "        alg_states[exp_name][int(run_dir)] = pd.read_csv(alg_states_path, index_col=[0])#.reset_index(names='iteration')\n",
    "\n",
    "        summary_train_path = os.path.join(run_dir_path, 'full_set_eval_train.csv')\n",
    "        full_eval_train[exp_name][int(run_dir)] = pd.read_pickle(summary_train_path)\n",
    "        summary_val_path = os.path.join(run_dir_path, 'full_set_eval_val.csv')\n",
    "        full_eval_val[exp_name][int(run_dir)] = pd.read_pickle(summary_val_path)\n",
    "        summary_test_path = os.path.join(run_dir_path, 'full_set_eval_test.csv')\n",
    "        full_eval_test[exp_name][int(run_dir)] = pd.read_pickle(summary_test_path)\n",
    "    \n",
    "    alg_states[exp_name] = pd.concat(alg_states[exp_name]).reset_index(names=['trial', 'iteration'])\n",
    "    full_eval_train[exp_name] = pd.concat(full_eval_train[exp_name]).reset_index(names=['trial', 'iteration'])\n",
    "    full_eval_val[exp_name] = pd.concat(full_eval_val[exp_name]).reset_index(names=['trial', 'iteration'])\n",
    "    full_eval_test[exp_name] = pd.concat(full_eval_test[exp_name]).reset_index(names=['trial', 'iteration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def explode_column(df, col, names, m):\n",
    "    df[names] = list(\n",
    "        df[col].map(\n",
    "                    lambda x: [np.nan]*m if np.all(np.isnan(x)) else x \n",
    "                    )\n",
    "        )\n",
    "\n",
    "def aggregate_stats(live_stats, after_stats, constraint_bound):\n",
    "    \"\"\"\n",
    "    live_stats: states of the algorithm evaluated during run\n",
    "    after_stats: statistics evaluated on full set after run\n",
    "    \"\"\"\n",
    "    live_stats.columns = live_stats.columns.map(lambda x: str(x) + '_live' if x == 'c' else x)\n",
    "    after_stats.columns = after_stats.columns.map(lambda x: str(x) + '_full' if x == 'c' else x)\n",
    "    # combine into one dataframe\n",
    "    stats_joined = after_stats.set_index(['iteration', 'trial']).join(live_stats.set_index(['iteration', 'trial']), on=['iteration', 'trial'], how='inner',lsuffix='_full', rsuffix='_live')\n",
    "    for col in stats_joined.columns:\n",
    "        if col == \"time\":\n",
    "            continue\n",
    "        # change [list] into list\n",
    "        if isinstance(stats_joined[col].iloc[0], list):\n",
    "            stats_joined[col] = stats_joined[col].map(lambda x: x[0] if not np.all(np.isnan(x)) else x)\n",
    "        # \"explode\" ndarray column into separate columns\n",
    "        if isinstance(stats_joined[col].iloc[0], np.ndarray):\n",
    "            if len(stats_joined[col].iloc[0].shape) == 1 and col in ['c_live', 'c_full']:\n",
    "                m = len(stats_joined[col].iloc[0])\n",
    "            \n",
    "                explode_column(stats_joined, col, [f'{col}{i}' for i in range(m)], m)\n",
    "                for i in range(m):\n",
    "                    stats_joined[f'{col}{i}_corrected'] = stats_joined[f'{col}{i}'] + constraint_bound\n",
    "\n",
    "            # add norm for ndarray columns\n",
    "            stats_joined[f'{col}_norm'] = stats_joined.apply(lambda x: np.linalg.norm(x[col]),axis=1)\n",
    "\n",
    "    stats_joined.dropna(axis=0, how='all', inplace=True, subset=[x for x in stats_joined.columns if x not in ['cb', 'time', 'constraint_name']])\n",
    "\n",
    "    # convert one-element ndarrays into floats\n",
    "    for col in stats_joined.columns:\n",
    "        if isinstance(stats_joined[col].iloc[0], np.ndarray):\n",
    "            if stats_joined[col].iloc[0].ndim == 0 or stats_joined[col].iloc[0].ndim == 1 and stats_joined[col].iloc[0].shape[0] == 1:\n",
    "                stats_joined[col] = stats_joined[col].astype(float)\n",
    "\n",
    "    stats_joined.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    return stats_joined\n",
    "\n",
    "\n",
    "data_train = {}\n",
    "data_val = {}\n",
    "data_test = {}\n",
    "\n",
    "for name in full_eval_train.keys():\n",
    "    print(name)\n",
    "    with torch.no_grad():\n",
    "        data_train[name] = aggregate_stats(alg_states[name][['f', 'c', 'time','iteration','trial']], full_eval_train[name][['f', 'c','iteration','trial']], 0.05)\n",
    "        data_val[name] = aggregate_stats(alg_states[name][['f', 'c', 'time','iteration','trial']], full_eval_val[name][['f', 'c','iteration','trial']], 0.05)\n",
    "        data_test[name] = aggregate_stats(alg_states[name][['f', 'c', 'time','iteration','trial']], full_eval_test[name][['f', 'c','iteration','trial']], 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_codes = {\n",
    "    \"MAR\": {0: \"OTHER\", 1: \"Mar\", 2: \"Wid\", 3: \"Div\", 4: \"Sep\", 5:\"Nev\"},\n",
    "    \"SEX\": {0: \"OTHER\", 1: \"M\", 2: \"F\"},\n",
    "    \"RAC1P\": {0: \"OTHER\", 1: \"W\", 2: \"B\", 3: \"AI\", 4: \"AN\", 5: \"AIAN\", 6: \"A\", 7: \"PA\", 8: \"OT\", 9: \"TW\"}\n",
    "}\n",
    "# groups_sep = [[int(g) for g in gr.split('_')] for gr in group_order]\n",
    "# group_names = [\n",
    "#     [\n",
    "#         group_codes[sens_cols[i]][c]\n",
    "#         for i, c in enumerate(gc)\n",
    "#     ]\n",
    "#     for gc in groups_sep]\n",
    "# group_names = ['_'.join(g) for g in group_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import plot_qmeans, plot_sep, groupby_time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "experiments = [\n",
    "    # ('SGD', 'abs_loss_equality', 0.05),\n",
    "    # ('SGD', 'abs_diff_pr', 0.05),\n",
    "\n",
    "    # ('SGD+Reg', 'abs_loss_equality', 0.05),\n",
    "\n",
    "    # ('SSG', 'abs_loss_equality', 0.05),\n",
    "    # ('SSG', 'abs_diff_pr', 0.05),\n",
    "\n",
    "    # ('ssg_start_cc', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cc10', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cc01', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cc11', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_dd10', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cd00', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cd01', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cd11', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cd12', 'abs_loss_equality', 0.05),\n",
    "    # ('ssg_start_cd15', 'abs_loss_equality', 0.05),\n",
    "\n",
    "\n",
    "    # ('SGD+Reg', 'abs_diff_pr', 0.05),\n",
    "\n",
    "    # ('sgd_0.', 'abs_loss_equality', 0.05),\n",
    "    # ('sgd_0.3', 'abs_loss_equality', 0.05),\n",
    "    # ('sgd_0.4', 'abs_loss_equality', 0.05),\n",
    "    # ('sgd_0.5', 'abs_loss_equality', 0.05),\n",
    "    # ('sgd_1.0', 'abs_loss_equality', 0.05),\n",
    "\n",
    "\n",
    "    # ('SSLALM', \"abs_loss_equality\", 0.05),\n",
    "    # ('SSLALM', 'abs_diff_pr', 0.05),\n",
    "\n",
    "    # ('sslalm_005005', 'abs_loss_equality', 0.05),\n",
    "    # ('sslalm_005001', 'abs_loss_equality', 0.05),\n",
    "    # ('sslalm_001005', 'abs_loss_equality', 0.05),\n",
    "    # ('sslalm_001001', 'abs_loss_equality', 0.05),\n",
    "    \n",
    "    ('CURRENT_DATE_TIME','idk', 0.05)\n",
    "\n",
    "    # ('StochasticGhost', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_210', 'abs_loss_equality', 0.05),\n",
    "    # ('StochasticGhost', 'abs_diff_pr', 0.05),\n",
    "\n",
    "    # ('ghost_z005_g005_a03_b10', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z001_g001_a03_b10', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z001_g001_a03_b20', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z001_g001_a03_b1', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z001_g001_a03_b1_r05', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z001_g0005_a03_b1_r01', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z001_g0005_a03_b1_r08', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z005_g0005_a03_b1_r01', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z005_g0005_a03_b1_r08', 'abs_loss_equality', 0.05),\n",
    "    # ('ghost_z001_g001_a04_b10', 'abs_loss_equality', 0.05),\n",
    "]\n",
    "n_constraints = 2\n",
    "n_rows = len(experiments)\n",
    "n_cols = n_constraints+1\n",
    "\n",
    "f, axes = plt.subplots(n_rows,n_cols)\n",
    "\n",
    "def plot_conv(f, axes, datas, experiments, col):\n",
    "    for i, (alg_name, c, cb) in enumerate(experiments):\n",
    "        alg = f\"{alg_name}__{c}__{cb}\"\n",
    "        data = datas['CURRENT_DATE_TIME']\n",
    "        # data = datas[alg]\n",
    "        if 'time_r' not in data.columns:\n",
    "            _ = groupby_time(\n",
    "                data = data,\n",
    "                round_step=1.0)\n",
    "            \n",
    "        if isinstance(axes, plt.Axes):\n",
    "            axes = [[axes]]\n",
    "        for j in range(n_cols):\n",
    "            # ax = axes[j, i] if axes.ndim > 1 else axes[j]\n",
    "            ax = axes[i, j] if axes.ndim > 1 else axes[j]\n",
    "            if j == 0:\n",
    "                plot_qmeans(data, plot_col='f_full', x_col='time_r', idx_col='trial', ax=ax, group_by_col='time_r', q1=0.25, q2=0.75, c=col)\n",
    "                # plot_sep(data, plot_col='f_full', x_col='time_r', idx_col='trial', ax=ax,  q1=0, q2=1, col=col)\n",
    "                ax.set_ylabel(alg_name if alg_name != 'SSG' else 'SSw')\n",
    "            else:\n",
    "                c_idx = j-1\n",
    "                plot_qmeans(data, plot_col=f'c_full{c_idx}_corrected', x_col='time_r', idx_col='trial', ax=ax, group_by_col='time_r', q1=0.25, q2=0.75, c=col)\n",
    "                # plot_sep(data, plot_col=f'c_full{c_idx}_corrected', x_col='time_r', idx_col='trial', ax=ax, q1=0, q2=1, col=col)\n",
    "        f.set_figwidth(30)\n",
    "        f.set_figheight(4*n_rows)\n",
    "        data.drop('time_r', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_conv(f, axes, data_train, experiments,col='blue')\n",
    "# plot_conv(f, axes, data_val, experiments, col='pink')\n",
    "plot_conv(f, axes, data_test, experiments, col='orange')\n",
    "\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        ax = axes[row, col] if axes.ndim > 1 else axes[col]\n",
    "        ax.get_legend().remove()\n",
    "        ax.grid()\n",
    "        if row == 0:\n",
    "            ax.set_title(group_codes['MAR'][col] if col > 0 else 'Loss')\n",
    "        if col == 0:\n",
    "            ax.set_ylim(0.4,0.75)\n",
    "            ax.patch.set_linewidth(2)\n",
    "            ax.patch.set_edgecolor('black')\n",
    "        else:\n",
    "            ax.hlines((0, 0.05), 0, ax.get_xbound()[1], ls='--', color='black')\n",
    "            ax.set_ylim((-0.01, 0.14))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
